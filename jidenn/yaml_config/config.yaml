defaults:
  # - data: pythia
  - _self_

hydra:
  run:
    dir: ${general.logdir} #${general.base_logdir}/${now:%Y-%m-%d}__${now:%H_%M_%S}
  sweep:
    dir: ${general.logdir} #${general.base_logdir}/${now:%Y-%m-%d}__${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  job_logging:
    formatters:
      simple:
        format: "[%(asctime)s][%(levelname)s] - %(message)s"
  output_subdir: config

general:
  model: part # Model to use, options:  transformer, basic_fc, BDT, highway.
  base_logdir: logs # Base log directory for all runs.
  seed: 42 # Random seed.
  threads: # Maximum number of threads to use. 0 means all available.
  debug: False # Debug mode.
  logdir: ${general.base_logdir}/${now:%Y-%m-%d}__${now:%H-%M-%S} #${hydra:runtime.output_dir} # Path to log directory.
  checkpoint: # Path to checkpoint directory (inside logdir). If nothing is set (or null), no checkpoints are saved.
  backup: "backup" # Path to backup directory (inside logdir). If nothing is set (or null), no backups are saved.
  backup_freq: 10_000 # Frequency of backups (in batches). Use None (null) to backup every epoch.
  load_checkpoint_path:

data:
  path: ../jetclass/JetClass_tf/train_100M
  dev_path: ../jetclass/JetClass_tf/val_5M
  dataset_weigths:
  dataset_norm: 
  target: [label_Tbl, label_Tbqq, label_Hgg, label_Hbb, label_H4q, label_Wqq, label_Hcc, label_QCD, label_Zqq, label_Hqql]
  labels: # list of labels to use.
    - t->b+l
    - t->b+qq
    - H->gg
    - H->bb
    - H->4q
    - W->qq
    - H->cc
    - QCD
    - Z->qq
    - H->qq+l
  target_labels:
    - [0]
    - [1]
    - [2]
    - [3]
    - [4]
    - [5]
    - [6]
    - [7]
    - [8]
    - [9]
  variable_unknown_labels: 
  max_constituents: 100
  label_weights:
  cut:
  weight: 

test_data: 
  path: ../jetclass/JetClass_tf/test_20M
  dataset_weigths:
  dataset_norm: 
  target: [label_Tbl, label_Tbqq, label_Hgg, label_Hbb, label_H4q, label_Wqq, label_Hcc, label_QCD, label_Zqq, label_Hqql]
  labels: # list of labels to use.
    - t->b+l
    - t->b+qq
    - H->gg
    - H->bb
    - H->4q
    - W->qq
    - H->cc
    - QCD
    - Z->qq
    - H->qq+l
  target_labels:
    - [0]
    - [1]
    - [2]
    - [3]
    - [4]
    - [5]
    - [6]
    - [7]
    - [8]
    - [9]
  variable_unknown_labels: 
  max_constituents: 100
  label_weights:
  cut:
  weight: 

optimizer:
  name: Lamb # Optimizer to use, options: Adam, RAdam, AdamW, NAdam, Lion, Lamb.
  warmup_steps: 2_000  # Number of steps, when learning rate is increased linearly.
  learning_rate: 0.001 #0.001 #0.001 #0.000025 #0.0001 # Learning rate.
  min_learning_rate: 0.01 # Minimal learning rate as a fraction of the initial learning rate.
  decay_steps: #976_560 #244_140 #976_560 #1263000 #315_500 # If None autocomputes as: int(epochs * take / batch_size) - warmup_steps. Automatic computation is supported only if take is set.
  clipnorm:  # Gradient clipping (maximal norm of the gradient).
  clipvalue: 100. #100. 
  weight_decay:   # Weight decay (L2 regularization).
  beta_1:  # Adam beta_1 (optimizer parameter).
  label_smoothing:  # Label smoothing.
  beta_2:  #part: 0.999
  epsilon:  #part : 0.0001

dataset:
  epochs: 5 # Number of epochs.
  steps_per_epoch:  #195_312 # Number of steps per epoch. If None, autocomputes as: int(take * (1 - dev_size) / batch_size).
  take: # Length of data to use.
  batch_size: 512 #2048 #1024 # Batch size.
  dev_take:  # Size of dev dataset.
  test_take: 100_000 # Size of test dataset.
  shuffle_buffer: 1_000 # Size of shuffler buffer.
  cache:  # Cache dataset in memory (`mem`) or on disk (`disk`).

preprocess:
  normalization_size: 100 # Size of normalization dataset (if normalize is True).
  draw_distribution:  # Size of the distribution to draw.
  min_max_normalization: False

models:
  fc:
    layer_size: 512 # Hidden layer sizes.
    num_layers: 11
    dropout: 0.2 # Dropout after FC layers.
    activation: swish # Activation function to use (relu/elu/gelu/silu).
    train_input: jet_class_highlevel

  highway:
    layer_size: 344 # Size of the highway layer.
    num_layers: 11 # Number of highway layers.
    dropout: 0.2 # Dropout after highway layers.
    activation: gelu # Activation function to use (relu/elu/gelu/silu).
    train_input: jet_class_highlevel

  transformer:
    dropout: 0.1
    expansion: 4 #4,  number of hidden units in FFN is expansion * embed_dim
    heads: 8 #12, must divide embed_dim
    self_attn_layers: 13 #,12
    embed_dim: 128 #232
    embed_layers: 3 # Number of embedding layers.
    activation: gelu # Activation function to use (relu/elu/gelu/silu).
    train_input: jet_class_non_int

  part:
    self_attn_layers: 8
    embed_dim: 128 #256
    class_attn_layers: 2
    expansion: 4 #4,  number of hidden units in FFN is expansion * embed_dim
    heads: 8 #12, must divide embed_dim
    dropout: 0.1
    embed_layers: 3 # Number of embedding layers.
    interaction_embedding_layers: 3 # Number of embedding layers. Last one must be heads
    interaction_embedding_layer_size: 64 # Size of the embedding layer.
    activation: gelu # Activation function to use (relu/elu/gelu/swish).
    train_input: jet_class_non_int

  depart:
    self_attn_layers: 11 # 6,12
    embed_dim: 128 #232
    embed_layers: 3 # Number of embedding layers.
    expansion: 4 # 4,  number of hidden units in FFN is expansion * embed_dim
    heads: 8 # 12, must divide embed_dim
    class_attn_layers: 2
    dropout: 0.1 # Dropout after FFN layer.
    class_dropout: 0. # Dropout after FFN layer.
    layer_scale_init_value: 5.0e-3
    stochastic_depth_drop_rate: 0.2
    class_stochastic_depth_drop_rate: 0.
    interaction_embedding_layers: 3 # Number of embedding layers. Last one must be heads
    interaction_embedding_layer_size: 64 # Size of the embedding layer.
    activation: gelu # Activation function to use (relu/elu/gelu/silu).
    train_input: jet_class_non_int

  pfn:
    Phi_sizes: [512, 512, 512, 512, 512] # Sizes of the per particle mapping.
    F_sizes: [512, 512, 512, 512, 512, 512] # Sizes of jet mapping.
    Phi_backbone: fc #  Backbone network to use, options: cnn, fc.
    batch_norm: False #  Use batch normalization before PHI.
    activation: gelu #  Activation function to use (relu/elu/gelu/silu).
    Phi_dropout: #  Dropout after PHI.
    F_dropout: #  Dropout after F.
    train_input: jet_class_non_int

  efn:
    Phi_sizes: [512, 512, 512, 512, 136] # Sizes of the per particle mapping.
    F_sizes: [512, 512, 512, 512, 512, 512] # Sizes of jet mapping.
    Phi_backbone: fc #  Backbone network to use, options: cnn, fc.
    batch_norm: False #  Use batch normalization before PHI.
    activation: gelu #  Activation function to use (relu/elu/gelu/silu).
    Phi_dropout: #  Dropout after PHI.
    F_dropout: #  Dropout after F.
    train_input: irc_safe

  bdt:
    num_trees: 300 #  Number of trees in the forest.
    growing_strategy: BEST_FIRST_GLOBAL #  Growing strategy.
    max_depth: 30 #  Maximum depth of the tree.
    split_axis: SPARSE_OBLIQUE #  Split axis.
    shrinkage: 0.1 #learning rate
    max_num_nodes: 200 #  Maximum number of nodes in the tree.
    min_examples: 2_500 #  Minimum number of examples in a leaf.
    num_threads: 64 #  Number of threads to use.
    l2_regularization: 0.1 #  L2 regularization.
    tmp_dir: ${general.logdir}/backup #  Temporary directory for BDT.
    train_input: highlevel_constituents

  particlenet:
    pooling: average #  Convolutional pooling, options: max, average.
    fc_layers: [768, 768] #  Hidden layer sizes.
    fc_dropout: [0.3, 0.3] #  Dropout after FC layers.
    edge_knn:
      - 16 #  Number of neighbors.
      - 16 #  Number of neighbors.
      - 16 #  Number of neighbors.
      - 16 #  Number of neighbors.
      - 16 #  Number of neighbors.
    edge_layers:
      - [128, 128, 128] #  Number of channels.
      - [128, 128, 128] #  Number of channels.
      - [256, 256, 256] #  Number of channels.
      - [256, 256, 256] #  Number of channels.
      - [512, 512, 512]
    train_input: jet_class_pnet
    activation: gelu #  Activation function to use (relu/elu/gelu/silu).

augmentations:
  order: []
  rotation:
    prob: 0.5
    const_name: Constituent
    max_angle: 360
  collinear_split:
    prob: 0.5
    const_name: Constituent
    splitting_amount: 0.05
  soft_smear:
    prob: 0.5
    const_name: Constituent
    energy_scale: 1000
  pt_smear:
    prob: 0.5
    const_name: Constituent
    std_pt_frac: 0.02
  drop_soft:
    prob: 0.5
    const_name: Constituent
    skew: 100
    center_location: 0.5
    min_number_consti: 3
  shift_weights:
    prob: 0.5
    const_name: Constituent
    training_weight: weight_flat
    shift_weight: weight_mc
    shift_weight_idxs: [1,2,23,24,25,26,3,10]
    nominal_weight_idx: 0
  boost:
    prob: 0.5
    const_name: Constituent
    max_beta: 0.1


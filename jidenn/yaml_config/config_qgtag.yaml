defaults:
  # - data: pythia
  - _self_

hydra:
  run:
    dir: ${general.logdir} #${general.base_logdir}/${now:%Y-%m-%d}__${now:%H_%M_%S}
  sweep:
    dir: ${general.logdir} #${general.base_logdir}/${now:%Y-%m-%d}__${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  job_logging:
    formatters:
      simple:
        format: "[%(asctime)s][%(levelname)s] - %(message)s"
  output_subdir: config

general:
  model: transformer # Model to use, options:  transformer, basic_fc, BDT, highway.
  base_logdir: logs # Base log directory for all runs.
  seed: 42 # Random seed.
  threads: # Maximum number of threads to use. 0 means all available.
  debug: False # Debug mode.
  logdir: ${general.base_logdir}/${now:%Y-%m-%d}__${now:%H-%M-%S} #${hydra:runtime.output_dir} # Path to log directory.
  checkpoint: # Path to checkpoint directory (inside logdir). If nothing is set (or null), no checkpoints are saved.
  backup: "backup" # Path to backup directory (inside logdir). If nothing is set (or null), no backups are saved.
  backup_freq: 10_000 # Frequency of backups (in batches). Use None (null) to backup every epoch.
  load_checkpoint_path:

data:
  #path: #[data/r22_PFO/fwd_2d_flat_20-160GeV/Pythia8EvtGen_A14NNPDF23LO_jetjet, data/r22_PFO/central_2d_flat_160-1300GeV/Pythia8EvtGen_A14NNPDF23LO_jetjet, data/r22_PFO/super_central_2d_flat_1300-2500GeV/Pythia8EvtGen_A14NNPDF23LO_jetjet]
  # path: #sum = 193994742
  #   - 'data/r22_PFO/fwd_2d_flat_20-160GeV_all_jets/Sherpa_CT10_CT14nnlo_CSShower_Lund_2to2jets'
  #   - 'data/r22_PFO/fwd_2d_flat_20-160GeV_all_jets/Sherpa_CT10_CT14nnlo_CSShower_2to2jets'
  #   - 'data/r22_PFO/fwd_2d_flat_20-160GeV_all_jets/Pythia8EvtGen_A14NNPDF23LO_jetjet'
  #   - 'data/r22_PFO/fwd_2d_flat_20-160GeV_all_jets/PhPy8EG_jj'
  #   - 'data/r22_PFO/fwd_2d_flat_20-160GeV_all_jets/H7EG_Matchbox_dipole_jetjetNLO'
  #   - 'data/r22_PFO/fwd_2d_flat_20-160GeV_all_jets/H7EG_Matchbox_angular_jetjetNLO'
  #   - 'data/r22_PFO/central_2d_flat_160-1300GeV_all_jets/Sherpa_CT10_CT14nnlo_CSShower_Lund_2to2jets'
  #   - 'data/r22_PFO/central_2d_flat_160-1300GeV_all_jets/Sherpa_CT10_CT14nnlo_CSShower_2to2jets'
  #   - 'data/r22_PFO/central_2d_flat_160-1300GeV_all_jets/Pythia8EvtGen_A14NNPDF23LO_jetjet'
  #   - 'data/r22_PFO/central_2d_flat_160-1300GeV_all_jets/PhPy8EG_jj'P
  #   - 'data/r22_PFO/central_2d_flat_160-1300GeV_all_jets/H7EG_Matchbox_dipole_jetjetNLO'
  #   - 'data/r22_PFO/central_2d_flat_160-1300GeV_all_jets/H7EG_Matchbox_angular_jetjetNLO'
  #   - 'data/r22_PFO/super_central_2d_flat_1300-2500GeV_all_jets/Sherpa_CT10_CT14nnlo_CSShower_Lund_2to2jets'
  #   - 'data/r22_PFO/super_central_2d_flat_1300-2500GeV_all_jets/Sherpa_CT10_CT14nnlo_CSShower_2to2jets'
  #   - 'data/r22_PFO/super_central_2d_flat_1300-2500GeV_all_jets/Pythia8EvtGen_A14NNPDF23LO_jetjet'
  #   - 'data/r22_PFO/super_central_2d_flat_1300-2500GeV_all_jets/PhPy8EG_jj'
  #   - 'data/r22_PFO/super_central_2d_flat_1300-2500GeV_all_jets/H7EG_Matchbox_dipole_jetjetNLO'
  #   - 'data/r22_PFO/super_central_2d_flat_1300-2500GeV_all_jets/H7EG_Matchbox_angular_jetjetNLO' 
  # dataset_weigths: [0.02112982010615525, 0.12106414203741667, 0.2577389442854075, 0.0515477888570815, 0.030236587546274837, 0.031197129043837694, 0.016141720995716473, 0.035589495513234065, 0.2577389442854075, 0.02577389442854075, 0.05402791277714115, 0.05439303607517362, 0.007360374746651638, 0.00733480188859964, 0.011061304950213548, 0.00515477888570815, 0.006320367177786705, 0.0061889563996533475]

  # path: # total 64,678,482
  #   - 'data/r22_PFO_v2/super_central_2d_flat_1300-2500GeV/Pythia8EvtGen_A14NNPDF23LO_jetjet'
  #   - 'data/r22_PFO_v2/central_2d_flat_160-1300GeV/Pythia8EvtGen_A14NNPDF23LO_jetjet'
  #   - 'data/r22_PFO_v2/fwd_2d_flat_20-160GeV/Pythia8EvtGen_A14NNPDF23LO_jetjet'
  # dataset_weigths: [0.03314086746810168, 0.7730546304410794, 0.193804502090819]
  path:
    - data/r22_PFO/fwd_2d_flat_20-160GeV/Pythia8EvtGen_A14NNPDF23LO_jetjet # (50M)
    - data/r22_PFO/central_2d_flat_160-1300GeV/Pythia8EvtGen_A14NNPDF23LO_jetjet # (50M)
    - data/r22_PFO/super_central_2d_flat_1300-2500GeV/Pythia8EvtGen_A14NNPDF23LO_jetjet # (2.143M)
  dataset_weigths:
    - 0.4895058032626247
    - 0.4895058032626247
    - 0.02098839347475058
    # - 0.4895058032626247
    # - 0.4895058032626247
    # - 0.02098839347475058
  # dataset_weigths: [0.48949621881303335,0.48949621881303335,0.02100756237393331]
  # path:
  #   - data/r22_UFO/fwd_2d_flat_20-160GeV/Pythia8EvtGen_A14NNPDF23LO_jetjet
  #   - data/r22_UFO/central_2d_flat_160-1300GeV/Pythia8EvtGen_A14NNPDF23LO_jetjet
  #   - data/r22_UFO/super_central_2d_flat_1300-2500GeV/Pythia8EvtGen_A14NNPDF23LO_jetjet
  # dataset_weigths:
  #   - 0.46967126090780276
  #   - 0.48439295584138187
  #   - 0.045935783250815376
  
  # dataset_weigths: [0.46967126090780276, 0.48439295584138187, 0.045935783250815376] #UFO
  #dataset_weigths: #[0.4895058032626247, 0.4895058032626247, 0.02098839347475058] #PFO
  # dataset_norm: [0.47593540639309573, 0.4827016009574705, 0.04591820947437745]
  dataset_norm: # [0.47593540639309573, 0.4827016009574705, 0.04591820947437745]
  # equalize_dataset_weights: True
  target: jets_PartonTruthLabelID
  labels: # list of labels to use.
    - gluon
    - quark
  target_labels:
    - [21]
    - [1, 2, 3, 4, 5, 6]
  variable_unknown_labels: [-1, -999]
  max_constituents: 60
  label_weights:
  cut:
  weight: weight_flat

test_data: 
  # path: data/all_MCs/Pythia8EvtGen_A14NNPDF23LO_jetjet/dev
  # target: jets_PartonTruthLabelID
  # labels: # list of labels to use.
  #   - gluon
  #   - quark
  # target_labels:
  #   - [21]
  #   - [1, 2, 3, 4, 5, 6]
  # variable_unknown_labels: [-1, -999]
  # label_weights:
  # cut:
  # weight: weight

optimizer:
  name: Adam # Optimizer to use, options: Adam, LAMB, Lion.
  warmup_steps: 2_000 # Number of steps, when learning rate is increased linearly.
  learning_rate: 0.0001 #0.000025 #0.0001 # Learning rate.
  min_learning_rate: 0.001 # Minimal learning rate as a fraction of the initial learning rate.
  decay_steps: 498_500 #1263000 #315_500 # If None autocomputes as: int(epochs * take / batch_size) - warmup_steps. Automatic computation is supported only if take is set.
  clipnorm: 0.8 # Gradient clipping (maximal norm of the gradient).
  weight_decay: # Weight decay (L2 regularization).
  beta_1: # Adam beta_1 (optimizer parameter).
  label_smoothing: 0. # Label smoothing.
  beta_2: #part: 0.999
  epsilon: #part : 0.0001

dataset:
  epochs: 10 # Number of epochs.
  steps_per_epoch: 49_850 #31_550 #126300 #31_550 #49849 #49875 # Number of steps per epoch. If None, autocomputes as: int(take * (1 - dev_size) / batch_size).
  take: # Length of data to use.
  batch_size: 2048 #512 #2048 #1024 # Batch size.
  dev_size: 0.1 # Size of dev dataset.
  test_take: 100_000 # Size of test dataset.
  shuffle_buffer: 1_000 # Size of shuffler buffer.
  cache: disk # Cache dataset in memory (`mem`) or on disk (`disk`).

preprocess:
  normalization_size: 150 # Size of normalization dataset (if normalize is True).
  draw_distribution: 50000 # Size of the distribution to draw.

models:
  fc:
    layer_size: 512 # Hidden layer sizes.
    num_layers: 11
    dropout: 0.2 # Dropout after FC layers.
    activation: swish # Activation function to use (relu/elu/gelu/silu).
    train_input: highlevel

  highway:
    layer_size: 344 # Size of the highway layer.
    num_layers: 11 # Number of highway layers.
    dropout: 0.2 # Dropout after highway layers.
    activation: gelu # Activation function to use (relu/elu/gelu/silu).
    train_input: highlevel

  transformer:
    dropout: 0.1
    expansion: 4 #4,  number of hidden units in FFN is expansion * embed_dim
    heads: 8 #12, must divide embed_dim
    self_attn_layers: 13 #,12
    embed_dim: 128 #232
    embed_layers: 3 # Number of embedding layers.
    activation: gelu # Activation function to use (relu/elu/gelu/silu).
    train_input: constituents

  part:
    self_attn_layers: 11
    embed_dim: 128 #256
    class_attn_layers: 2
    expansion: 4 #4,  number of hidden units in FFN is expansion * embed_dim
    heads: 8 #12, must divide embed_dim
    dropout: 0.1
    embed_layers: 3 # Number of embedding layers.
    interaction_embedding_layers: 3 # Number of embedding layers. Last one must be heads
    interaction_embedding_layer_size: 64 # Size of the embedding layer.
    activation: gelu # Activation function to use (relu/elu/gelu/silu).
    train_input: constituents

  depart:
    self_attn_layers: 11 # 6,12
    embed_dim: 128 #232
    embed_layers: 3 # Number of embedding layers.
    expansion: 4 # 4,  number of hidden units in FFN is expansion * embed_dim
    heads: 8 # 12, must divide embed_dim
    class_attn_layers: 2
    dropout: 0.1 # Dropout after FFN layer.
    class_dropout: 0. # Dropout after FFN layer.
    layer_scale_init_value: 5.0e-3
    stochastic_depth_drop_rate: 0.2
    class_stochastic_depth_drop_rate: 0.
    interaction_embedding_layers: 3 # Number of embedding layers. Last one must be heads
    interaction_embedding_layer_size: 64 # Size of the embedding layer.
    activation: gelu # Activation function to use (relu/elu/gelu/silu).
    train_input: constituents

  pfn:
    Phi_sizes: [512, 512, 512, 512, 512] # Sizes of the per particle mapping.
    F_sizes: [512, 512, 512, 512, 512, 512] # Sizes of jet mapping.
    Phi_backbone: fc #  Backbone network to use, options: cnn, fc.
    batch_norm: False #  Use batch normalization before PHI.
    activation: gelu #  Activation function to use (relu/elu/gelu/silu).
    Phi_dropout: #  Dropout after PHI.
    F_dropout: #  Dropout after F.
    train_input: constituents

  efn:
    Phi_sizes: [512, 512, 512, 512, 136] # Sizes of the per particle mapping.
    F_sizes: [512, 512, 512, 512, 512, 512] # Sizes of jet mapping.
    Phi_backbone: fc #  Backbone network to use, options: cnn, fc.
    batch_norm: False #  Use batch normalization before PHI.
    activation: gelu #  Activation function to use (relu/elu/gelu/silu).
    Phi_dropout: #  Dropout after PHI.
    F_dropout: #  Dropout after F.
    train_input: irc_safe

  bdt:
    num_trees: 300 #  Number of trees in the forest.
    growing_strategy: BEST_FIRST_GLOBAL #  Growing strategy.
    max_depth: 30 #  Maximum depth of the tree.
    split_axis: SPARSE_OBLIQUE #  Split axis.
    shrinkage: 0.1 #learning rate
    max_num_nodes: 200 #  Maximum number of nodes in the tree.
    min_examples: 2_500 #  Minimum number of examples in a leaf.
    num_threads: 64 #  Number of threads to use.
    l2_regularization: 0.1 #  L2 regularization.
    tmp_dir: ${general.logdir}/backup #  Temporary directory for BDT.
    train_input: highlevel_constituents

  particlenet:
    pooling: average #  Convolutional pooling, options: max, average.
    fc_layers: [768, 768] #  Hidden layer sizes.
    fc_dropout: [0.3, 0.3] #  Dropout after FC layers.
    edge_knn:
      - 16 #  Number of neighbors.
      - 16 #  Number of neighbors.
      - 16 #  Number of neighbors.
      - 16 #  Number of neighbors.
      - 16 #  Number of neighbors.
    edge_layers:
      - [128, 128, 128] #  Number of channels.
      - [128, 128, 128] #  Number of channels.
      - [256, 256, 256] #  Number of channels.
      - [256, 256, 256] #  Number of channels.
      - [512, 512, 512]
    train_input: gnn
    activation: gelu #  Activation function to use (relu/elu/gelu/silu).

augmentations:
  order: []
  rotation:
    prob: 0.5
    const_name: Constituent
    max_angle: 360
  collinear_split:
    prob: 0.5
    const_name: Constituent
    splitting_amount: 0.05
  soft_smear:
    prob: 0.5
    const_name: Constituent
    energy_scale: 1000
  pt_smear:
    prob: 0.5
    const_name: Constituent
    std_pt_frac: 0.02
  drop_soft:
    prob: 0.5
    const_name: Constituent
    skew: 100
    center_location: 0.5
    min_number_consti: 3
  shift_weights:
    prob: 0.5
    const_name: Constituent
    training_weight: weight_flat
    shift_weight: weight_mc
    shift_weight_idxs: [1,2,23,24,25,26,3,10]
    nominal_weight_idx: 0
  boost:
    prob: 0.5
    const_name: Constituent
    max_beta: 0.1


# data.path=['data/r22_PFO/fwd_2d_flat_20-160GeV/Sherpa_CT10_CT14nnlo_CSShower_Lund_2to2jets', 
#         'data/r22_PFO/fwd_2d_flat_20-160GeV/Sherpa_CT10_CT14nnlo_CSShower_2to2jets', 
#         'data/r22_PFO/fwd_2d_flat_20-160GeV/Pythia8EvtGen_A14NNPDF23LO_jetjet', 
#         'data/r22_PFO/fwd_2d_flat_20-160GeV/PhPy8EG_jj', 
#         'data/r22_PFO/fwd_2d_flat_20-160GeV/H7EG_Matchbox_dipole_jetjetNLO', 
#         'data/r22_PFO/fwd_2d_flat_20-160GeV/H7EG_Matchbox_angular_jetjetNLO', 
        
#         'data/r22_PFO/central_2d_flat_160-1300GeV/Sherpa_CT10_CT14nnlo_CSShower_Lund_2to2jets', 
#         'data/r22_PFO/central_2d_flat_160-1300GeV/Sherpa_CT10_CT14nnlo_CSShower_2to2jets', 
#         'data/r22_PFO/central_2d_flat_160-1300GeV/Pythia8EvtGen_A14NNPDF23LO_jetjet', 
#         'data/r22_PFO/central_2d_flat_160-1300GeV/PhPy8EG_jj',
#         'data/r22_PFO/central_2d_flat_160-1300GeV/H7EG_Matchbox_dipole_jetjetNLO', 
#         'data/r22_PFO/central_2d_flat_160-1300GeV/H7EG_Matchbox_angular_jetjetNLO', 
        
#         'data/r22_PFO/super_central_2d_flat_1300-2500GeV/Sherpa_CT10_CT14nnlo_CSShower_Lund_2to2jets', 
#         'data/r22_PFO/super_central_2d_flat_1300-2500GeV/Sherpa_CT10_CT14nnlo_CSShower_2to2jets', 
#         'data/r22_PFO/super_central_2d_flat_1300-2500GeV/Pythia8EvtGen_A14NNPDF23LO_jetjet',
#         'data/r22_PFO/super_central_2d_flat_1300-2500GeV/PhPy8EG_jj', 
#         'data/r22_PFO/super_central_2d_flat_1300-2500GeV/H7EG_Matchbox_dipole_jetjetNLO', 
#         'data/r22_PFO/super_central_2d_flat_1300-2500GeV/H7EG_Matchbox_angular_jetjetNLO' 
#         ]



# data.dataset_weigths=[
#            4099074,#'data/r22_PFO/fwd_2d_flat_20-160GeV/Sherpa_CT10_CT14nnlo_CSShower_Lund_2to2jets', 
#            23485807,#'data/r22_PFO/fwd_2d_flat_20-160GeV/Sherpa_CT10_CT14nnlo_CSShower_2to2jets', 
#            50000000,#'data/r22_PFO/fwd_2d_flat_20-160GeV/Pythia8EvtGen_A14NNPDF23LO_jetjet', 
#            # 40032194,#'data/r22_PFO/fwd_2d_flat_20-160GeV/PhPy8EG_jj', 
#            10000000,#'data/r22_PFO/fwd_2d_flat_20-160GeV/PhPy8EG_jj', 
#            5865739,#'data/r22_PFO/fwd_2d_flat_20-160GeV/H7EG_Matchbox_dipole_jetjetNLO', 
#            6052079,#'data/r22_PFO/fwd_2d_flat_20-160GeV/H7EG_Matchbox_angular_jetjetNLO', 
#            #
#            3131409,#'data/r22_PFO/central_2d_flat_160-1300GeV/Sherpa_CT10_CT14nnlo_CSShower_Lund_2to2jets', 
#            6904175,#'data/r22_PFO/central_2d_flat_160-1300GeV/Sherpa_CT10_CT14nnlo_CSShower_2to2jets', 
#            50000000,#'data/r22_PFO/central_2d_flat_160-1300GeV/Pythia8EvtGen_A14NNPDF23LO_jetjet', 
#            #45101921,#'data/r22_PFO/central_2d_flat_160-1300GeV/PhPy8EG_jj',
#            5000000,#'data/r22_PFO/central_2d_flat_160-1300GeV/PhPy8EG_jj',
#            10481131,#'data/r22_PFO/central_2d_flat_160-1300GeV/H7EG_Matchbox_dipole_jetjetNLO', 
#            10551963,#'data/r22_PFO/central_2d_flat_160-1300GeV/H7EG_Matchbox_angular_jetjetNLO', 
#            #
#            1427874,#'data/r22_PFO/super_central_2d_flat_1300-2500GeV/Sherpa_CT10_CT14nnlo_CSShower_Lund_2to2jets', 
#            1422913,#'data/r22_PFO/super_central_2d_flat_1300-2500GeV/Sherpa_CT10_CT14nnlo_CSShower_2to2jets', 
#            2145835,#'data/r22_PFO/super_central_2d_flat_1300-2500GeV/Pythia8EvtGen_A14NNPDF23LO_jetjet',
#            #4362334,#'data/r22_PFO/super_central_2d_flat_1300-2500GeV/PhPy8EG_jj', 
#            1000000,#'data/r22_PFO/super_central_2d_flat_1300-2500GeV/PhPy8EG_jj', 
#            1226118,#'data/r22_PFO/super_central_2d_flat_1300-2500GeV/H7EG_Matchbox_dipole_jetjetNLO', 
#            1200625,#'data/r22_PFO/super_central_2d_flat_1300-2500GeV/H7EG_Matchbox_angular_jetjetNLO' 
#            ]





# data=sum([4099074, 23485807, 50000000, 10000000, 5865739, 6052079, 3131409, 6904175, 50000000, 5000000, 10481131, 10551963,1427874, 1422913, 2145835, 1000000, 1226118, 1200625])


# data.path=['data/r22_PFO/fwd_2d_flat_20-160GeV/Sherpa_CT10_CT14nnlo_CSShower_Lund_2to2jets', 
#            'data/r22_PFO/fwd_2d_flat_20-160GeV/Sherpa_CT10_CT14nnlo_CSShower_2to2jets', 
#            'data/r22_PFO/fwd_2d_flat_20-160GeV/Pythia8EvtGen_A14NNPDF23LO_jetjet', 
#            'data/r22_PFO/fwd_2d_flat_20-160GeV/PhPy8EG_jj', 
#            'data/r22_PFO/fwd_2d_flat_20-160GeV/H7EG_Matchbox_dipole_jetjetNLO', 
#            'data/r22_PFO/fwd_2d_flat_20-160GeV/H7EG_Matchbox_angular_jetjetNLO', 
#            'data/r22_PFO/central_2d_flat_160-1300GeV/Sherpa_CT10_CT14nnlo_CSShower_Lund_2to2jets', 
#            'data/r22_PFO/central_2d_flat_160-1300GeV/Sherpa_CT10_CT14nnlo_CSShower_2to2jets', 
#            'data/r22_PFO/central_2d_flat_160-1300GeV/Pythia8EvtGen_A14NNPDF23LO_jetjet', 
#            'data/r22_PFO/central_2d_flat_160-1300GeV/PhPy8EG_jj',
#            'data/r22_PFO/central_2d_flat_160-1300GeV/H7EG_Matchbox_dipole_jetjetNLO', 
#            'data/r22_PFO/central_2d_flat_160-1300GeV/H7EG_Matchbox_angular_jetjetNLO', 
#            'data/r22_PFO/super_central_2d_flat_1300-2500GeV/Sherpa_CT10_CT14nnlo_CSShower_Lund_2to2jets', 
#            'data/r22_PFO/super_central_2d_flat_1300-2500GeV/Sherpa_CT10_CT14nnlo_CSShower_2to2jets', 
#            'data/r22_PFO/super_central_2d_flat_1300-2500GeV/Pythia8EvtGen_A14NNPDF23LO_jetjet',
#            'data/r22_PFO/super_central_2d_flat_1300-2500GeV/PhPy8EG_jj', 
#            'data/r22_PFO/super_central_2d_flat_1300-2500GeV/H7EG_Matchbox_dipole_jetjetNLO', 
#            'data/r22_PFO/super_central_2d_flat_1300-2500GeV/H7EG_Matchbox_angular_jetjetNLO' 
#            ]
# data.dataset_weigths=[0.02112982010615525, 0.12106414203741667, 0.2577389442854075, 0.0515477888570815, 0.030236587546274837, 0.031197129043837694, 0.016141720995716473, 0.035589495513234065, 0.2577389442854075, 0.02577389442854075, 0.05402791277714115, 0.05439303607517362, 0.007360374746651638, 0.00733480188859964, 0.011061304950213548, 0.00515477888570815, 0.006320367177786705, 0.0061889563996533475]
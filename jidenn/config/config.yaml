defaults:
  - data: data2
  - _self_

hydra:
  run:
    dir: ${general.logdir} #${general.base_logdir}/${now:%Y-%m-%d}__${now:%H_%M_%S}
  sweep:
    dir: ${general.logdir} #${general.base_logdir}/${now:%Y-%m-%d}__${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  job_logging:
    formatters:
      simple:
        format: "[%(asctime)s][%(levelname)s] - %(message)s"
  output_subdir: config

general:
  model: transformer # Model to use, options:  transformer, basic_fc, BDT, highway.
  base_logdir: logs # Base log directory for all runs.
  seed: 42 # Random seed.
  threads: # Maximum number of threads to use. 0 means all available.
  debug: False # Debug mode.
  logdir: ${general.base_logdir}/${now:%Y-%m-%d}__${now:%H-%M-%S} #${hydra:runtime.output_dir} # Path to log directory.
  checkpoint: # Path to checkpoint directory (inside logdir). If nothing is set (or null), no checkpoints are saved.
  backup: "backup" # Path to backup directory (inside logdir). If nothing is set (or null), no backups are saved.
  load_checkpoint_path:

optimizer:
  name: LAMB # Optimizer to use, options: Adam, LAMB.
  warmup_steps: 100 # Number of steps, when learning rate is increased linearly.
  learning_rate: 0.001 # Learning rate.
  decay_steps: # If None autocomputes as: int(epochs * take / batch_size) - warmup_steps
  clipnorm: # Gradient clipping (maximal norm of the gradient).
  weight_decay: # Weight decay (L2 regularization).
  beta_1: # Adam beta_1 (optimizer parameter).
  label_smoothing: 0. # Label smoothing.
  beta_2: #part: 0.999
  epsilon: #part : 0.0001

dataset:
  epochs: 4 # Number of epochs.
  take: 5_000_000 # Length of data to use.
  batch_size: 512 # Batch size.
  dev_size: 0.1 # Size of dev dataset.
  test_size: 0.1 # Size of test dataset.
  shuffle_buffer: 1000 # Size of shuffler buffer.

preprocess:
  normalization_size: 500 # Size of normalization dataset (if normalize is True).
  draw_distribution: 5_000 # Size of the distribution to draw.

models:
  fc:
    layer_size: 512 # Hidden layer sizes.
    num_layers: 11
    dropout: 0.2 # Dropout after FC layers.
    activation: swish # Activation function to use (relu/elu/gelu/silu).
    train_input: highlevel

  highway:
    layer_size: 344 # Size of the highway layer.
    num_layers: 11 # Number of highway layers.
    dropout: 0.2 # Dropout after highway layers.
    activation: swish # Activation function to use (relu/elu/gelu/silu).
    train_input: highlevel

  transformer:
    dropout: 0.
    expansion: 4 #4,  number of hidden units in FFN is expansion * embed_dim
    heads: 8 #12, must divide embed_dim
    self_attn_layers: 13 #,12
    embed_dim: 128 #232
    embed_layers: 3 # Number of embedding layers.
    activation: gelu # Activation function to use (relu/elu/gelu/silu).
    train_input: constituents

  part:
    self_attn_layers: 11 #,12
    embed_dim: 128 #256
    class_attn_layers: 2
    expansion: 4 #4,  number of hidden units in FFN is expansion * embed_dim
    heads: 8 #12, must divide embed_dim
    dropout: 0.1
    embed_layers: 3 # Number of embedding layers.
    interaction_embedding_layers: 3 # Number of embedding layers. Last one must be heads
    interaction_embedding_layer_size: 64 # Size of the embedding layer.
    activation: gelu # Activation function to use (relu/elu/gelu/silu).
    train_input: constituents

  depart:
    self_attn_layers: 11 # 6,12
    embed_dim: 128 #232
    embed_layers: 3 # Number of embedding layers.
    expansion: 4 # 4,  number of hidden units in FFN is expansion * embed_dim
    heads: 8 # 12, must divide embed_dim
    class_attn_layers: 2
    dropout: 0. # Dropout after FFN layer.
    class_dropout: 0. # Dropout after FFN layer.
    layer_scale_init_value: 1.0e-05
    stochastic_depth_drop_rate: 0.1
    class_stochastic_depth_drop_rate: 0.
    interaction_embedding_layers: 3 # Number of embedding layers. Last one must be heads
    interaction_embedding_layer_size: 64 # Size of the embedding layer.
    activation: gelu # Activation function to use (relu/elu/gelu/silu).
    train_input: constituents

  pfn:
    Phi_sizes: [512, 512, 512, 512, 512] # Sizes of the per particle mapping.
    F_sizes: [512, 512, 512, 512, 512, 512] # Sizes of jet mapping.
    Phi_backbone: cnn #  Backbone network to use, options: cnn, fc.
    batch_norm: False #  Use batch normalization before PHI.
    activation: relu #  Activation function to use (relu/elu/gelu/silu).
    Phi_dropout: #  Dropout after PHI.
    F_dropout: #  Dropout after F.
    train_input: constituents

  efn:
    Phi_sizes: [512, 512, 512, 512, 136] # Sizes of the per particle mapping.
    F_sizes: [512, 512, 512, 512, 512, 512] # Sizes of jet mapping.
    Phi_backbone: cnn #  Backbone network to use, options: cnn, fc.
    batch_norm: False #  Use batch normalization before PHI.
    activation: relu #  Activation function to use (relu/elu/gelu/silu).
    Phi_dropout: #  Dropout after PHI.
    F_dropout: #  Dropout after F.
    train_input: constituents

  bdt:
    num_trees: 1000 #  Number of trees in the forest.
    growing_strategy: BEST_FIRST_GLOBAL #  Growing strategy.
    max_depth: 30 #  Maximum depth of the tree.
    split_axis: SPARSE_OBLIQUE #  Split axis.
    shrinkage: 0.1 #learning rate
    max_num_nodes: 200 #  Maximum number of nodes in the tree.
    min_examples: 2_500 #  Minimum number of examples in a leaf.
    num_threads: 64 #  Number of threads to use.
    l2_regularization: 0.1 #  L2 regularization.
    tmp_dir: ${general.logdir}/backup #  Temporary directory for BDT.
    train_input: highlevel_constituents

defaults:
  - data: data2
  - _self_

hydra:
  run:
    dir: ${params.logdir} #${params.base_logdir}/${now:%Y-%m-%d}__${now:%H_%M_%S}
  sweep:
    dir: ${params.logdir} #${params.base_logdir}/${now:%Y-%m-%d}__${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  job_logging:
    formatters:
      simple:
        format: "[%(asctime)s][%(levelname)s] - %(message)s"
  output_subdir: config

params:
  base_logdir: logs # Base log directory for all runs.
  model: transformer # Model to use, options:  transformer, basic_fc, BDT, highway.
  epochs: 50 # Number of epochs.
  seed: 42 # Random seed.
  threads: # Maximum number of threads to use. 0 means all available.
  debug: False # Debug mode.
  logdir: ${params.base_logdir}/${now:%Y-%m-%d}__${now:%H-%M-%S} #${hydra:runtime.output_dir} # Path to log directory.
  checkpoint: # Path to checkpoint directory (inside logdir). If nothing is set (or null), no checkpoints are saved.
  backup: "backup" # Path to backup directory (inside logdir). If nothing is set (or null), no backups are saved.
  load_checkpoint_path:

optimizer:
  name: LAMB # Optimizer to use, options: Adam, LAMB.
  warmup_steps: 10_000 # 100
  learning_rate: 0.001 #0.0005 part
  label_smoothing: 0. # Label smoothing.
  decay_steps: 966_000 # 98_000 #(10 * 5_000_000 / 512 - 10_000) == (epochs * take / batch_size - warmup_steps)
  beta_1: 0.9 #part: 0.95
  beta_2: #part: 0.999
  epsilon: #part : 0.0001
  clipnorm: 1.0 # Gradient clipping.
  weight_decay: 0.02 #part: None

dataset:
  take: 10_000_000 # Length of data to use.
  batch_size: 512 # Batch size.
  validation_step: 200 # Validation every n batches.
  dev_size: 0.1 # Size of dev dataset.
  test_size: 0.1 # Size of test dataset.
  shuffle_buffer: 1000 # Size of shuffler buffer.

preprocess:
  normalize: False # Normalize data. max size 146208768
  normalization_size: 500 # 100_000 is better Size of normalization dataset.
  draw_distribution: 5_000 # Size of the distribution to draw.
  min_max_path: #'min_max.csv'

models:
  basic_fc:
    layer_size: 512 # Hidden layer sizes.
    num_layers: 11
    dropout: 0.2 # Dropout after FC layers.
    activation: swish # Activation function to use (relu/elu/gelu/silu).

  highway:
    layer_size: 344 # Size of the highway layer.
    num_layers: 11 # Number of highway layers.
    dropout: 0.2 # Dropout after highway layers.
    activation: swish # Activation function to use (relu/elu/gelu/silu).

  transformer:
    dropout: 0.
    expansion: 4 #4,  number of hidden units in FFN is expansion * embed_dim
    heads: 8 #12, must divide embed_dim
    layers: 13 #,12
    embed_dim: 128 #232
    num_embed_layers: 3 # Number of embedding layers.
    activation: gelu # Activation function to use (relu/elu/gelu/silu).

  part:
    particle_block_dropout: 0.1
    expansion: 4 #4,  number of hidden units in FFN is expansion * embed_dim
    heads: 8 #12, must divide embed_dim
    particle_block_layers: 11 #,12
    class_block_layers: 2
    embed_dim: 512 #256
    num_embed_layers: 3 # Number of embedding layers.
    interaction: False
    interaction_embedding_num_layers: 3 # Number of embedding layers. Last one must be heads
    interaction_embedding_layer_size: 64 # Size of the embedding layer.
    activation: gelu # Activation function to use (relu/elu/gelu/silu).

  depart:
    embed_dim: 512 #232
    num_embed_layers: 3 # Number of embedding layers.
    expansion: 4 # 4,  number of hidden units in FFN is expansion * embed_dim
    heads: 8 # 12, must divide embed_dim
    layers: 11 # 6,12
    class_layers: 2
    dropout: 0. # Dropout after FFN layer.
    layer_scale_init_value: 1.0e-04
    stochastic_depth_drop_rate: 0.2
    interaction: True
    interaction_embedding_num_layers: 3 # Number of embedding layers. Last one must be heads
    interaction_embedding_layer_size: 64 # Size of the embedding layer.
    activation: gelu # Activation function to use (relu/elu/gelu/silu).

  bdt:
    num_trees: 600 #  Number of trees in the forest.
    growing_strategy: BEST_FIRST_GLOBAL #  Growing strategy.
    max_depth: 5 #  Maximum depth of the tree.
    split_axis: SPARSE_OBLIQUE #  Split axis.
    shrinkage: 0.5 #learning rate
    min_examples: 5000 #  Minimum number of examples in a leaf.
    num_threads: 64 #  Number of threads to use.
    l2_regularization: 0.1 #  L2 regularization.

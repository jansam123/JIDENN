defaults:
  - data: data1
  - _self_

hydra:
  run:
    dir: ${params.base_logdir}/${now:%Y-%m-%d}__${now:%H_%M_%S}
  sweep:
    dir: ${params.base_logdir}/${now:%Y-%m-%d}__${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  job_logging:
    formatters:
      simple:
        format: "[${now:%Y-%m-%d}  ${now:%H:%M:%S}][%(levelname)s] - %(message)s"
  output_subdir: config

params:
  base_logdir: logs # Base log directory for all runs.
  model: basic_fc # Model to use, options:  transformer, basics_fc, BDT.
  epochs: 3 # Number of epochs.
  label_smoothing: 0 # Label smoothing.
  learning_rate: 0.001
  seed: 42 # Random seed.
  threads: # Maximum number of threads to use. 0 means all available.
  debug: False # Debug mode.
  logdir: ${hydra:runtime.output_dir} # Path to log directory, set automatically. CHANGE 'base_logdir' INSTEAD.
  # TODO
  decay_steps: 10
  weight_decay: 0.0

data:
  num_workers: 6 # Number of workers to use when loading data.
  toy: False # Use toy dataset.
  labels: # list of labels to use.
    - gluon
    - quark
  num_labels: 2 # Number of labels to use.
  cut: # Cut to apply to data.
  tttree_name: NOMINAL # Name of the TTree in the root file.
  gluon: 0
  quark: 1
  raw_gluon: 21
  raw_quarks: [1, 2, 3, 4, 5]
  raw_unknown: [-1, -999]
  reading_size: 1_000 # Number of events to load at a time.
  JZ_slices: [1, 2, 3, 4, 5]
  JZ_cut:
    - "jets_pt>10_000"
    - "jets_pt>30_000"
    - "jets_pt>100_000"
    - "jets_pt>300_000"
    - "jets_pt>600_000"
  JZ_weights: [0.34, 0.12, 0.29, 0.16, 0.09] # [83, 26, 71, 39, 22]/241
  draw_distribution: 10_000 # Length of the distribution to draw.
  #TODO:
  distribution_weights: True # Whether to use weights when drawing distributions.

dataset:
  take: # Length of data to use.
  batch_size: 256 # Batch size.
  validation_step: 200 # Validation every n batches.
  dev_size: 0.02 # Size of dev dataset.
  test_size: 0.02 # Size of test dataset.
  shuffle_buffer: 2_000 # Size of shuffler buffer.

preprocess:
  normalize: True # Normalize data.
  normalization_size: 10_000 # Size of normalization dataset.

basic_fc:
  hidden_layers: [512, 512, 512] # Hidden layer sizes.
  dropout: # Dropout after FC layers.
  rnn_dim: 128

transformer:
  warmup_steps: 100 # Number of steps to warmup for
  transformer_dropout: 0.2
  transformer_expansion: 4 #4,  number of hidden units in FFN is transformer_expansion * embed_dim
  transformer_heads: 8 #12, must divide embed_dim
  transformer_layers: 6 #6,12
  last_hidden_layer: 512 # Size of last fully connected layer.
  embed_dim: 256 #512

bdt:
  num_trees: 10
  growing_strategy: LOCAL
  max_depth: 3
  split_axis: SPARSE_OBLIQUE
  categorical_algorithm: CART

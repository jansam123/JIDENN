defaults:
  - data: data2
  - _self_

hydra:
  run:
    dir: ${params.logdir} #${params.base_logdir}/${now:%Y-%m-%d}__${now:%H_%M_%S}
  sweep:
    dir: ${params.logdir} #${params.base_logdir}/${now:%Y-%m-%d}__${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  job_logging:
    formatters:
      simple:
        format: "[${now:%Y-%m-%d}  ${now:%H:%M:%S}][%(levelname)s] - %(message)s"
  output_subdir: config

params:
  base_logdir: logs # Base log directory for all runs.
  model: transformer # Model to use, options:  transformer, basic_fc, BDT, highway.
  activation: gelu # Activation function to use (relu/elu/gelu/silu).
  epochs: 8 # Number of epochs.
  label_smoothing: 0 # Label smoothing.
  learning_rate: 0.001
  seed: 42 # Random seed.
  threads: # Maximum number of threads to use. 0 means all available.
  debug: False # Debug mode.
  logdir: ${params.base_logdir}/${now:%Y-%m-%d}__${now:%H_%M_%S} #${hydra:runtime.output_dir} # Path to log directory, set automatically. CHANGE 'base_logdir' INSTEAD.
  decay_steps: 80_000 #(10 * 50_000_000 / 1024) == (epochs * take / batch_size)
  weight_decay:

dataset:
  take: 10_000_000 # Length of data to use.
  batch_size: 1024 # Batch size.
  validation_step: 200 # Validation every n batches.
  dev_size: 0.1 # Size of dev dataset.
  test_size: 0.1 # Size of test dataset.
  shuffle_buffer: 1000 # Size of shuffler buffer.

preprocess:
  normalize: True # Normalize data. max size 146208768
  normalization_size: 400 # 100_000 is better Size of normalization dataset.
  draw_distribution: 5_000 # Size of the distribution to draw.
  min_max_path: #'min_max.csv'

basic_fc:
  layer_size: 512 # Hidden layer sizes.
  num_layers: 3
  dropout: # Dropout after FC layers.
  rnn_dim: 128

highway:
  layer_size: 1024 # Size of the highway layer.
  num_layers: 10 # Number of highway layers.
  dropout: # Dropout after highway layers.

transformer:
  warmup_steps: 100 # Number of steps to warmup for
  transformer_dropout: 0.1
  transformer_expansion: 4 #4,  number of hidden units in FFN is transformer_expansion * embed_dim
  transformer_heads: 8 #12, must divide embed_dim
  transformer_layers: 8 #,12
  embed_dim: 128 #512
  num_embed_layers: 3 # Number of embedding layers.

part:
  warmup_steps: 100 # Number of steps to warmup for
  particle_block_dropout: 0.1
  transformer_expansion: 4 #4,  number of hidden units in FFN is transformer_expansion * embed_dim
  transformer_heads: 8 #12, must divide embed_dim
  particle_block_layers: 8 #,12
  class_block_layers: 2
  embed_dim: 128 #512
  num_embed_layers: 3 # Number of embedding layers.

bdt:
  num_trees: 600 #  Number of trees in the forest.
  growing_strategy: LOCAL
  max_depth: 10 #  Maximum depth of the tree.
  split_axis: SPARSE_OBLIQUE
  categorical_algorithm: CART
  shrinkage: 0.5 #learning rate
  min_examples: 1_000
  num_threads: 40
  l2_regularization: 0.2

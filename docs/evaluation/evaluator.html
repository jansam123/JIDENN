<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>jidenn.evaluation.evaluator API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="canonical" href="http://jansam.wieno.sk/JIDENN/jidenn/evaluation/evaluator.html">
<link rel="icon" href="images/q_g_tagging.jpeg">
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>jidenn.evaluation.evaluator</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from typing import Optional, List, Dict, Tuple, Union, Callable, Literal
import tensorflow as tf
import tensorflow_addons as tfa
import numpy as np
import pandas as pd
import logging
import time
#
from jidenn.data.JIDENNDataset import JIDENNDataset, ROOTVariables
from jidenn.data.TrainInput import input_classes_lookup
from .evaluation_metrics import calculate_metrics
from .WorkingPoint import BinnedVariable
from multiprocessing import Pool


def add_score_to_dataset(dataset: JIDENNDataset,
                         score: np.ndarray,
                         score_name: str = &#39;score&#39;) -&gt; JIDENNDataset:
    &#34;&#34;&#34;Add a score array to a JIDENNDataset. Score could me any variable that is not part of the original dataset.
    It is important that the score array has the same length as the dataset. This is useful for adding the output of a
    ML model to the original dataset before the trining input is created.

    I/O Example:
    ```python
    example_input_element = {&#39;E&#39;: 1.0, &#39;eta&#39;: 1.0, &#39;pt&#39;: 1.0, &#39;phi&#39;: 1.0, &#39;label&#39;: 1, &#39;num&#39;: 1, &#39;event&#39;: 1, &#39;mu&#39;: 1.0, &#39;corr_mu&#39;: 1.0}
    example_output_element = {&#39;E&#39;: 1.0, &#39;eta&#39;: 1.0, &#39;pt&#39;: 1.0, &#39;phi&#39;: 1.0, &#39;label&#39;: 1, &#39;num&#39;: 1, &#39;event&#39;: 1, &#39;mu&#39;: 1.0, &#39;corr_mu&#39;: 1.0, &#39;score&#39;: 0.5}
    ```

    Args:
        dataset (JIDENNDataset): JIDENNDataset to add the score to.
        score (np.ndarray): Array containing the score values to add.
        score_name (str, optional): Name of the score variable inside the new dataset. Default is &#39;score&#39;.

    Returns:
        JIDENNDataset: JIDENNDataset with the score added. Its elements will have the same structure as the original,
        i.e. a dictionary with the same kay-value pairs with one additional key-value pair for the score `{score_name: score[i]}`.

    &#34;&#34;&#34;
    @tf.function
    def add_to_dict(data_label: Tuple[ROOTVariables, tf.Tensor], score: tf.Tensor) -&gt; Tuple[ROOTVariables, tf.Tensor]:
        data, label = data_label[0].copy(), data_label[1]
        data[score_name] = score
        return data, label

    score_dataset = tf.data.Dataset.from_tensor_slices(score)
    dataset = tf.data.Dataset.zip((dataset.dataset, score_dataset))
    dataset = dataset.map(add_to_dict)
    variables = list(dataset.element_spec[0].keys())

    return JIDENNDataset(variables).set_dataset(dataset, element_spec=dataset.element_spec)


def _calculate_metrics_in_bin(x):
    y, score_variable, threshold, validation_plotter = x
    inter, x = y
    if x.empty:
        return
    if len(x[&#39;label&#39;].unique()) &lt; 2:
        return
    if isinstance(threshold, BinnedVariable):
        threshold_val = threshold[x[&#39;bin&#39;].iloc[0]]
    else:
        threshold_val = threshold
    if validation_plotter is not None:
        validation_plotter(x)
    ret = calculate_metrics(x[&#39;label&#39;], x[score_variable], threshold=threshold_val)
    ret[&#39;num_events&#39;] = len(x)
    ret[&#39;bin&#39;] = inter
    return ret


def calculate_binned_metrics(df: pd.DataFrame,
                             binned_variable: str,
                             score_variable: str,
                             bins: Union[List[Union[float, int]], np.ndarray],
                             validation_plotter: Optional[Callable[[pd.DataFrame], None]] = None,
                             threshold: Union[BinnedVariable, float] = 0.5,
                             threads: Optional[int] = None) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Calculate metrics for a binary classification problem binned by a continuous variable.

    Example pd.DataFrame structure:
    ```python
    df = pd.DataFrame({&#39;label&#39;: [0, 1, 0, 1, 0, 1, 0, 1],
                        &#39;score&#39;: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, .7, .8],
                        &#39;jets_pt&#39;: [1, 2, 3, 4, 5, 6, 7, 8]})
    score_variable = &#39;score&#39;
    binning_variable = &#39;jets_pt&#39;
    bins = [2, 5, 7]
    binned_metrics = calculate_binned_metrics(df=df,
                                                binned_variable=binned_variable,
                                                score_variable=score_variable,
                                                bins=bins)
    print(binned_metrics)
    # Output:
    #    accuracy  signal_efficiency background_efficiency num_events           bin
    # 1  0.500000           0.500000              0.500000          2   &#39;(2.0, 5.0]&#39;
    # 2  0.666667           0.666667              0.666667          3   &#39;(5.0, 7.0]&#39;

    ```

    Args:
        df (pd.DataFrame): DataFrame containing columns `label`, `score_variable` and `binned_variable`.
        binned_variable (str): Name of the column containing the continuous variable to bin.
        score_variable (str): Name of the column containing the model scores.
        bins (Union[List[Union[float, int]], np.ndarray]): List or array of bin edges to use.
        validation_plotter (Callable[[pd.DataFrame], None], optional): Function to plot validation data
            for each bin (confusion matrix, ROC, score outputs histogram,...). Default is None.
        threshold (Union[pd.DataFrame, float], optional): Threshold value for the binary classification.
            If a DataFrame is provided, it should contain a &#39;bin&#39; column with string representation of 
            pd.Interval (e.g. &#39;m(0.5, 1.0]&#39;) and a column with the name specified in `threshold_name` containing
            the threshold values for each bin. Default is 0.5.
        threshold_name (str, optional): Name of the column containing the threshold values in the threshold
            DataFrame. Only used if a DataFrame is provided as the threshold argument. Default is None.

    Returns:
        pd.DataFrame: DataFrame containing the calculated metrics for each bin.

    &#34;&#34;&#34;

    df[&#39;bin&#39;] = pd.cut(df[binned_variable], bins=bins)

    grouped_metrics = df.groupby(&#39;bin&#39;)
    args = [(x, score_variable, threshold, validation_plotter) for x in grouped_metrics]

    if threads is not None and threads &gt; 1:
        with Pool(threads) as pool:
            metrics = pool.map(_calculate_metrics_in_bin, args)
    else:
        metrics = map(_calculate_metrics_in_bin, args)

    metrics = [x for x in metrics if x is not None]
    metrics = pd.DataFrame(metrics)
    return metrics


def evaluate_multiple_models(model_paths: List[str],
                             model_names: List[str],
                             dataset: JIDENNDataset,
                             model_input_name: List[Literal[&#39;highlevel&#39;,
                                                            &#39;highlevel_constituents&#39;,
                                                            &#39;constituents&#39;,
                                                            &#39;relative_constituents&#39;,
                                                            &#39;interaction_constituents&#39;]],
                             batch_size: int,
                             take: Optional[int] = None,
                             score_name: str = &#39;score&#39;,
                             log: Optional[logging.Logger] = None,
                             custom_objects: Optional[Dict[str, Callable]] = None,
                             distribution_drawer: Optional[Callable[[JIDENNDataset], None]] = None) -&gt; JIDENNDataset:
    &#34;&#34;&#34;Evaluate multiple Keras models on a JIDENNDataset. The explicit training inputs are created automatically
    from the JIDENNDataset. Input type for each model is deduced from the `model_input_name` argument. The order of 
    evaluation is **NOT** determined by the `model_names` argument. The iteration order is given by the unigue values
    in `model_input_name`, to reduce the number of times the dataset is prepared.

    Args:
        model_paths (List[str]): List of paths to the Keras model files. They will be loaded with
            `tf.keras.models.load_model(model_path, custom_objects=custom_objects)`.
        model_names (List[str]): List of names for each model.
        dataset (JIDENNDataset): JIDENNDataset to evaluate the models on and to add the scores to.
        model_input_name (List[str]): List of input names for each model. See `jidenn.data.TrainInput.input_classes_lookup`
            for options.
        batch_size (int): Batch size to use for the evaluation.
        take (int, optional): Number of events to evaluate. If not provided, all events will be used. Default is None.
        score_name (str, optional): Name of the score variable to add to the dataset. Default is &#39;score&#39;. For each model,
            the score will be added with the name `f&#39;{model_name}_{score_name}&#39;`.
        log (logging.Logger, optional): Logger to use for logging messages and evaluation/loading times. Default is None.
        custom_objects (Dict[str, Callable], optional): Dictionary of custom objects to use when loading the models.
            Passed to `tf.keras.models.load_model(model_path, custom_objects=custom_objects)`. Default is None.
        distribution_drawer (Callable[[JIDENNDataset], None], optional): Function to plot the data distribution of 
            the input variables which are automatically created with the `jidenn.data.TrainInput` class. Default is None.

    Returns:
        JIDENNDataset: JIDENNDataset with the scores added.

    &#34;&#34;&#34;

    # iterate over all input types to reduce the number of times the dataset is prepared
    log.info(f&#39;Batches will be of size: {batch_size}, total number of events: {take}&#39;) if log is not None else None
    for input_type in set(model_input_name):
        train_input_class = input_classes_lookup(input_type)
        train_input_class = train_input_class()
        model_input = tf.function(func=train_input_class)
        ds = dataset.create_train_input(model_input)
        if distribution_drawer is not None:
            log.info(f&#39;----- Drawing data distribution for: {input_type}&#39;) if log is not None else None
            distribution_drawer(ds)
        ds = ds.get_prepared_dataset(batch_size=batch_size, take=take)

        # iterate over all models with the same input type
        idxs = np.array(model_input_name) == input_type
        for model_path, model_name in zip(np.array(model_paths)[idxs], np.array(model_names)[idxs]):
            log.info(f&#39;----- Loading model: {model_name}&#39;) if log is not None else None
            start = time.time()
            model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)
            stop = time.time()
            log.info(f&#39;----- Loading model took: {stop-start:.2f} s&#39;) if log is not None else None
            log.info(f&#39;----- Predicting with model: {model_name}&#39;) if log is not None else None
            start = time.time()
            score = model.predict(ds).ravel()
            stop = time.time()
            log.info(f&#39;----- Predicting took: {stop-start:.2f} s&#39;) if log is not None else None
            dataset = add_score_to_dataset(dataset, score, f&#39;{model_name}_{score_name}&#39;)

    return dataset</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="jidenn.evaluation.evaluator.add_score_to_dataset"><code class="name flex">
<span>def <span class="ident">add_score_to_dataset</span></span>(<span>dataset: <a title="jidenn.data.JIDENNDataset.JIDENNDataset" href="../data/JIDENNDataset.html#jidenn.data.JIDENNDataset.JIDENNDataset">JIDENNDataset</a>, score: numpy.ndarray, score_name: str = 'score') ‑> <a title="jidenn.data.JIDENNDataset.JIDENNDataset" href="../data/JIDENNDataset.html#jidenn.data.JIDENNDataset.JIDENNDataset">JIDENNDataset</a></span>
</code></dt>
<dd>
<div class="desc"><p>Add a score array to a JIDENNDataset. Score could me any variable that is not part of the original dataset.
It is important that the score array has the same length as the dataset. This is useful for adding the output of a
ML model to the original dataset before the trining input is created.</p>
<p>I/O Example:</p>
<pre><code class="language-python">example_input_element = {'E': 1.0, 'eta': 1.0, 'pt': 1.0, 'phi': 1.0, 'label': 1, 'num': 1, 'event': 1, 'mu': 1.0, 'corr_mu': 1.0}
example_output_element = {'E': 1.0, 'eta': 1.0, 'pt': 1.0, 'phi': 1.0, 'label': 1, 'num': 1, 'event': 1, 'mu': 1.0, 'corr_mu': 1.0, 'score': 0.5}
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>JIDENNDataset</code></dt>
<dd>JIDENNDataset to add the score to.</dd>
<dt><strong><code>score</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Array containing the score values to add.</dd>
<dt><strong><code>score_name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of the score variable inside the new dataset. Default is 'score'.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>JIDENNDataset</code></dt>
<dd>JIDENNDataset with the score added. Its elements will have the same structure as the original,</dd>
</dl>
<p>i.e. a dictionary with the same kay-value pairs with one additional key-value pair for the score <code>{score_name: score[i]}</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_score_to_dataset(dataset: JIDENNDataset,
                         score: np.ndarray,
                         score_name: str = &#39;score&#39;) -&gt; JIDENNDataset:
    &#34;&#34;&#34;Add a score array to a JIDENNDataset. Score could me any variable that is not part of the original dataset.
    It is important that the score array has the same length as the dataset. This is useful for adding the output of a
    ML model to the original dataset before the trining input is created.

    I/O Example:
    ```python
    example_input_element = {&#39;E&#39;: 1.0, &#39;eta&#39;: 1.0, &#39;pt&#39;: 1.0, &#39;phi&#39;: 1.0, &#39;label&#39;: 1, &#39;num&#39;: 1, &#39;event&#39;: 1, &#39;mu&#39;: 1.0, &#39;corr_mu&#39;: 1.0}
    example_output_element = {&#39;E&#39;: 1.0, &#39;eta&#39;: 1.0, &#39;pt&#39;: 1.0, &#39;phi&#39;: 1.0, &#39;label&#39;: 1, &#39;num&#39;: 1, &#39;event&#39;: 1, &#39;mu&#39;: 1.0, &#39;corr_mu&#39;: 1.0, &#39;score&#39;: 0.5}
    ```

    Args:
        dataset (JIDENNDataset): JIDENNDataset to add the score to.
        score (np.ndarray): Array containing the score values to add.
        score_name (str, optional): Name of the score variable inside the new dataset. Default is &#39;score&#39;.

    Returns:
        JIDENNDataset: JIDENNDataset with the score added. Its elements will have the same structure as the original,
        i.e. a dictionary with the same kay-value pairs with one additional key-value pair for the score `{score_name: score[i]}`.

    &#34;&#34;&#34;
    @tf.function
    def add_to_dict(data_label: Tuple[ROOTVariables, tf.Tensor], score: tf.Tensor) -&gt; Tuple[ROOTVariables, tf.Tensor]:
        data, label = data_label[0].copy(), data_label[1]
        data[score_name] = score
        return data, label

    score_dataset = tf.data.Dataset.from_tensor_slices(score)
    dataset = tf.data.Dataset.zip((dataset.dataset, score_dataset))
    dataset = dataset.map(add_to_dict)
    variables = list(dataset.element_spec[0].keys())

    return JIDENNDataset(variables).set_dataset(dataset, element_spec=dataset.element_spec)</code></pre>
</details>
</dd>
<dt id="jidenn.evaluation.evaluator.calculate_binned_metrics"><code class="name flex">
<span>def <span class="ident">calculate_binned_metrics</span></span>(<span>df: pandas.core.frame.DataFrame, binned_variable: str, score_variable: str, bins: Union[List[Union[float, int]], numpy.ndarray], validation_plotter: Optional[Callable[[pandas.core.frame.DataFrame], None]] = None, threshold: Union[<a title="jidenn.evaluation.WorkingPoint.BinnedVariable" href="WorkingPoint.html#jidenn.evaluation.WorkingPoint.BinnedVariable">BinnedVariable</a>, float] = 0.5, threads: Optional[int] = None) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate metrics for a binary classification problem binned by a continuous variable.</p>
<p>Example pd.DataFrame structure:</p>
<pre><code class="language-python">df = pd.DataFrame({'label': [0, 1, 0, 1, 0, 1, 0, 1],
                    'score': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, .7, .8],
                    'jets_pt': [1, 2, 3, 4, 5, 6, 7, 8]})
score_variable = 'score'
binning_variable = 'jets_pt'
bins = [2, 5, 7]
binned_metrics = calculate_binned_metrics(df=df,
                                            binned_variable=binned_variable,
                                            score_variable=score_variable,
                                            bins=bins)
print(binned_metrics)
# Output:
#    accuracy  signal_efficiency background_efficiency num_events           bin
# 1  0.500000           0.500000              0.500000          2   '(2.0, 5.0]'
# 2  0.666667           0.666667              0.666667          3   '(5.0, 7.0]'

</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>DataFrame containing columns <code>label</code>, <code>score_variable</code> and <code>binned_variable</code>.</dd>
<dt><strong><code>binned_variable</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the column containing the continuous variable to bin.</dd>
<dt><strong><code>score_variable</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the column containing the model scores.</dd>
<dt><strong><code>bins</code></strong> :&ensp;<code>Union[List[Union[float, int]], np.ndarray]</code></dt>
<dd>List or array of bin edges to use.</dd>
<dt><strong><code>validation_plotter</code></strong> :&ensp;<code>Callable[[pd.DataFrame], None]</code>, optional</dt>
<dd>Function to plot validation data
for each bin (confusion matrix, ROC, score outputs histogram,&hellip;). Default is None.</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>Union[pd.DataFrame, float]</code>, optional</dt>
<dd>Threshold value for the binary classification.
If a DataFrame is provided, it should contain a 'bin' column with string representation of
pd.Interval (e.g. 'm(0.5, 1.0]') and a column with the name specified in <code>threshold_name</code> containing
the threshold values for each bin. Default is 0.5.</dd>
<dt><strong><code>threshold_name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of the column containing the threshold values in the threshold
DataFrame. Only used if a DataFrame is provided as the threshold argument. Default is None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>DataFrame containing the calculated metrics for each bin.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_binned_metrics(df: pd.DataFrame,
                             binned_variable: str,
                             score_variable: str,
                             bins: Union[List[Union[float, int]], np.ndarray],
                             validation_plotter: Optional[Callable[[pd.DataFrame], None]] = None,
                             threshold: Union[BinnedVariable, float] = 0.5,
                             threads: Optional[int] = None) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Calculate metrics for a binary classification problem binned by a continuous variable.

    Example pd.DataFrame structure:
    ```python
    df = pd.DataFrame({&#39;label&#39;: [0, 1, 0, 1, 0, 1, 0, 1],
                        &#39;score&#39;: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, .7, .8],
                        &#39;jets_pt&#39;: [1, 2, 3, 4, 5, 6, 7, 8]})
    score_variable = &#39;score&#39;
    binning_variable = &#39;jets_pt&#39;
    bins = [2, 5, 7]
    binned_metrics = calculate_binned_metrics(df=df,
                                                binned_variable=binned_variable,
                                                score_variable=score_variable,
                                                bins=bins)
    print(binned_metrics)
    # Output:
    #    accuracy  signal_efficiency background_efficiency num_events           bin
    # 1  0.500000           0.500000              0.500000          2   &#39;(2.0, 5.0]&#39;
    # 2  0.666667           0.666667              0.666667          3   &#39;(5.0, 7.0]&#39;

    ```

    Args:
        df (pd.DataFrame): DataFrame containing columns `label`, `score_variable` and `binned_variable`.
        binned_variable (str): Name of the column containing the continuous variable to bin.
        score_variable (str): Name of the column containing the model scores.
        bins (Union[List[Union[float, int]], np.ndarray]): List or array of bin edges to use.
        validation_plotter (Callable[[pd.DataFrame], None], optional): Function to plot validation data
            for each bin (confusion matrix, ROC, score outputs histogram,...). Default is None.
        threshold (Union[pd.DataFrame, float], optional): Threshold value for the binary classification.
            If a DataFrame is provided, it should contain a &#39;bin&#39; column with string representation of 
            pd.Interval (e.g. &#39;m(0.5, 1.0]&#39;) and a column with the name specified in `threshold_name` containing
            the threshold values for each bin. Default is 0.5.
        threshold_name (str, optional): Name of the column containing the threshold values in the threshold
            DataFrame. Only used if a DataFrame is provided as the threshold argument. Default is None.

    Returns:
        pd.DataFrame: DataFrame containing the calculated metrics for each bin.

    &#34;&#34;&#34;

    df[&#39;bin&#39;] = pd.cut(df[binned_variable], bins=bins)

    grouped_metrics = df.groupby(&#39;bin&#39;)
    args = [(x, score_variable, threshold, validation_plotter) for x in grouped_metrics]

    if threads is not None and threads &gt; 1:
        with Pool(threads) as pool:
            metrics = pool.map(_calculate_metrics_in_bin, args)
    else:
        metrics = map(_calculate_metrics_in_bin, args)

    metrics = [x for x in metrics if x is not None]
    metrics = pd.DataFrame(metrics)
    return metrics</code></pre>
</details>
</dd>
<dt id="jidenn.evaluation.evaluator.evaluate_multiple_models"><code class="name flex">
<span>def <span class="ident">evaluate_multiple_models</span></span>(<span>model_paths: List[str], model_names: List[str], dataset: <a title="jidenn.data.JIDENNDataset.JIDENNDataset" href="../data/JIDENNDataset.html#jidenn.data.JIDENNDataset.JIDENNDataset">JIDENNDataset</a>, model_input_name: List[Literal['highlevel', 'highlevel_constituents', 'constituents', 'relative_constituents', 'interaction_constituents']], batch_size: int, take: Optional[int] = None, score_name: str = 'score', log: Optional[logging.Logger] = None, custom_objects: Optional[Dict[str, Callable]] = None, distribution_drawer: Optional[Callable[[<a title="jidenn.data.JIDENNDataset.JIDENNDataset" href="../data/JIDENNDataset.html#jidenn.data.JIDENNDataset.JIDENNDataset">JIDENNDataset</a>], None]] = None) ‑> <a title="jidenn.data.JIDENNDataset.JIDENNDataset" href="../data/JIDENNDataset.html#jidenn.data.JIDENNDataset.JIDENNDataset">JIDENNDataset</a></span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate multiple Keras models on a JIDENNDataset. The explicit training inputs are created automatically
from the JIDENNDataset. Input type for each model is deduced from the <code>model_input_name</code> argument. The order of
evaluation is <strong>NOT</strong> determined by the <code>model_names</code> argument. The iteration order is given by the unigue values
in <code>model_input_name</code>, to reduce the number of times the dataset is prepared.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_paths</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>List of paths to the Keras model files. They will be loaded with
<code>tf.keras.models.load_model(model_path, custom_objects=custom_objects)</code>.</dd>
<dt><strong><code>model_names</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>List of names for each model.</dd>
<dt><strong><code>dataset</code></strong> :&ensp;<code>JIDENNDataset</code></dt>
<dd>JIDENNDataset to evaluate the models on and to add the scores to.</dd>
<dt><strong><code>model_input_name</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>List of input names for each model. See <code><a title="jidenn.data.TrainInput.input_classes_lookup" href="../data/TrainInput.html#jidenn.data.TrainInput.input_classes_lookup">input_classes_lookup()</a></code>
for options.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Batch size to use for the evaluation.</dd>
<dt><strong><code>take</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of events to evaluate. If not provided, all events will be used. Default is None.</dd>
<dt><strong><code>score_name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of the score variable to add to the dataset. Default is 'score'. For each model,
the score will be added with the name <code>f'{model_name}_{score_name}'</code>.</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>logging.Logger</code>, optional</dt>
<dd>Logger to use for logging messages and evaluation/loading times. Default is None.</dd>
<dt><strong><code>custom_objects</code></strong> :&ensp;<code>Dict[str, Callable]</code>, optional</dt>
<dd>Dictionary of custom objects to use when loading the models.
Passed to <code>tf.keras.models.load_model(model_path, custom_objects=custom_objects)</code>. Default is None.</dd>
<dt><strong><code>distribution_drawer</code></strong> :&ensp;<code>Callable[[JIDENNDataset], None]</code>, optional</dt>
<dd>Function to plot the data distribution of
the input variables which are automatically created with the <code><a title="jidenn.data.TrainInput" href="../data/TrainInput.html">jidenn.data.TrainInput</a></code> class. Default is None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>JIDENNDataset</code></dt>
<dd>JIDENNDataset with the scores added.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate_multiple_models(model_paths: List[str],
                             model_names: List[str],
                             dataset: JIDENNDataset,
                             model_input_name: List[Literal[&#39;highlevel&#39;,
                                                            &#39;highlevel_constituents&#39;,
                                                            &#39;constituents&#39;,
                                                            &#39;relative_constituents&#39;,
                                                            &#39;interaction_constituents&#39;]],
                             batch_size: int,
                             take: Optional[int] = None,
                             score_name: str = &#39;score&#39;,
                             log: Optional[logging.Logger] = None,
                             custom_objects: Optional[Dict[str, Callable]] = None,
                             distribution_drawer: Optional[Callable[[JIDENNDataset], None]] = None) -&gt; JIDENNDataset:
    &#34;&#34;&#34;Evaluate multiple Keras models on a JIDENNDataset. The explicit training inputs are created automatically
    from the JIDENNDataset. Input type for each model is deduced from the `model_input_name` argument. The order of 
    evaluation is **NOT** determined by the `model_names` argument. The iteration order is given by the unigue values
    in `model_input_name`, to reduce the number of times the dataset is prepared.

    Args:
        model_paths (List[str]): List of paths to the Keras model files. They will be loaded with
            `tf.keras.models.load_model(model_path, custom_objects=custom_objects)`.
        model_names (List[str]): List of names for each model.
        dataset (JIDENNDataset): JIDENNDataset to evaluate the models on and to add the scores to.
        model_input_name (List[str]): List of input names for each model. See `jidenn.data.TrainInput.input_classes_lookup`
            for options.
        batch_size (int): Batch size to use for the evaluation.
        take (int, optional): Number of events to evaluate. If not provided, all events will be used. Default is None.
        score_name (str, optional): Name of the score variable to add to the dataset. Default is &#39;score&#39;. For each model,
            the score will be added with the name `f&#39;{model_name}_{score_name}&#39;`.
        log (logging.Logger, optional): Logger to use for logging messages and evaluation/loading times. Default is None.
        custom_objects (Dict[str, Callable], optional): Dictionary of custom objects to use when loading the models.
            Passed to `tf.keras.models.load_model(model_path, custom_objects=custom_objects)`. Default is None.
        distribution_drawer (Callable[[JIDENNDataset], None], optional): Function to plot the data distribution of 
            the input variables which are automatically created with the `jidenn.data.TrainInput` class. Default is None.

    Returns:
        JIDENNDataset: JIDENNDataset with the scores added.

    &#34;&#34;&#34;

    # iterate over all input types to reduce the number of times the dataset is prepared
    log.info(f&#39;Batches will be of size: {batch_size}, total number of events: {take}&#39;) if log is not None else None
    for input_type in set(model_input_name):
        train_input_class = input_classes_lookup(input_type)
        train_input_class = train_input_class()
        model_input = tf.function(func=train_input_class)
        ds = dataset.create_train_input(model_input)
        if distribution_drawer is not None:
            log.info(f&#39;----- Drawing data distribution for: {input_type}&#39;) if log is not None else None
            distribution_drawer(ds)
        ds = ds.get_prepared_dataset(batch_size=batch_size, take=take)

        # iterate over all models with the same input type
        idxs = np.array(model_input_name) == input_type
        for model_path, model_name in zip(np.array(model_paths)[idxs], np.array(model_names)[idxs]):
            log.info(f&#39;----- Loading model: {model_name}&#39;) if log is not None else None
            start = time.time()
            model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)
            stop = time.time()
            log.info(f&#39;----- Loading model took: {stop-start:.2f} s&#39;) if log is not None else None
            log.info(f&#39;----- Predicting with model: {model_name}&#39;) if log is not None else None
            start = time.time()
            score = model.predict(ds).ravel()
            stop = time.time()
            log.info(f&#39;----- Predicting took: {stop-start:.2f} s&#39;) if log is not None else None
            dataset = add_score_to_dataset(dataset, score, f&#39;{model_name}_{score_name}&#39;)

    return dataset</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="JIDENN" href="https://jansam.wieno.sk/JIDENN/">
<img src="images/q_g_tagging.jpeg" alt=""> JIDENN
</a>
</header>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="jidenn.evaluation" href="index.html">jidenn.evaluation</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="jidenn.evaluation.evaluator.add_score_to_dataset" href="#jidenn.evaluation.evaluator.add_score_to_dataset">add_score_to_dataset</a></code></li>
<li><code><a title="jidenn.evaluation.evaluator.calculate_binned_metrics" href="#jidenn.evaluation.evaluator.calculate_binned_metrics">calculate_binned_metrics</a></code></li>
<li><code><a title="jidenn.evaluation.evaluator.evaluate_multiple_models" href="#jidenn.evaluation.evaluator.evaluate_multiple_models">evaluate_multiple_models</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>
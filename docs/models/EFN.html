<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>jidenn.models.EFN API documentation</title>
<meta name="description" content="Module implementing the Energy Flow Network (EFN) model.
See https://energyflow.network for original implementation or
the paper â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>jidenn.models.EFN</code></h1>
</header>
<section id="section-intro">
<p>Module implementing the Energy Flow Network (EFN) model.
See <a href="https://energyflow.network">https://energyflow.network</a> for original implementation or
the paper <a href="https://arxiv.org/abs/1810.05165">https://arxiv.org/abs/1810.05165</a> for more details.</p>
<p>The input are the jet consituents, i.e. the particles in the jet foorming an input shape of <code>(batch_size, num_particles, num_features)</code>.
The angular features are mapped with the Phi mapping, afterwards the energy variables are multiplied by the angular features
of each constituent separately and summed up. A F function is applied to the summed features forming an output
<span><span class="MathJax_Preview"> \mathrm{output} = F\left(\sum_i \pmb{E_i} \Phi{(\eta_i,\phi_i)}\right) </span><script type="math/tex; mode=display"> \mathrm{output} = F\left(\sum_i \pmb{E_i} \Phi{(\eta_i,\phi_i)}\right) </script></span></p>
<p>The F function is any function, which is applied to the summed features. The default is a fully-connected network.
The mapping Phi can be a fully-connected network or a convolutional network. The default is a fully-connected network.</p>
<p>The energy part of the input is not used in the model to maintain <strong>Infrared and Collinear safety</strong> of the model.</p>
<p><img alt="EFN_PFN" src="../../../diagrams/pfn_efn.png">
On the left is the PFN model, on the right the EFN model.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">r&#34;&#34;&#34;
Module implementing the Energy Flow Network (EFN) model.
See https://energyflow.network for original implementation or
the paper https://arxiv.org/abs/1810.05165 for more details.

The input are the jet consituents, i.e. the particles in the jet foorming an input shape of `(batch_size, num_particles, num_features)`.
The angular features are mapped with the Phi mapping, afterwards the energy variables are multiplied by the angular features 
of each constituent separately and summed up. A F function is applied to the summed features forming an output
$$ \mathrm{output} = F\left(\sum_i \pmb{E_i} \Phi{(\eta_i,\phi_i)}\right) $$

The F function is any function, which is applied to the summed features. The default is a fully-connected network.
The mapping Phi can be a fully-connected network or a convolutional network. The default is a fully-connected network.

The energy part of the input is not used in the model to maintain **Infrared and Collinear safety** of the model.

![EFN_PFN](../../../diagrams/pfn_efn.png)
On the left is the PFN model, on the right the EFN model.
&#34;&#34;&#34;
import tensorflow as tf
from typing import Optional, Callable, List, Literal, Tuple


class EinsumLayer(tf.keras.layers.Layer):
    &#34;&#34;&#34;
    This is needed to wrap the einsum operation, because the einsum operation produces an error when loded from a saved model with tf.keras.models.load_model.
    For more information see https://github.com/keras-team/keras/issues/15783.
    For more information about the einsum operation see https://www.tensorflow.org/api_docs/python/tf/einsum.

    Example:
    ```python
    x = EinsumLayer(&#34;bmhwf,bmoh-&gt;bmowf&#34;)((x1, x2))
    ```
    Args:
        equation (str): The equation to be used in the einsum operation.
    &#34;&#34;&#34;

    def __init__(self, equation: str):
        super().__init__()
        self.equation = equation

    def call(self, inputs: Tuple[tf.Tensor, tf.Tensor]) -&gt; tf.Tensor:
        &#34;&#34;&#34;Call the layer.

        Args:
            inputs (Tuple[tf.Tensor, tf.Tensor]): The inputs to the layer with shapes described in the equation.

        Returns:
            tf.Tensor: The output of the layer.
        &#34;&#34;&#34;
        return tf.einsum(self.equation, *inputs)

    def get_config(self):
        return {&#34;equation&#34;: self.equation}


class EFNModel(tf.keras.Model):
    &#34;&#34;&#34;The Energy Flow Network model.

    The input is expected to be a tensor of shape `(batch_size, num_particles, num_features=8)`,
    where the last 3 features are angular and the first 5 features are energy based.
    The second dimension is ragged, as the number of particles in each jet is not the same.
    See `jidenn.data.TrainInput.ConstituentVariables` for more details. 

    The model already contains the `tf.keras.layers.Input` layer, so it can be used as a standalone model.

    Args:
        input_shape (Tuple[None, int]): The shape of the input.
        Phi_sizes (List[int]): The sizes of the Phi layers.
        F_sizes (List[int]): The sizes of the F layers.
        output_layer (tf.keras.layers.Layer): The output layer.
        activation (Callable[[tf.Tensor], tf.Tensor]) The activation function for the Phi and F layers. 
        Phi_backbone (str, optional): The backbone of the Phi mapping. Options are &#34;cnn&#34; or &#34;fc&#34;. Defaults to &#34;fc&#34;.
            This argument is not in the config option, as the CNN backbone is not used in the paper and 
            might violate the Infrared and Collinear safety of the model. 
        batch_norm (bool, optional): Whether to use batch normalization. Defaults to False.
            This argument is not in the config option, as it is not used in the paper and 
            might violate the Infrared and Collinear safety of the model.
        F_dropout (float, optional): The dropout rate for the F layers. Defaults to None.
        preprocess (tf.keras.layers.Layer, optional): The preprocessing layer. Defaults to None.
    &#34;&#34;&#34;

    def __init__(self,
                 input_shape: Tuple[None, int],
                 Phi_sizes: List[int],
                 F_sizes: List[int],
                 output_layer: tf.keras.layers.Layer,
                 activation: Callable[[tf.Tensor], tf.Tensor],
                 Phi_backbone: Literal[&#34;cnn&#34;, &#34;fc&#34;] = &#34;fc&#34;,
                 batch_norm: bool = False,
                 Phi_dropout: Optional[float] = None,
                 F_dropout: Optional[float] = None,
                 preprocess: Optional[tf.keras.layers.Layer] = None):

        self.Phi_sizes, self.F_sizes = Phi_sizes, F_sizes
        self.Phi_dropout = Phi_dropout
        self.F_dropout = F_dropout
        self.activation = activation

        input = tf.keras.layers.Input(shape=input_shape, ragged=True)

        angular = input[:, :, 6:]
        energy = input[:, :, :6].to_tensor()

        row_lengths = angular.row_lengths()
        mask = tf.sequence_mask(row_lengths)
        angular = angular.to_tensor()

        if preprocess is not None:
            angular = preprocess(angular)

        if batch_norm:
            angular = tf.keras.layers.BatchNormalization()(angular)

        if Phi_backbone == &#34;cnn&#34;:
            angular = self.cnn_Phi(angular)
        elif Phi_backbone == &#34;fc&#34;:
            angular = self.fc_Phi(angular)
        else:
            raise ValueError(f&#34;backbone must be either &#39;cnn&#39; or &#39;fc&#39;, not {Phi_backbone}&#34;)

        angular = angular * tf.expand_dims(tf.cast(mask, tf.float32), -1)
        hidden = EinsumLayer(&#39;BPC,BPD-&gt;BCD&#39;)((angular, energy))
        hidden = tf.keras.layers.Flatten()(hidden)
        hidden = self.fc_F(hidden)
        output = output_layer(hidden)

        super().__init__(inputs=input, outputs=output)

    def cnn_Phi(self, inputs: tf.Tensor) -&gt; tf.Tensor:
        &#34;&#34;&#34;Convolutional Phi mapping.

        Args:
            inputs (tf.Tensor): The input tensor of shape `(batch_size, num_particles, num_features)`.

        Returns:
            tf.Tensor: The output tensor of shape `(batch_size, num_particles, Phi_sizes[-1])`.
        &#34;&#34;&#34;
        hidden = inputs
        for size in self.Phi_sizes:
            hidden = tf.keras.layers.Conv1D(size, 1)(hidden)
            hidden = tf.keras.layers.BatchNormalization()(hidden)
            hidden = tf.keras.layers.Activation(self.activation)(hidden)
            if self.Phi_dropout is not None:
                hidden = tf.keras.layers.Dropout(self.Phi_dropout)(hidden)
        return hidden

    def fc_Phi(self, inputs: tf.Tensor) -&gt; tf.Tensor:
        &#34;&#34;&#34;Fully connected Phi mapping.

        Args:
            inputs (tf.Tensor): The input tensor of shape `(batch_size, num_particles, num_features)`.

        Returns:
            tf.Tensor: The output tensor of shape `(batch_size, num_particles, Phi_sizes[-1])`.
        &#34;&#34;&#34;
        hidden = inputs
        for size in self.Phi_sizes:
            hidden = tf.keras.layers.Dense(size)(hidden)
            hidden = tf.keras.layers.Activation(self.activation)(hidden)
            if self.Phi_dropout is not None:
                hidden = tf.keras.layers.Dropout(self.Phi_dropout)(hidden)
        return hidden

    def fc_F(self, inputs: tf.Tensor) -&gt; tf.Tensor:
        &#34;&#34;&#34;Fully connected F mapping.

        Args:
            inputs (tf.Tensor): The input tensor of shape `(batch_size, num_features)`.

        Returns:
            tf.Tensor: The output tensor of shape `(batch_size, F_sizes[-1])`.
        &#34;&#34;&#34;
        hidden = inputs
        for size in self.F_sizes:
            hidden = tf.keras.layers.Dense(size)(hidden)
            hidden = tf.keras.layers.Activation(self.activation)(hidden)
            if self.F_dropout is not None:
                hidden = tf.keras.layers.Dropout(self.F_dropout)(hidden)
        return hidden</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="jidenn.models.EFN.EFNModel"><code class="flex name class">
<span>class <span class="ident">EFNModel</span></span>
<span>(</span><span>input_shape:Â Tuple[None,Â int], Phi_sizes:Â List[int], F_sizes:Â List[int], output_layer:Â keras.engine.base_layer.Layer, activation:Â Callable[[tensorflow.python.framework.ops.Tensor],Â tensorflow.python.framework.ops.Tensor], Phi_backbone:Â Literal['cnn',Â 'fc']Â =Â 'fc', batch_norm:Â boolÂ =Â False, Phi_dropout:Â Optional[float]Â =Â None, F_dropout:Â Optional[float]Â =Â None, preprocess:Â Optional[keras.engine.base_layer.Layer]Â =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>The Energy Flow Network model.</p>
<p>The input is expected to be a tensor of shape <code>(batch_size, num_particles, num_features=8)</code>,
where the last 3 features are angular and the first 5 features are energy based.
The second dimension is ragged, as the number of particles in each jet is not the same.
See <code><a title="jidenn.data.TrainInput.ConstituentVariables" href="../data/TrainInput.html#jidenn.data.TrainInput.ConstituentVariables">ConstituentVariables</a></code> for more details. </p>
<p>The model already contains the <code>tf.keras.layers.Input</code> layer, so it can be used as a standalone model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_shape</code></strong> :&ensp;<code>Tuple[None, int]</code></dt>
<dd>The shape of the input.</dd>
<dt><strong><code>Phi_sizes</code></strong> :&ensp;<code>List[int]</code></dt>
<dd>The sizes of the Phi layers.</dd>
<dt><strong><code>F_sizes</code></strong> :&ensp;<code>List[int]</code></dt>
<dd>The sizes of the F layers.</dd>
<dt><strong><code>output_layer</code></strong> :&ensp;<code>tf.keras.layers.Layer</code></dt>
<dd>The output layer.</dd>
<dt>activation (Callable[[tf.Tensor], tf.Tensor]) The activation function for the Phi and F layers.</dt>
<dt><strong><code>Phi_backbone</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The backbone of the Phi mapping. Options are "cnn" or "fc". Defaults to "fc".
This argument is not in the config option, as the CNN backbone is not used in the paper and
might violate the Infrared and Collinear safety of the model. </dd>
<dt><strong><code>batch_norm</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use batch normalization. Defaults to False.
This argument is not in the config option, as it is not used in the paper and
might violate the Infrared and Collinear safety of the model.</dd>
<dt><strong><code>F_dropout</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The dropout rate for the F layers. Defaults to None.</dd>
<dt><strong><code>preprocess</code></strong> :&ensp;<code>tf.keras.layers.Layer</code>, optional</dt>
<dd>The preprocessing layer. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EFNModel(tf.keras.Model):
    &#34;&#34;&#34;The Energy Flow Network model.

    The input is expected to be a tensor of shape `(batch_size, num_particles, num_features=8)`,
    where the last 3 features are angular and the first 5 features are energy based.
    The second dimension is ragged, as the number of particles in each jet is not the same.
    See `jidenn.data.TrainInput.ConstituentVariables` for more details. 

    The model already contains the `tf.keras.layers.Input` layer, so it can be used as a standalone model.

    Args:
        input_shape (Tuple[None, int]): The shape of the input.
        Phi_sizes (List[int]): The sizes of the Phi layers.
        F_sizes (List[int]): The sizes of the F layers.
        output_layer (tf.keras.layers.Layer): The output layer.
        activation (Callable[[tf.Tensor], tf.Tensor]) The activation function for the Phi and F layers. 
        Phi_backbone (str, optional): The backbone of the Phi mapping. Options are &#34;cnn&#34; or &#34;fc&#34;. Defaults to &#34;fc&#34;.
            This argument is not in the config option, as the CNN backbone is not used in the paper and 
            might violate the Infrared and Collinear safety of the model. 
        batch_norm (bool, optional): Whether to use batch normalization. Defaults to False.
            This argument is not in the config option, as it is not used in the paper and 
            might violate the Infrared and Collinear safety of the model.
        F_dropout (float, optional): The dropout rate for the F layers. Defaults to None.
        preprocess (tf.keras.layers.Layer, optional): The preprocessing layer. Defaults to None.
    &#34;&#34;&#34;

    def __init__(self,
                 input_shape: Tuple[None, int],
                 Phi_sizes: List[int],
                 F_sizes: List[int],
                 output_layer: tf.keras.layers.Layer,
                 activation: Callable[[tf.Tensor], tf.Tensor],
                 Phi_backbone: Literal[&#34;cnn&#34;, &#34;fc&#34;] = &#34;fc&#34;,
                 batch_norm: bool = False,
                 Phi_dropout: Optional[float] = None,
                 F_dropout: Optional[float] = None,
                 preprocess: Optional[tf.keras.layers.Layer] = None):

        self.Phi_sizes, self.F_sizes = Phi_sizes, F_sizes
        self.Phi_dropout = Phi_dropout
        self.F_dropout = F_dropout
        self.activation = activation

        input = tf.keras.layers.Input(shape=input_shape, ragged=True)

        angular = input[:, :, 6:]
        energy = input[:, :, :6].to_tensor()

        row_lengths = angular.row_lengths()
        mask = tf.sequence_mask(row_lengths)
        angular = angular.to_tensor()

        if preprocess is not None:
            angular = preprocess(angular)

        if batch_norm:
            angular = tf.keras.layers.BatchNormalization()(angular)

        if Phi_backbone == &#34;cnn&#34;:
            angular = self.cnn_Phi(angular)
        elif Phi_backbone == &#34;fc&#34;:
            angular = self.fc_Phi(angular)
        else:
            raise ValueError(f&#34;backbone must be either &#39;cnn&#39; or &#39;fc&#39;, not {Phi_backbone}&#34;)

        angular = angular * tf.expand_dims(tf.cast(mask, tf.float32), -1)
        hidden = EinsumLayer(&#39;BPC,BPD-&gt;BCD&#39;)((angular, energy))
        hidden = tf.keras.layers.Flatten()(hidden)
        hidden = self.fc_F(hidden)
        output = output_layer(hidden)

        super().__init__(inputs=input, outputs=output)

    def cnn_Phi(self, inputs: tf.Tensor) -&gt; tf.Tensor:
        &#34;&#34;&#34;Convolutional Phi mapping.

        Args:
            inputs (tf.Tensor): The input tensor of shape `(batch_size, num_particles, num_features)`.

        Returns:
            tf.Tensor: The output tensor of shape `(batch_size, num_particles, Phi_sizes[-1])`.
        &#34;&#34;&#34;
        hidden = inputs
        for size in self.Phi_sizes:
            hidden = tf.keras.layers.Conv1D(size, 1)(hidden)
            hidden = tf.keras.layers.BatchNormalization()(hidden)
            hidden = tf.keras.layers.Activation(self.activation)(hidden)
            if self.Phi_dropout is not None:
                hidden = tf.keras.layers.Dropout(self.Phi_dropout)(hidden)
        return hidden

    def fc_Phi(self, inputs: tf.Tensor) -&gt; tf.Tensor:
        &#34;&#34;&#34;Fully connected Phi mapping.

        Args:
            inputs (tf.Tensor): The input tensor of shape `(batch_size, num_particles, num_features)`.

        Returns:
            tf.Tensor: The output tensor of shape `(batch_size, num_particles, Phi_sizes[-1])`.
        &#34;&#34;&#34;
        hidden = inputs
        for size in self.Phi_sizes:
            hidden = tf.keras.layers.Dense(size)(hidden)
            hidden = tf.keras.layers.Activation(self.activation)(hidden)
            if self.Phi_dropout is not None:
                hidden = tf.keras.layers.Dropout(self.Phi_dropout)(hidden)
        return hidden

    def fc_F(self, inputs: tf.Tensor) -&gt; tf.Tensor:
        &#34;&#34;&#34;Fully connected F mapping.

        Args:
            inputs (tf.Tensor): The input tensor of shape `(batch_size, num_features)`.

        Returns:
            tf.Tensor: The output tensor of shape `(batch_size, F_sizes[-1])`.
        &#34;&#34;&#34;
        hidden = inputs
        for size in self.F_sizes:
            hidden = tf.keras.layers.Dense(size)(hidden)
            hidden = tf.keras.layers.Activation(self.activation)(hidden)
            if self.F_dropout is not None:
                hidden = tf.keras.layers.Dropout(self.F_dropout)(hidden)
        return hidden</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.training.Model</li>
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
<li>keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="jidenn.models.EFN.EFNModel.cnn_Phi"><code class="name flex">
<span>def <span class="ident">cnn_Phi</span></span>(<span>self, inputs:Â tensorflow.python.framework.ops.Tensor) â€‘>Â tensorflow.python.framework.ops.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Convolutional Phi mapping.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>The input tensor of shape <code>(batch_size, num_particles, num_features)</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tf.Tensor</code></dt>
<dd>The output tensor of shape <code>(batch_size, num_particles, Phi_sizes[-1])</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cnn_Phi(self, inputs: tf.Tensor) -&gt; tf.Tensor:
    &#34;&#34;&#34;Convolutional Phi mapping.

    Args:
        inputs (tf.Tensor): The input tensor of shape `(batch_size, num_particles, num_features)`.

    Returns:
        tf.Tensor: The output tensor of shape `(batch_size, num_particles, Phi_sizes[-1])`.
    &#34;&#34;&#34;
    hidden = inputs
    for size in self.Phi_sizes:
        hidden = tf.keras.layers.Conv1D(size, 1)(hidden)
        hidden = tf.keras.layers.BatchNormalization()(hidden)
        hidden = tf.keras.layers.Activation(self.activation)(hidden)
        if self.Phi_dropout is not None:
            hidden = tf.keras.layers.Dropout(self.Phi_dropout)(hidden)
    return hidden</code></pre>
</details>
</dd>
<dt id="jidenn.models.EFN.EFNModel.fc_F"><code class="name flex">
<span>def <span class="ident">fc_F</span></span>(<span>self, inputs:Â tensorflow.python.framework.ops.Tensor) â€‘>Â tensorflow.python.framework.ops.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Fully connected F mapping.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>The input tensor of shape <code>(batch_size, num_features)</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tf.Tensor</code></dt>
<dd>The output tensor of shape <code>(batch_size, F_sizes[-1])</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fc_F(self, inputs: tf.Tensor) -&gt; tf.Tensor:
    &#34;&#34;&#34;Fully connected F mapping.

    Args:
        inputs (tf.Tensor): The input tensor of shape `(batch_size, num_features)`.

    Returns:
        tf.Tensor: The output tensor of shape `(batch_size, F_sizes[-1])`.
    &#34;&#34;&#34;
    hidden = inputs
    for size in self.F_sizes:
        hidden = tf.keras.layers.Dense(size)(hidden)
        hidden = tf.keras.layers.Activation(self.activation)(hidden)
        if self.F_dropout is not None:
            hidden = tf.keras.layers.Dropout(self.F_dropout)(hidden)
    return hidden</code></pre>
</details>
</dd>
<dt id="jidenn.models.EFN.EFNModel.fc_Phi"><code class="name flex">
<span>def <span class="ident">fc_Phi</span></span>(<span>self, inputs:Â tensorflow.python.framework.ops.Tensor) â€‘>Â tensorflow.python.framework.ops.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Fully connected Phi mapping.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>The input tensor of shape <code>(batch_size, num_particles, num_features)</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tf.Tensor</code></dt>
<dd>The output tensor of shape <code>(batch_size, num_particles, Phi_sizes[-1])</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fc_Phi(self, inputs: tf.Tensor) -&gt; tf.Tensor:
    &#34;&#34;&#34;Fully connected Phi mapping.

    Args:
        inputs (tf.Tensor): The input tensor of shape `(batch_size, num_particles, num_features)`.

    Returns:
        tf.Tensor: The output tensor of shape `(batch_size, num_particles, Phi_sizes[-1])`.
    &#34;&#34;&#34;
    hidden = inputs
    for size in self.Phi_sizes:
        hidden = tf.keras.layers.Dense(size)(hidden)
        hidden = tf.keras.layers.Activation(self.activation)(hidden)
        if self.Phi_dropout is not None:
            hidden = tf.keras.layers.Dropout(self.Phi_dropout)(hidden)
    return hidden</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="jidenn.models.EFN.EinsumLayer"><code class="flex name class">
<span>class <span class="ident">EinsumLayer</span></span>
<span>(</span><span>equation:Â str)</span>
</code></dt>
<dd>
<div class="desc"><p>This is needed to wrap the einsum operation, because the einsum operation produces an error when loded from a saved model with tf.keras.models.load_model.
For more information see <a href="https://github.com/keras-team/keras/issues/15783.">https://github.com/keras-team/keras/issues/15783.</a>
For more information about the einsum operation see <a href="https://www.tensorflow.org/api_docs/python/tf/einsum.">https://www.tensorflow.org/api_docs/python/tf/einsum.</a></p>
<p>Example:</p>
<pre><code class="language-python">x = EinsumLayer(&quot;bmhwf,bmoh-&gt;bmowf&quot;)((x1, x2))
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>equation</code></strong> :&ensp;<code>str</code></dt>
<dd>The equation to be used in the einsum operation.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EinsumLayer(tf.keras.layers.Layer):
    &#34;&#34;&#34;
    This is needed to wrap the einsum operation, because the einsum operation produces an error when loded from a saved model with tf.keras.models.load_model.
    For more information see https://github.com/keras-team/keras/issues/15783.
    For more information about the einsum operation see https://www.tensorflow.org/api_docs/python/tf/einsum.

    Example:
    ```python
    x = EinsumLayer(&#34;bmhwf,bmoh-&gt;bmowf&#34;)((x1, x2))
    ```
    Args:
        equation (str): The equation to be used in the einsum operation.
    &#34;&#34;&#34;

    def __init__(self, equation: str):
        super().__init__()
        self.equation = equation

    def call(self, inputs: Tuple[tf.Tensor, tf.Tensor]) -&gt; tf.Tensor:
        &#34;&#34;&#34;Call the layer.

        Args:
            inputs (Tuple[tf.Tensor, tf.Tensor]): The inputs to the layer with shapes described in the equation.

        Returns:
            tf.Tensor: The output of the layer.
        &#34;&#34;&#34;
        return tf.einsum(self.equation, *inputs)

    def get_config(self):
        return {&#34;equation&#34;: self.equation}</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="jidenn.models.EFN.EinsumLayer.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs:Â Tuple[tensorflow.python.framework.ops.Tensor,Â tensorflow.python.framework.ops.Tensor]) â€‘>Â tensorflow.python.framework.ops.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Call the layer.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong> :&ensp;<code>Tuple[tf.Tensor, tf.Tensor]</code></dt>
<dd>The inputs to the layer with shapes described in the equation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tf.Tensor</code></dt>
<dd>The output of the layer.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs: Tuple[tf.Tensor, tf.Tensor]) -&gt; tf.Tensor:
    &#34;&#34;&#34;Call the layer.

    Args:
        inputs (Tuple[tf.Tensor, tf.Tensor]): The inputs to the layer with shapes described in the equation.

    Returns:
        tf.Tensor: The output of the layer.
    &#34;&#34;&#34;
    return tf.einsum(self.equation, *inputs)</code></pre>
</details>
</dd>
<dt id="jidenn.models.EFN.EinsumLayer.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of
dict every time it is called. The callers should make a copy of the
returned dict if they want to modify it.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    return {&#34;equation&#34;: self.equation}</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="jidenn.models" href="index.html">jidenn.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="jidenn.models.EFN.EFNModel" href="#jidenn.models.EFN.EFNModel">EFNModel</a></code></h4>
<ul class="">
<li><code><a title="jidenn.models.EFN.EFNModel.cnn_Phi" href="#jidenn.models.EFN.EFNModel.cnn_Phi">cnn_Phi</a></code></li>
<li><code><a title="jidenn.models.EFN.EFNModel.fc_F" href="#jidenn.models.EFN.EFNModel.fc_F">fc_F</a></code></li>
<li><code><a title="jidenn.models.EFN.EFNModel.fc_Phi" href="#jidenn.models.EFN.EFNModel.fc_Phi">fc_Phi</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="jidenn.models.EFN.EinsumLayer" href="#jidenn.models.EFN.EinsumLayer">EinsumLayer</a></code></h4>
<ul class="">
<li><code><a title="jidenn.models.EFN.EinsumLayer.call" href="#jidenn.models.EFN.EinsumLayer.call">call</a></code></li>
<li><code><a title="jidenn.models.EFN.EinsumLayer.get_config" href="#jidenn.models.EFN.EinsumLayer.get_config">get_config</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>
<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>jidenn.models.Transformer API documentation</title>
<meta name="description" content="Implementation of the Transformer model from the paper &#34;Attention is all you need,&#34; see https://arxiv.org/abs/1706.03762 …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="canonical" href="http://jansam.wieno.sk/JIDENN/jidenn/models/Transformer.html">
<link rel="icon" href="images/q_g_tagging.jpeg">
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>jidenn.models.Transformer</code></h1>
</header>
<section id="section-intro">
<p>Implementation of the Transformer model from the paper "Attention is all you need," see <a href="https://arxiv.org/abs/1706.03762.">https://arxiv.org/abs/1706.03762.</a></p>
<p>The model is a stack of self-attention blocks, each of which contains a multi-head self-attention layer and a feed-forward network.
The input features are embedded into a vector of size <code>dim</code>, which is then passed through the self-attention blocks.</p>
<p><img alt="Transformer" src="images/transformer.png">
<img alt="Transformer" src="images/transformer_layers.png"></p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Implementation of the Transformer model from the paper &#34;Attention is all you need,&#34; see https://arxiv.org/abs/1706.03762.

The model is a stack of self-attention blocks, each of which contains a multi-head self-attention layer and a feed-forward network.
The input features are embedded into a vector of size `dim`, which is then passed through the self-attention blocks.

![Transformer](images/transformer.png)
![Transformer](images/transformer_layers.png)

&#34;&#34;&#34;
import tensorflow as tf
from typing import Callable, Tuple, Optional


class FFN(tf.keras.layers.Layer):
    &#34;&#34;&#34;Feed-forward network 

    Args:
        dim (int): dimension of the input and output
        expansion (int): expansion factor of the hidden layer, i.e. the hidden layer has size `dim * expansion`
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function
        dropout (float, optional): dropout rate. Defaults to None.
    &#34;&#34;&#34;

    def __init__(self, dim: int, expansion: int, activation: Callable[[tf.Tensor], tf.Tensor], dropout: Optional[float] = None):
        super().__init__()
        self.dim, self.expansion, self.activation, self.dropout = dim, expansion, activation, dropout

        self.wide_dense = tf.keras.layers.Dense(dim * expansion, activation=activation)
        self.dense = tf.keras.layers.Dense(dim, activation=None)
        self.layer_dropout = tf.keras.layers.Dropout(dropout)

    def get_config(self):
        config = super(FFN, self).get_config()
        config.update({&#34;dim&#34;: self.dim, &#34;expansion&#34;: self.expansion,
                      &#34;activation&#34;: self.activation, &#34;dropout&#34;: self.dropout})
        return config

    def call(self, inputs: tf.Tensor) -&gt; tf.Tensor:
        &#34;&#34;&#34;Forward pass of the feed-forward network

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
        &#34;&#34;&#34;
        output = self.wide_dense(inputs)
        output = self.dense(output)
        output = self.layer_dropout(output)
        output = self.layer_dropout(output)
        return output


class MultiheadSelfAttention(tf.keras.layers.Layer):
    &#34;&#34;&#34;Multi-head self-attention layer
    This layer is a wrapper around the `tf.keras.layers.MultiHeadAttention` layer, 
    to fix the key, value, and query to be the same.

    Args:
        dim (int): dimension of the input and output
        heads (int): number of heads

    &#34;&#34;&#34;

    def __init__(self, dim: int, heads: int):
        super().__init__()
        self.dim, self.heads = dim, heads
        self.mha = tf.keras.layers.MultiHeadAttention(key_dim=dim // heads, num_heads=heads)

    def get_config(self):
        config = super(MultiheadSelfAttention, self).get_config()
        config.update({&#34;dim&#34;: self.dim, &#34;heads&#34;: self.heads})
        return config

    def call(self, inputs: tf.Tensor, mask: tf.Tensor) -&gt; tf.Tensor:
        &#34;&#34;&#34;Forward pass of the multi-head self-attention layer

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
            mask (tf.Tensor): mask tensor of shape `(batch_size, num_particles, num_particles)`
                This mask is used to mask out the attention of padding particles, generated when
                tf.RaggedTensor is converted to tf.Tensor.

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
        &#34;&#34;&#34;
        output = self.mha(query=inputs, value=inputs, key=inputs, attention_mask=mask)
        return output


class SelfAttentionBlock(tf.keras.layers.Layer):
    &#34;&#34;&#34;Self-attention block.
    It contains a multi-head self-attention layer and a feed-forward network with residual connections
    and layer normalizations.

    Args:
        dim (int): dimension of the input and output
        heads (int): number of heads
        expansion (int): expansion factor of the hidden layer, i.e. the hidden layer has size `dim * expansion`
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function
        dropout (float, optional): dropout rate. Defaults to None.
    &#34;&#34;&#34;

    def __init__(self, dim: int, heads: int, expansion: int, activation: Callable[[tf.Tensor], tf.Tensor], dropout: Optional[float] = None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.dim, self.heads, self.dropout = dim, heads, dropout
        self.expansion, self.activation = expansion, activation
        self.mhsa_ln = tf.keras.layers.LayerNormalization()
        self.mhsa = MultiheadSelfAttention(dim, heads)
        self.mhsa_dropout = tf.keras.layers.Dropout(dropout)

        self.ffn_ln = tf.keras.layers.LayerNormalization()
        self.ffn = FFN(dim, expansion, activation, dropout)

    def call(self, inputs: tf.Tensor, mask: tf.Tensor) -&gt; tf.Tensor:
        &#34;&#34;&#34;Forward pass of the self-attention block

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
            mask (tf.Tensor): mask tensor of shape `(batch_size, num_particles, num_particles)`
                This mask is used to mask out the attention of padding particles, generated when
                tf.RaggedTensor is converted to tf.Tensor.
        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
        &#34;&#34;&#34;
        attented = self.mhsa_ln(inputs)
        attented = self.mhsa(attented, mask)
        attented = self.mhsa_dropout(attented)
        attented = attented + inputs

        ffned = self.ffn_ln(attented)
        ffned = self.ffn(ffned)
        output = ffned + attented
        return output


class Transformer(tf.keras.layers.Layer):
    &#34;&#34;&#34;Pure Transformer layers without embedding and output layers.

    It also creates the class token, which is used to encode the global information of the input,
    by concatenating the class token to the input.

    Args:
        layers (int): number of Self-Attention layers
        dim (int): dimension of the input and output
        expansion (int): expansion factor of the hidden layer, i.e. the hidden layer has size `dim * expansion`
        heads (int): number of heads
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function
        dropout (float, optional): dropout rate. Defaults to None.

    &#34;&#34;&#34;

    def __init__(self, layers: int, dim: int, expansion: int, heads: int, activation: Callable[[tf.Tensor], tf.Tensor], dropout: Optional[float] = None):
        # Make sure `dim` is even.
        assert dim % 2 == 0

        super().__init__()
        self.layers, self.dim, self.expansion, self.heads, self.dropout, self.activation = layers, dim, expansion, heads, dropout, activation
        self.class_token = tf.Variable(initial_value=tf.random.truncated_normal(
            (1, 1, dim), stddev=0.02), trainable=True)
        self.sa_layers = [SelfAttentionBlock(dim, heads, expansion, activation, dropout) for _ in range(layers)]

    def get_config(self):
        config = super(Transformer, self).get_config()
        config.update({name: getattr(self, name)
                      for name in [&#34;layers&#34;, &#34;dim&#34;, &#34;expansion&#34;, &#34;heads&#34;, &#34;dropout&#34;, &#34;activation&#34;]})
        return config

    def call(self, inputs: tf.Tensor, mask: tf.Tensor) -&gt; tf.Tensor:
        &#34;&#34;&#34;Forward pass of the Transformer layers

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
            mask (tf.Tensor): mask tensor of shape `(batch_size, num_particles)`.
                From the mask, a mask tensor of shape `(batch_size, num_particles, num_particles)`
                is calculated, which is used to mask out the attention of padding particles, generated when
                `tf.RaggedTensor` is converted to `tf.Tensor`. 
        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
        &#34;&#34;&#34;
        mask = mask[:, tf.newaxis, :] &amp; mask[:, :, tf.newaxis]
        class_tokens = tf.tile(self.class_token, [tf.shape(inputs)[0], 1, 1])
        hidden = tf.concat([class_tokens, inputs], axis=1)
        for sa_block in self.sa_layers:
            hidden = sa_block(hidden, mask)
        return hidden


class FCEmbedding(tf.keras.layers.Layer):
    &#34;&#34;&#34;Embedding layer as a series of fully-connected layers.

    Args:
        embed_dim (int): dimension of the embedding
        embed_layers (int): number of fully-connected layers
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function
    &#34;&#34;&#34;

    def __init__(self, embed_dim: int, embed_layers: int, activation: Callable[[tf.Tensor], tf.Tensor]):
        super().__init__()
        self.embedding_dim, self.activation, self.num_embeding_layers = embed_dim, activation, embed_layers
        self.layers = [tf.keras.layers.Dense(self.embedding_dim, activation=self.activation)
                       for _ in range(self.num_embeding_layers)]

    def get_config(self):
        config = super(FCEmbedding, self).get_config()
        config.update({name: getattr(self, name) for name in [&#34;embedding_dim&#34;, &#34;num_embeding_layers&#34;, &#34;activation&#34;]})
        return config

    def call(self, inputs):
        &#34;&#34;&#34;Forward pass of the embedding layer

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, num_features)`

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, embed_dim)`
        &#34;&#34;&#34;
        hidden = inputs
        for layer in self.layers:
            hidden = layer(hidden)
        return hidden


class TransformerModel(tf.keras.Model):
    &#34;&#34;&#34;Transformer model with embedding and output layers.

    The model already contains the `tf.keras.layers.Input` layer, so it can be used as a standalone model.

    The input tensor is first passed through the embedding layer, then the Transformer layers, and finally the output layer.
    If the preprocessing layer is not None, the input tensor is first passed through the preprocessing layer before the embedding layer.
    The input to the output layer is the extracted class token.  

    Args:
        input_shape (Tuple[int]): shape of the input
        embed_dim (int): dimension of the embedding
        embed_layers (int): number of fully-connected layers in the embedding
        self_attn_layers (int): number of Self-Attention layers
        expansion (int): expansion factor of the hidden layer, i.e. the hidden layer has size `dim * expansion`
        heads (int): number of heads
        dropout (float, optional): dropout rate. Defaults to None.
        output_layer (tf.keras.layers.Layer): output layer
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function used in all the layers
        preprocess (tf.keras.layers.Layer, optional): preprocessing layer. Defaults to None.
    &#34;&#34;&#34;

    def __init__(self,
                 input_shape: Tuple[None, int],
                 embed_dim: int,
                 embed_layers: int,
                 self_attn_layers: int,
                 expansion: int,
                 heads: int,
                 dropout: float,
                 output_layer: tf.keras.layers.Layer,
                 activation: Callable[[tf.Tensor], tf.Tensor],
                 preprocess: Optional[tf.keras.layers.Layer] = None):

        input = tf.keras.layers.Input(shape=input_shape, ragged=True)

        row_lengths = input.row_lengths()
        hidden = input.to_tensor()

        if preprocess is not None:
            hidden = preprocess(hidden)

        hidden = FCEmbedding(embed_dim, embed_layers, activation)(hidden)
        row_lengths += 1

        transformed = Transformer(self_attn_layers, embed_dim, expansion,
                                  heads, activation, dropout)(hidden, mask=tf.sequence_mask(row_lengths))

        transformed = tf.keras.layers.LayerNormalization()(transformed[:, 0, :])
        output = output_layer(transformed)

        super().__init__(inputs=input, outputs=output)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="jidenn.models.Transformer.FCEmbedding"><code class="flex name class">
<span>class <span class="ident">FCEmbedding</span></span>
<span>(</span><span>embed_dim: int, embed_layers: int, activation: Callable[[tensorflow.python.framework.ops.Tensor], tensorflow.python.framework.ops.Tensor])</span>
</code></dt>
<dd>
<div class="desc"><p>Embedding layer as a series of fully-connected layers.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>embed_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>dimension of the embedding</dd>
<dt><strong><code>embed_layers</code></strong> :&ensp;<code>int</code></dt>
<dd>number of fully-connected layers</dd>
</dl>
<p>activation (Callable[[tf.Tensor], tf.Tensor]) activation function</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FCEmbedding(tf.keras.layers.Layer):
    &#34;&#34;&#34;Embedding layer as a series of fully-connected layers.

    Args:
        embed_dim (int): dimension of the embedding
        embed_layers (int): number of fully-connected layers
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function
    &#34;&#34;&#34;

    def __init__(self, embed_dim: int, embed_layers: int, activation: Callable[[tf.Tensor], tf.Tensor]):
        super().__init__()
        self.embedding_dim, self.activation, self.num_embeding_layers = embed_dim, activation, embed_layers
        self.layers = [tf.keras.layers.Dense(self.embedding_dim, activation=self.activation)
                       for _ in range(self.num_embeding_layers)]

    def get_config(self):
        config = super(FCEmbedding, self).get_config()
        config.update({name: getattr(self, name) for name in [&#34;embedding_dim&#34;, &#34;num_embeding_layers&#34;, &#34;activation&#34;]})
        return config

    def call(self, inputs):
        &#34;&#34;&#34;Forward pass of the embedding layer

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, num_features)`

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, embed_dim)`
        &#34;&#34;&#34;
        hidden = inputs
        for layer in self.layers:
            hidden = layer(hidden)
        return hidden</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="jidenn.models.Transformer.FCEmbedding.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs)</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass of the embedding layer</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>input tensor of shape <code>(batch_size, num_particles, num_features)</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tf.Tensor</code></dt>
<dd>output tensor of shape <code>(batch_size, num_particles, embed_dim)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs):
    &#34;&#34;&#34;Forward pass of the embedding layer

    Args:
        inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, num_features)`

    Returns:
        tf.Tensor: output tensor of shape `(batch_size, num_particles, embed_dim)`
    &#34;&#34;&#34;
    hidden = inputs
    for layer in self.layers:
        hidden = layer(hidden)
    return hidden</code></pre>
</details>
</dd>
<dt id="jidenn.models.Transformer.FCEmbedding.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of
dict every time it is called. The callers should make a copy of the
returned dict if they want to modify it.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = super(FCEmbedding, self).get_config()
    config.update({name: getattr(self, name) for name in [&#34;embedding_dim&#34;, &#34;num_embeding_layers&#34;, &#34;activation&#34;]})
    return config</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="jidenn.models.Transformer.FFN"><code class="flex name class">
<span>class <span class="ident">FFN</span></span>
<span>(</span><span>dim: int, expansion: int, activation: Callable[[tensorflow.python.framework.ops.Tensor], tensorflow.python.framework.ops.Tensor], dropout: Optional[float] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Feed-forward network </p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>dimension of the input and output</dd>
<dt><strong><code>expansion</code></strong> :&ensp;<code>int</code></dt>
<dd>expansion factor of the hidden layer, i.e. the hidden layer has size <code>dim * expansion</code></dd>
<dt>activation (Callable[[tf.Tensor], tf.Tensor]) activation function</dt>
<dt><strong><code>dropout</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>dropout rate. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FFN(tf.keras.layers.Layer):
    &#34;&#34;&#34;Feed-forward network 

    Args:
        dim (int): dimension of the input and output
        expansion (int): expansion factor of the hidden layer, i.e. the hidden layer has size `dim * expansion`
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function
        dropout (float, optional): dropout rate. Defaults to None.
    &#34;&#34;&#34;

    def __init__(self, dim: int, expansion: int, activation: Callable[[tf.Tensor], tf.Tensor], dropout: Optional[float] = None):
        super().__init__()
        self.dim, self.expansion, self.activation, self.dropout = dim, expansion, activation, dropout

        self.wide_dense = tf.keras.layers.Dense(dim * expansion, activation=activation)
        self.dense = tf.keras.layers.Dense(dim, activation=None)
        self.layer_dropout = tf.keras.layers.Dropout(dropout)

    def get_config(self):
        config = super(FFN, self).get_config()
        config.update({&#34;dim&#34;: self.dim, &#34;expansion&#34;: self.expansion,
                      &#34;activation&#34;: self.activation, &#34;dropout&#34;: self.dropout})
        return config

    def call(self, inputs: tf.Tensor) -&gt; tf.Tensor:
        &#34;&#34;&#34;Forward pass of the feed-forward network

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
        &#34;&#34;&#34;
        output = self.wide_dense(inputs)
        output = self.dense(output)
        output = self.layer_dropout(output)
        output = self.layer_dropout(output)
        return output</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="jidenn.models.Transformer.FFN.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs: tensorflow.python.framework.ops.Tensor) ‑> tensorflow.python.framework.ops.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass of the feed-forward network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>input tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tf.Tensor</code></dt>
<dd>output tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs: tf.Tensor) -&gt; tf.Tensor:
    &#34;&#34;&#34;Forward pass of the feed-forward network

    Args:
        inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`

    Returns:
        tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
    &#34;&#34;&#34;
    output = self.wide_dense(inputs)
    output = self.dense(output)
    output = self.layer_dropout(output)
    output = self.layer_dropout(output)
    return output</code></pre>
</details>
</dd>
<dt id="jidenn.models.Transformer.FFN.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of
dict every time it is called. The callers should make a copy of the
returned dict if they want to modify it.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = super(FFN, self).get_config()
    config.update({&#34;dim&#34;: self.dim, &#34;expansion&#34;: self.expansion,
                  &#34;activation&#34;: self.activation, &#34;dropout&#34;: self.dropout})
    return config</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="jidenn.models.Transformer.MultiheadSelfAttention"><code class="flex name class">
<span>class <span class="ident">MultiheadSelfAttention</span></span>
<span>(</span><span>dim: int, heads: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Multi-head self-attention layer
This layer is a wrapper around the <code>tf.keras.layers.MultiHeadAttention</code> layer,
to fix the key, value, and query to be the same.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>dimension of the input and output</dd>
<dt><strong><code>heads</code></strong> :&ensp;<code>int</code></dt>
<dd>number of heads</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiheadSelfAttention(tf.keras.layers.Layer):
    &#34;&#34;&#34;Multi-head self-attention layer
    This layer is a wrapper around the `tf.keras.layers.MultiHeadAttention` layer, 
    to fix the key, value, and query to be the same.

    Args:
        dim (int): dimension of the input and output
        heads (int): number of heads

    &#34;&#34;&#34;

    def __init__(self, dim: int, heads: int):
        super().__init__()
        self.dim, self.heads = dim, heads
        self.mha = tf.keras.layers.MultiHeadAttention(key_dim=dim // heads, num_heads=heads)

    def get_config(self):
        config = super(MultiheadSelfAttention, self).get_config()
        config.update({&#34;dim&#34;: self.dim, &#34;heads&#34;: self.heads})
        return config

    def call(self, inputs: tf.Tensor, mask: tf.Tensor) -&gt; tf.Tensor:
        &#34;&#34;&#34;Forward pass of the multi-head self-attention layer

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
            mask (tf.Tensor): mask tensor of shape `(batch_size, num_particles, num_particles)`
                This mask is used to mask out the attention of padding particles, generated when
                tf.RaggedTensor is converted to tf.Tensor.

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
        &#34;&#34;&#34;
        output = self.mha(query=inputs, value=inputs, key=inputs, attention_mask=mask)
        return output</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="jidenn.models.Transformer.MultiheadSelfAttention.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs: tensorflow.python.framework.ops.Tensor, mask: tensorflow.python.framework.ops.Tensor) ‑> tensorflow.python.framework.ops.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass of the multi-head self-attention layer</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>input tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
<dt><strong><code>mask</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>mask tensor of shape <code>(batch_size, num_particles, num_particles)</code>
This mask is used to mask out the attention of padding particles, generated when
tf.RaggedTensor is converted to tf.Tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tf.Tensor</code></dt>
<dd>output tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs: tf.Tensor, mask: tf.Tensor) -&gt; tf.Tensor:
    &#34;&#34;&#34;Forward pass of the multi-head self-attention layer

    Args:
        inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
        mask (tf.Tensor): mask tensor of shape `(batch_size, num_particles, num_particles)`
            This mask is used to mask out the attention of padding particles, generated when
            tf.RaggedTensor is converted to tf.Tensor.

    Returns:
        tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
    &#34;&#34;&#34;
    output = self.mha(query=inputs, value=inputs, key=inputs, attention_mask=mask)
    return output</code></pre>
</details>
</dd>
<dt id="jidenn.models.Transformer.MultiheadSelfAttention.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of
dict every time it is called. The callers should make a copy of the
returned dict if they want to modify it.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = super(MultiheadSelfAttention, self).get_config()
    config.update({&#34;dim&#34;: self.dim, &#34;heads&#34;: self.heads})
    return config</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="jidenn.models.Transformer.SelfAttentionBlock"><code class="flex name class">
<span>class <span class="ident">SelfAttentionBlock</span></span>
<span>(</span><span>dim: int, heads: int, expansion: int, activation: Callable[[tensorflow.python.framework.ops.Tensor], tensorflow.python.framework.ops.Tensor], dropout: Optional[float] = None, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Self-attention block.
It contains a multi-head self-attention layer and a feed-forward network with residual connections
and layer normalizations.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>dimension of the input and output</dd>
<dt><strong><code>heads</code></strong> :&ensp;<code>int</code></dt>
<dd>number of heads</dd>
<dt><strong><code>expansion</code></strong> :&ensp;<code>int</code></dt>
<dd>expansion factor of the hidden layer, i.e. the hidden layer has size <code>dim * expansion</code></dd>
<dt>activation (Callable[[tf.Tensor], tf.Tensor]) activation function</dt>
<dt><strong><code>dropout</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>dropout rate. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SelfAttentionBlock(tf.keras.layers.Layer):
    &#34;&#34;&#34;Self-attention block.
    It contains a multi-head self-attention layer and a feed-forward network with residual connections
    and layer normalizations.

    Args:
        dim (int): dimension of the input and output
        heads (int): number of heads
        expansion (int): expansion factor of the hidden layer, i.e. the hidden layer has size `dim * expansion`
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function
        dropout (float, optional): dropout rate. Defaults to None.
    &#34;&#34;&#34;

    def __init__(self, dim: int, heads: int, expansion: int, activation: Callable[[tf.Tensor], tf.Tensor], dropout: Optional[float] = None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.dim, self.heads, self.dropout = dim, heads, dropout
        self.expansion, self.activation = expansion, activation
        self.mhsa_ln = tf.keras.layers.LayerNormalization()
        self.mhsa = MultiheadSelfAttention(dim, heads)
        self.mhsa_dropout = tf.keras.layers.Dropout(dropout)

        self.ffn_ln = tf.keras.layers.LayerNormalization()
        self.ffn = FFN(dim, expansion, activation, dropout)

    def call(self, inputs: tf.Tensor, mask: tf.Tensor) -&gt; tf.Tensor:
        &#34;&#34;&#34;Forward pass of the self-attention block

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
            mask (tf.Tensor): mask tensor of shape `(batch_size, num_particles, num_particles)`
                This mask is used to mask out the attention of padding particles, generated when
                tf.RaggedTensor is converted to tf.Tensor.
        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
        &#34;&#34;&#34;
        attented = self.mhsa_ln(inputs)
        attented = self.mhsa(attented, mask)
        attented = self.mhsa_dropout(attented)
        attented = attented + inputs

        ffned = self.ffn_ln(attented)
        ffned = self.ffn(ffned)
        output = ffned + attented
        return output</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="jidenn.models.Transformer.SelfAttentionBlock.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs: tensorflow.python.framework.ops.Tensor, mask: tensorflow.python.framework.ops.Tensor) ‑> tensorflow.python.framework.ops.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass of the self-attention block</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>input tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
<dt><strong><code>mask</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>mask tensor of shape <code>(batch_size, num_particles, num_particles)</code>
This mask is used to mask out the attention of padding particles, generated when
tf.RaggedTensor is converted to tf.Tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tf.Tensor</code></dt>
<dd>output tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs: tf.Tensor, mask: tf.Tensor) -&gt; tf.Tensor:
    &#34;&#34;&#34;Forward pass of the self-attention block

    Args:
        inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
        mask (tf.Tensor): mask tensor of shape `(batch_size, num_particles, num_particles)`
            This mask is used to mask out the attention of padding particles, generated when
            tf.RaggedTensor is converted to tf.Tensor.
    Returns:
        tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
    &#34;&#34;&#34;
    attented = self.mhsa_ln(inputs)
    attented = self.mhsa(attented, mask)
    attented = self.mhsa_dropout(attented)
    attented = attented + inputs

    ffned = self.ffn_ln(attented)
    ffned = self.ffn(ffned)
    output = ffned + attented
    return output</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="jidenn.models.Transformer.Transformer"><code class="flex name class">
<span>class <span class="ident">Transformer</span></span>
<span>(</span><span>layers: int, dim: int, expansion: int, heads: int, activation: Callable[[tensorflow.python.framework.ops.Tensor], tensorflow.python.framework.ops.Tensor], dropout: Optional[float] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Pure Transformer layers without embedding and output layers.</p>
<p>It also creates the class token, which is used to encode the global information of the input,
by concatenating the class token to the input.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>layers</code></strong> :&ensp;<code>int</code></dt>
<dd>number of Self-Attention layers</dd>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>dimension of the input and output</dd>
<dt><strong><code>expansion</code></strong> :&ensp;<code>int</code></dt>
<dd>expansion factor of the hidden layer, i.e. the hidden layer has size <code>dim * expansion</code></dd>
<dt><strong><code>heads</code></strong> :&ensp;<code>int</code></dt>
<dd>number of heads</dd>
<dt>activation (Callable[[tf.Tensor], tf.Tensor]) activation function</dt>
<dt><strong><code>dropout</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>dropout rate. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Transformer(tf.keras.layers.Layer):
    &#34;&#34;&#34;Pure Transformer layers without embedding and output layers.

    It also creates the class token, which is used to encode the global information of the input,
    by concatenating the class token to the input.

    Args:
        layers (int): number of Self-Attention layers
        dim (int): dimension of the input and output
        expansion (int): expansion factor of the hidden layer, i.e. the hidden layer has size `dim * expansion`
        heads (int): number of heads
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function
        dropout (float, optional): dropout rate. Defaults to None.

    &#34;&#34;&#34;

    def __init__(self, layers: int, dim: int, expansion: int, heads: int, activation: Callable[[tf.Tensor], tf.Tensor], dropout: Optional[float] = None):
        # Make sure `dim` is even.
        assert dim % 2 == 0

        super().__init__()
        self.layers, self.dim, self.expansion, self.heads, self.dropout, self.activation = layers, dim, expansion, heads, dropout, activation
        self.class_token = tf.Variable(initial_value=tf.random.truncated_normal(
            (1, 1, dim), stddev=0.02), trainable=True)
        self.sa_layers = [SelfAttentionBlock(dim, heads, expansion, activation, dropout) for _ in range(layers)]

    def get_config(self):
        config = super(Transformer, self).get_config()
        config.update({name: getattr(self, name)
                      for name in [&#34;layers&#34;, &#34;dim&#34;, &#34;expansion&#34;, &#34;heads&#34;, &#34;dropout&#34;, &#34;activation&#34;]})
        return config

    def call(self, inputs: tf.Tensor, mask: tf.Tensor) -&gt; tf.Tensor:
        &#34;&#34;&#34;Forward pass of the Transformer layers

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
            mask (tf.Tensor): mask tensor of shape `(batch_size, num_particles)`.
                From the mask, a mask tensor of shape `(batch_size, num_particles, num_particles)`
                is calculated, which is used to mask out the attention of padding particles, generated when
                `tf.RaggedTensor` is converted to `tf.Tensor`. 
        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
        &#34;&#34;&#34;
        mask = mask[:, tf.newaxis, :] &amp; mask[:, :, tf.newaxis]
        class_tokens = tf.tile(self.class_token, [tf.shape(inputs)[0], 1, 1])
        hidden = tf.concat([class_tokens, inputs], axis=1)
        for sa_block in self.sa_layers:
            hidden = sa_block(hidden, mask)
        return hidden</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="jidenn.models.Transformer.Transformer.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs: tensorflow.python.framework.ops.Tensor, mask: tensorflow.python.framework.ops.Tensor) ‑> tensorflow.python.framework.ops.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass of the Transformer layers</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>input tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
<dt><strong><code>mask</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>mask tensor of shape <code>(batch_size, num_particles)</code>.
From the mask, a mask tensor of shape <code>(batch_size, num_particles, num_particles)</code>
is calculated, which is used to mask out the attention of padding particles, generated when
<code>tf.RaggedTensor</code> is converted to <code>tf.Tensor</code>. </dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tf.Tensor</code></dt>
<dd>output tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs: tf.Tensor, mask: tf.Tensor) -&gt; tf.Tensor:
    &#34;&#34;&#34;Forward pass of the Transformer layers

    Args:
        inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
        mask (tf.Tensor): mask tensor of shape `(batch_size, num_particles)`.
            From the mask, a mask tensor of shape `(batch_size, num_particles, num_particles)`
            is calculated, which is used to mask out the attention of padding particles, generated when
            `tf.RaggedTensor` is converted to `tf.Tensor`. 
    Returns:
        tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
    &#34;&#34;&#34;
    mask = mask[:, tf.newaxis, :] &amp; mask[:, :, tf.newaxis]
    class_tokens = tf.tile(self.class_token, [tf.shape(inputs)[0], 1, 1])
    hidden = tf.concat([class_tokens, inputs], axis=1)
    for sa_block in self.sa_layers:
        hidden = sa_block(hidden, mask)
    return hidden</code></pre>
</details>
</dd>
<dt id="jidenn.models.Transformer.Transformer.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of
dict every time it is called. The callers should make a copy of the
returned dict if they want to modify it.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = super(Transformer, self).get_config()
    config.update({name: getattr(self, name)
                  for name in [&#34;layers&#34;, &#34;dim&#34;, &#34;expansion&#34;, &#34;heads&#34;, &#34;dropout&#34;, &#34;activation&#34;]})
    return config</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="jidenn.models.Transformer.TransformerModel"><code class="flex name class">
<span>class <span class="ident">TransformerModel</span></span>
<span>(</span><span>input_shape: Tuple[None, int], embed_dim: int, embed_layers: int, self_attn_layers: int, expansion: int, heads: int, dropout: float, output_layer: keras.engine.base_layer.Layer, activation: Callable[[tensorflow.python.framework.ops.Tensor], tensorflow.python.framework.ops.Tensor], preprocess: Optional[keras.engine.base_layer.Layer] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Transformer model with embedding and output layers.</p>
<p>The model already contains the <code>tf.keras.layers.Input</code> layer, so it can be used as a standalone model.</p>
<p>The input tensor is first passed through the embedding layer, then the Transformer layers, and finally the output layer.
If the preprocessing layer is not None, the input tensor is first passed through the preprocessing layer before the embedding layer.
The input to the output layer is the extracted class token.
</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_shape</code></strong> :&ensp;<code>Tuple[int]</code></dt>
<dd>shape of the input</dd>
<dt><strong><code>embed_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>dimension of the embedding</dd>
<dt><strong><code>embed_layers</code></strong> :&ensp;<code>int</code></dt>
<dd>number of fully-connected layers in the embedding</dd>
<dt><strong><code>self_attn_layers</code></strong> :&ensp;<code>int</code></dt>
<dd>number of Self-Attention layers</dd>
<dt><strong><code>expansion</code></strong> :&ensp;<code>int</code></dt>
<dd>expansion factor of the hidden layer, i.e. the hidden layer has size <code>dim * expansion</code></dd>
<dt><strong><code>heads</code></strong> :&ensp;<code>int</code></dt>
<dd>number of heads</dd>
<dt><strong><code>dropout</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>dropout rate. Defaults to None.</dd>
<dt><strong><code>output_layer</code></strong> :&ensp;<code>tf.keras.layers.Layer</code></dt>
<dd>output layer</dd>
<dt>activation (Callable[[tf.Tensor], tf.Tensor]) activation function used in all the layers</dt>
<dt><strong><code>preprocess</code></strong> :&ensp;<code>tf.keras.layers.Layer</code>, optional</dt>
<dd>preprocessing layer. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TransformerModel(tf.keras.Model):
    &#34;&#34;&#34;Transformer model with embedding and output layers.

    The model already contains the `tf.keras.layers.Input` layer, so it can be used as a standalone model.

    The input tensor is first passed through the embedding layer, then the Transformer layers, and finally the output layer.
    If the preprocessing layer is not None, the input tensor is first passed through the preprocessing layer before the embedding layer.
    The input to the output layer is the extracted class token.  

    Args:
        input_shape (Tuple[int]): shape of the input
        embed_dim (int): dimension of the embedding
        embed_layers (int): number of fully-connected layers in the embedding
        self_attn_layers (int): number of Self-Attention layers
        expansion (int): expansion factor of the hidden layer, i.e. the hidden layer has size `dim * expansion`
        heads (int): number of heads
        dropout (float, optional): dropout rate. Defaults to None.
        output_layer (tf.keras.layers.Layer): output layer
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function used in all the layers
        preprocess (tf.keras.layers.Layer, optional): preprocessing layer. Defaults to None.
    &#34;&#34;&#34;

    def __init__(self,
                 input_shape: Tuple[None, int],
                 embed_dim: int,
                 embed_layers: int,
                 self_attn_layers: int,
                 expansion: int,
                 heads: int,
                 dropout: float,
                 output_layer: tf.keras.layers.Layer,
                 activation: Callable[[tf.Tensor], tf.Tensor],
                 preprocess: Optional[tf.keras.layers.Layer] = None):

        input = tf.keras.layers.Input(shape=input_shape, ragged=True)

        row_lengths = input.row_lengths()
        hidden = input.to_tensor()

        if preprocess is not None:
            hidden = preprocess(hidden)

        hidden = FCEmbedding(embed_dim, embed_layers, activation)(hidden)
        row_lengths += 1

        transformed = Transformer(self_attn_layers, embed_dim, expansion,
                                  heads, activation, dropout)(hidden, mask=tf.sequence_mask(row_lengths))

        transformed = tf.keras.layers.LayerNormalization()(transformed[:, 0, :])
        output = output_layer(transformed)

        super().__init__(inputs=input, outputs=output)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.training.Model</li>
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
<li>keras.utils.version_utils.ModelVersionSelector</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="JIDENN" href="https://jansam.wieno.sk/JIDENN/">
<img src="images/q_g_tagging.jpeg" alt=""> JIDENN
</a>
</header>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="jidenn.models" href="index.html">jidenn.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="jidenn.models.Transformer.FCEmbedding" href="#jidenn.models.Transformer.FCEmbedding">FCEmbedding</a></code></h4>
<ul class="">
<li><code><a title="jidenn.models.Transformer.FCEmbedding.call" href="#jidenn.models.Transformer.FCEmbedding.call">call</a></code></li>
<li><code><a title="jidenn.models.Transformer.FCEmbedding.get_config" href="#jidenn.models.Transformer.FCEmbedding.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="jidenn.models.Transformer.FFN" href="#jidenn.models.Transformer.FFN">FFN</a></code></h4>
<ul class="">
<li><code><a title="jidenn.models.Transformer.FFN.call" href="#jidenn.models.Transformer.FFN.call">call</a></code></li>
<li><code><a title="jidenn.models.Transformer.FFN.get_config" href="#jidenn.models.Transformer.FFN.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="jidenn.models.Transformer.MultiheadSelfAttention" href="#jidenn.models.Transformer.MultiheadSelfAttention">MultiheadSelfAttention</a></code></h4>
<ul class="">
<li><code><a title="jidenn.models.Transformer.MultiheadSelfAttention.call" href="#jidenn.models.Transformer.MultiheadSelfAttention.call">call</a></code></li>
<li><code><a title="jidenn.models.Transformer.MultiheadSelfAttention.get_config" href="#jidenn.models.Transformer.MultiheadSelfAttention.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="jidenn.models.Transformer.SelfAttentionBlock" href="#jidenn.models.Transformer.SelfAttentionBlock">SelfAttentionBlock</a></code></h4>
<ul class="">
<li><code><a title="jidenn.models.Transformer.SelfAttentionBlock.call" href="#jidenn.models.Transformer.SelfAttentionBlock.call">call</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="jidenn.models.Transformer.Transformer" href="#jidenn.models.Transformer.Transformer">Transformer</a></code></h4>
<ul class="">
<li><code><a title="jidenn.models.Transformer.Transformer.call" href="#jidenn.models.Transformer.Transformer.call">call</a></code></li>
<li><code><a title="jidenn.models.Transformer.Transformer.get_config" href="#jidenn.models.Transformer.Transformer.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="jidenn.models.Transformer.TransformerModel" href="#jidenn.models.Transformer.TransformerModel">TransformerModel</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>
<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>jidenn.models.DeParT API documentation</title>
<meta name="description" content="Implementation of Dynamicaly Ehnanced Particle Transformer (DeParT) model â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="canonical" href="http://jansam.wieno.sk/JIDENN/jidenn/models/DeParT.html">
<link rel="icon" href="images/q_g_tagging.jpeg">
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>jidenn.models.DeParT</code></h1>
</header>
<section id="section-intro">
<p>Implementation of Dynamicaly Ehnanced Particle Transformer (DeParT) model.</p>
<p>This is an extention of ParT model, utilizing several adjustments to the original, namely:</p>
<ul>
<li>Talkative Multihead Self-Attention</li>
<li>LayerScale</li>
<li>Stochastic Depth</li>
<li>Gated Feed-Forward Network</li>
</ul>
<p>All are based on the 'DeiT III: Revenge of the ViT' paper, <a href="https://arxiv.org/abs/2204.07118,">https://arxiv.org/abs/2204.07118,</a>
except the Gated FFN, <a href="https://arxiv.org/abs/2002.05202.">https://arxiv.org/abs/2002.05202.</a></p>
<p>The model also includes the intaraction variables as the ParT model.</p>
<p><img alt="DeParT" src="images/depart.png">
<img alt="DeParT" src="images/depart_layers.png"></p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Implementation of Dynamicaly Ehnanced Particle Transformer (DeParT) model.

This is an extention of ParT model, utilizing several adjustments to the original, namely:

- Talkative Multihead Self-Attention
- LayerScale
- Stochastic Depth
- Gated Feed-Forward Network

All are based on the &#39;DeiT III: Revenge of the ViT&#39; paper, https://arxiv.org/abs/2204.07118,
except the Gated FFN, https://arxiv.org/abs/2002.05202.

The model also includes the intaraction variables as the ParT model.

![DeParT](images/depart.png)
![DeParT](images/depart_layers.png)

&#34;&#34;&#34;
import tensorflow as tf
from typing import Callable, Union, Tuple, Optional


class FFN(tf.keras.layers.Layer):
    &#34;&#34;&#34;Feed-forward network
    On top of the Transformer FFN layer, it adds a layer normalization in between the two dense layers.

    On top of ParT FFN layer, it adds a gated linear unit (GLU) activation function.
    This adds additional weights to the layer, so to keep the number of parameters the same,
    the size of the first hidden layer is `dim * expansion * 2 / 3` and the gate 
    hidden layer is the same dimension. 

    Args:
        dim (int): dimension of the input and output
        expansion (int): expansion factor of the hidden layer, i.e. the hidden layer has size `dim * expansion`
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function
        dropout (float, optional): dropout rate. Defaults to None.
    &#34;&#34;&#34;

    def __init__(self, dim: int, expansion: int, activation: Callable[[tf.Tensor], tf.Tensor], dropout: Optional[float] = None):
        super().__init__()
        self.dim, self.expansion, self.activation, self.dropout = dim, expansion, activation, dropout

        self.wide_dense = tf.keras.layers.Dense(int(dim * expansion * 2 / 3), activation=activation, use_bias=False)
        self.gate_dense = tf.keras.layers.Dense(int(dim * expansion * 2 / 3), activation=None, use_bias=False)
        self.dense = tf.keras.layers.Dense(dim, activation=None, use_bias=False)
        self.ln = tf.keras.layers.LayerNormalization()
        self.layer_dropout = tf.keras.layers.Dropout(dropout)

    def get_config(self):
        config = super().get_config()
        config.update({&#34;dim&#34;: self.dim, &#34;expansion&#34;: self.expansion,
                      &#34;activation&#34;: self.activation, &#34;dropout&#34;: self.dropout})
        return config

    def call(self, inputs):
        &#34;&#34;&#34;Forward pass of the feed-forward network
        Includes a layer normalization layer in between the two dense layers

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
        &#34;&#34;&#34;
        output = self.wide_dense(inputs) * self.gate_dense(inputs)
        output = self.ln(output)
        output = self.dense(output)
        output = self.layer_dropout(output)
        return output


class LayerScale(tf.keras.layers.Layer):
    &#34;&#34;&#34;Layer scale layer
    Layer Scale layer helps to stabilize the training of the model.
    When the model has a large number of layers, the variance of the input to each layer can be very different.
    To stabilize the training, we scale the input to each layer by a learnable scalar parameter,
    which is initialized to a small value.

    Args:
        init_values (float): initial value of the layer scale
        dim (int): dimension of the input and output
    &#34;&#34;&#34;

    def __init__(self, init_values: float, dim: int, ):
        super().__init__()
        self.gamma = tf.Variable(init_values * tf.ones((dim,)))

    def call(self, x: tf.Tensor, training=False) -&gt; tf.Tensor:
        &#34;&#34;&#34;Forward pass of the layer scale layer

        Args:
            x (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
        &#34;&#34;&#34;
        return x * self.gamma


class StochasticDepth(tf.keras.layers.Layer):
    &#34;&#34;&#34;Stochastic depth layer.

    Stochastic depth is a regularization technique that randomly drops layers instead 
    of individial neurons.

    The probability of dropping should increase with the depth of the layer.
    This must be done manually by the user when creating the layer.

    Args:
        drop_prob (float): probability of dropping the layer
    &#34;&#34;&#34;

    def __init__(self, drop_prob: float):
        super().__init__()
        self.drop_prob = drop_prob

    def call(self, x: tf.Tensor, training=False) -&gt; tf.Tensor:
        &#34;&#34;&#34;Forward pass of the stochastic depth layer

        Args:
            x (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`

        &#34;&#34;&#34;
        if training:
            keep_prob = 1 - self.drop_prob
            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)
            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)
            random_tensor = tf.floor(random_tensor)
            return (x / keep_prob) * random_tensor
        return x


class TalkingMultiheadSelfAttention(tf.keras.layers.Layer):
    &#34;&#34;&#34;Talking Multi-head self-attention layer
    Standalone implementation of the multi-head self-attention layer, which
    includes the interaction variables and the talking heads mechanism.

    Args:
        dim (int): dimension of the input and output
        heads (int): number of heads
        dropout (float, optional): dropout rate. Defaults to None.

    &#34;&#34;&#34;

    def __init__(self, dim: int, heads: int, dropout: Optional[float] = None):
        super().__init__()
        self.dim, self.heads = dim, heads

        self.linear_qkv = tf.keras.layers.Dense(dim * 3)
        self.linear_out = tf.keras.layers.Dense(dim)

        self.linear_talking_1 = tf.keras.layers.Dense(heads)
        self.linear_talking_2 = tf.keras.layers.Dense(heads)

        self.dropout = tf.keras.layers.Dropout(dropout)
        self.attn_drop = tf.keras.layers.Dropout(dropout)

    def get_config(self):
        config = super(TalkingMultiheadSelfAttention, self).get_config()
        config.update({&#34;dim&#34;: self.dim, &#34;heads&#34;: self.heads})
        return config

    def call(self, inputs: tf.Tensor, mask: tf.Tensor, interaction: Optional[tf.Tensor] = None, training: bool = False) -&gt; tf.Tensor:
        &#34;&#34;&#34;Forward pass of the talking multi-head self-attention layer

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
            mask (tf.Tensor): mask tensor of shape `(batch_size, num_particles, num_particles)`
                This mask is used to mask out the attention of padding particles, generated when
                tf.RaggedTensor is converted to tf.Tensor.
            interaction (tf.Tensor, optional): interaction tensor of shape `(batch_size, num_particles, num_particles, heads)`
            training (bool, optional): whether the model is in training mode. Defaults to False.

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
        &#34;&#34;&#34;
        B, N, C = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2]

        qkv = self.linear_qkv(inputs)  # (B, N, C * 3)
        qkv = tf.reshape(qkv, [B, N, 3, self.heads, C // self.heads])  # (B, N, 3, H, C // H)
        qkv = tf.transpose(qkv, [2, 0, 3, 1, 4])  # (3, B, H, N, C // H)
        q, k, v = qkv[0], qkv[1], qkv[2]  # 3 x (B, H, N, C // H)

        attention_weights = tf.linalg.matmul(q, k, transpose_b=True) / (q.shape[-1] ** 0.5)  # (B, H, N, N)

        attention_weights = self.linear_talking_1(tf.transpose(attention_weights, [0, 2, 3, 1]))  # (B, N, N, H)
        attention_weights = tf.transpose(attention_weights, [0, 3, 1, 2])  # (B, H, N, N)

        if interaction is not None:
            interaction = tf.transpose(interaction, [0, 3, 1, 2])  # (B, H, N, N)
            attention_weights += interaction

        attention = tf.keras.layers.Softmax()(attention_weights, mask=mask)  # (B, H, N, N)
        attention = self.linear_talking_2(tf.transpose(attention, [0, 2, 3, 1]))  # (B, N, N, H)
        attention = tf.transpose(attention, [0, 3, 1, 2])  # (B, H, N, N)
        attention = self.attn_drop(attention, training)  # (B, H, N, N)

        output = tf.linalg.matmul(attention, v)  # (B, H, N, C // H)
        output = tf.transpose(output, [0, 2, 1, 3])  # (B, N, H, C // H)
        output = tf.reshape(output, [B, N, C])  # (B, N, C)
        output = self.linear_out(output)  # (B, N, C)
        output = self.dropout(output, training)
        return output


class TalkingMultiheadClassAttention(tf.keras.layers.Layer):
    &#34;&#34;&#34;Talking Multi-head class-attention layer
    Standalone implementation of the multi-head class-attention layer, which
    includes the talking heads mechanism.

    Args:
        dim (int): dimension of the input and output
        heads (int): number of heads
        dropout (float, optional): dropout rate, defaults to None

    &#34;&#34;&#34;

    def __init__(self, dim: int, heads: int, dropout: Optional[float] = None):
        super().__init__()
        self.dim, self.heads = dim, heads

        self.linear_kv = tf.keras.layers.Dense(dim * 2)
        self.linear_q = tf.keras.layers.Dense(dim)
        self.linear_out = tf.keras.layers.Dense(dim)

        self.linear_talking_1 = tf.keras.layers.Dense(heads)
        self.linear_talking_2 = tf.keras.layers.Dense(heads)

        self.dropout = tf.keras.layers.Dropout(dropout)
        self.attn_drop = tf.keras.layers.Dropout(dropout)

    def get_config(self):
        config = super(TalkingMultiheadClassAttention, self).get_config()
        config.update({&#34;dim&#34;: self.dim, &#34;heads&#34;: self.heads})
        return config

    def call(self, inputs: tf.Tensor, class_token: tf.Tensor, mask: tf.Tensor, training: bool = False) -&gt; tf.Tensor:
        &#34;&#34;&#34;Forward pass of the multi-head class-attention layer

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
            class_token (tf.Tensor): class token tensor of shape `(batch_size, 1, dim)`
            mask (tf.Tensor): mask tensor of shape `(batch_size, 1, num_particles)`
                This mask is used to mask out the attention of padding particles, generated when
                tf.RaggedTensor is converted to tf.Tensor.
            training (bool, optional): whether the model is in training mode. Defaults to False.
        Returns:
            tf.Tensor: output tensor of shape `(batch_size, 1, dim)`
        &#34;&#34;&#34;

        B, N, C = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2]

        kv = self.linear_kv(inputs)  # (B, N, C * 3)
        kv = tf.reshape(kv, [B, N, 2, self.heads, C // self.heads])  # (B, N, 3, H, C // H)
        kv = tf.transpose(kv, [2, 0, 3, 1, 4])  # (3, B, H, N, C // H)
        k, v = kv[0], kv[1]  # 2 x (B, H, N, C // H)

        q = self.linear_q(class_token)  # (B, 1, C)
        q = tf.reshape(q, [B, self.heads, 1, C // self.heads])  # (B, H, 1, C // H)

        attention_weights = tf.linalg.matmul(q, k, transpose_b=True) / (q.shape[-1] ** 0.5)  # (B, H, 1, N)

        attention_weights = self.linear_talking_1(tf.transpose(attention_weights, [0, 2, 3, 1]))  # (B, 1, N, H)
        attention_weights = tf.transpose(attention_weights, [0, 3, 1, 2])  # (B, H, 1, N)

        attention = tf.keras.layers.Softmax()(attention_weights, mask=mask)  # (B, H, 1, N)
        attention = self.linear_talking_2(tf.transpose(attention, [0, 2, 3, 1]))  # (B, 1, N, H)
        attention = tf.transpose(attention, [0, 3, 1, 2])  # (B, H, 1, N)
        attention = self.attn_drop(attention, training)  # (B, H, 1, N)

        output = tf.linalg.matmul(attention, v)  # (B, H, 1, C // H)
        output = tf.transpose(output, [0, 2, 1, 3])  # (B, 1, H, C // H)
        output = tf.reshape(output, [B, 1, C])  # (B, 1, C)
        output = self.linear_out(output)  # (B, 1, C)
        output = self.dropout(output, training)
        return output


class MultiheadClassAttention(tf.keras.layers.Layer):
    &#34;&#34;&#34;Multi-head class attention layer
    This layer is a wrapper around the `tf.keras.layers.MultiHeadAttention` layer, 
    to fix the key, and value to be the same as the input, and only use the class token
    as the query.

    Args:
        dim (int): dimension of the input and output
        heads (int): number of heads
        dropout (float, optional): dropout rate, defaults to None
    &#34;&#34;&#34;

    def __init__(self, dim: int, heads: int, dropout: Optional[float] = None):
        super().__init__()
        self.dim, self.heads, self.dropout = dim, heads, dropout
        self.mha = tf.keras.layers.MultiHeadAttention(key_dim=dim // heads, num_heads=heads, dropout=dropout)

    def get_config(self):
        config = super(MultiheadClassAttention, self).get_config()
        config.update({&#34;dim&#34;: self.dim, &#34;heads&#34;: self.heads, &#34;dropout&#34;: self.dropout})
        return config

    def call(self, query, inputs, mask):
        &#34;&#34;&#34;Forward pass of the multi-head self-attention layer

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
            class_token (tf.Tensor): class token tensor of shape `(batch_size, 1, dim)`
            mask (tf.Tensor): mask tensor of shape `(batch_size, 1, num_particles)`
                This mask is used to mask out the attention of padding particles, generated when
                tf.RaggedTensor is converted to tf.Tensor.
        Returns:
            tf.Tensor: output tensor of shape `(batch_size, 1, dim)`
        &#34;&#34;&#34;
        output = self.mha(query=query, value=inputs, key=inputs, attention_mask=mask)
        return output


class SelfAttentionBlock(tf.keras.layers.Layer):
    &#34;&#34;&#34;Self-attention block.
    It contains a talking multi-head self-attention layer and a feed-forward network with residual connections
    and layer normalizations. The self-attention layer includes the interaction variables.
    Additionally, the stochastic dropout and layer scale are applied to the output of the self-attention layer
    and feed-forward network.

    Args:
        dim (int): dimension of the input and output
        heads (int): number of heads
        stoch_drop_prob (float): probability of stochastic dropout
        layer_scale_init_value (float): initial value of layer scale
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function
        expansion (int): expansion factor of the feed-forward network, 
            the dimension of the feed-forward network is `dim * expansion`
        dropout (float, optional): dropout rate, defaults to None

    &#34;&#34;&#34;

    def __init__(self, dim: int, heads: int, stoch_drop_prob: float, layer_scale_init_value: float, activation: Callable[[tf.Tensor], tf.Tensor], expansion: int,
                 dropout: Optional[float] = None):
        super().__init__()
        self.dim, self.heads, self.dropout, self.stoch_drop_prob, self.layer_scale_init_value, self.activation, self.expansion = dim, heads, dropout, stoch_drop_prob, layer_scale_init_value, activation, expansion

        self.pre_mhsa_ln = tf.keras.layers.LayerNormalization()
        self.mhsa = TalkingMultiheadSelfAttention(dim, heads, dropout)
        self.post_mhsa_scale = LayerScale(layer_scale_init_value, dim)
        self.post_mhsa_stoch_depth = StochasticDepth(drop_prob=stoch_drop_prob)

        self.pre_ffn_ln = tf.keras.layers.LayerNormalization()
        self.ffn = FFN(dim, expansion, activation, dropout)
        self.post_ffn_scale = LayerScale(layer_scale_init_value, dim)
        self.post_ffn_stoch_depth = StochasticDepth(drop_prob=stoch_drop_prob)

    def get_config(self):
        config = super(SelfAttentionBlock, self).get_config()
        config.update({&#34;dim&#34;: self.dim, &#34;heads&#34;: self.heads, &#34;dropout&#34;: self.dropout, &#34;stoch_drop_prob&#34;: self.stoch_drop_prob,
                      &#34;layer_scale_init_value&#34;: self.layer_scale_init_value, &#34;activation&#34;: self.activation, &#34;expansion&#34;: self.expansion})
        return config

    def call(self, inputs, mask, interaction=None):
        &#34;&#34;&#34;Forward pass of the self-attention block

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
            mask (tf.Tensor, optional): mask tensor of shape `(batch_size, num_particles, num_particles)`. Defaults to None.
                This mask is used to mask out the attention of padding particles, generated when
                tf.RaggedTensor is converted to tf.Tensor.
            interaction (tf.Tensor, optional): interaction tensor of shape `(batch_size, num_particles, num_particles, heads)`. Defaults to None.

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
        &#34;&#34;&#34;
        attented = self.pre_mhsa_ln(inputs)
        attented = self.mhsa(attented, mask, interaction)
        attented = self.post_mhsa_scale(attented)
        attented = self.post_mhsa_stoch_depth(attented)
        attented = attented + inputs

        ffned = self.pre_ffn_ln(attented)
        ffned = self.ffn(ffned)
        ffned = self.post_ffn_scale(ffned)
        ffned = self.post_ffn_stoch_depth(ffned)
        output = ffned + attented

        return output


class ClassAttentionBlock(tf.keras.layers.Layer):
    &#34;&#34;&#34;Class attention block.
    It allows the class token to attend to the input particles, and then feed the attended class token
    to the feed-forward network with residual connections and layer normalizations.

    This extracts the class information from the attented particles more effectively.

    Args:
        dim (int): dimension of the input and output
        heads (int): number of heads
        stoch_drop_prob (float): probability of stochastic dropout
        layer_scale_init_value (float): initial value of layer scale
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function
        expansion (int): expansion factor of the feed-forward network, 
            the dimension of the feed-forward network is `dim * expansion`
        dropout (float, optional): dropout rate, defaults to None
    &#34;&#34;&#34;

    def __init__(self, dim: int, heads: int, stoch_drop_prob: float, layer_scale_init_value: float, activation: Callable[[tf.Tensor], tf.Tensor], expansion: int,
                 dropout: Optional[float] = None):
        super().__init__()
        self.dim, self.heads, self.dropout, self.stoch_drop_prob, self.layer_scale_init_value, self.activation, self.expansion = dim, heads, dropout, stoch_drop_prob, layer_scale_init_value, activation, expansion

        self.pre_mhca_ln = tf.keras.layers.LayerNormalization()
        self.mhca = MultiheadClassAttention(dim, heads, dropout)
        self.post_mhca_scale = LayerScale(layer_scale_init_value, dim)
        self.post_mhca_stoch_depth = StochasticDepth(drop_prob=stoch_drop_prob)

        self.pre_ffn_ln = tf.keras.layers.LayerNormalization()
        self.ffn = FFN(dim, expansion, activation, dropout)
        self.post_ffn_scale = LayerScale(layer_scale_init_value, dim)
        self.post_ffn_stoch_depth = StochasticDepth(drop_prob=stoch_drop_prob)

    def get_config(self):
        config = super(ClassAttentionBlock, self).get_config()
        config.update({&#34;dim&#34;: self.dim, &#34;heads&#34;: self.heads, &#34;dropout&#34;: self.dropout, &#34;stoch_drop_prob&#34;: self.stoch_drop_prob,
                      &#34;layer_scale_init_value&#34;: self.layer_scale_init_value, &#34;activation&#34;: self.activation, &#34;expansion&#34;: self.expansion})
        return config

    def call(self, inputs: tf.Tensor, class_token: tf.Tensor, mask: tf.Tensor) -&gt; tf.Tensor:
        &#34;&#34;&#34;Forward pass of the class attention block

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
            class_token (tf.Tensor): class token tensor of shape `(batch_size, 1, dim)`
                It is concatenated with the input tensor along the particle dimension,
                at the front of the input tensor.
            mask (tf.Tensor): mask tensor of shape `(batch_size, 1, num_particles)`
                This mask is used to mask out the attention of padding particles, generated when
                tf.RaggedTensor is converted to tf.Tensor.

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, 1, dim)`, an updated class token
        &#34;&#34;&#34;
        attented = tf.concat([class_token, inputs], axis=1)
        attented = self.pre_mhca_ln(attented)
        attented = self.mhca(query=class_token, inputs=attented, mask=mask)
        attented = self.post_mhca_scale(attented)
        attented = self.post_mhca_stoch_depth(attented)
        attented = attented + class_token

        ffned = self.pre_ffn_ln(attented)
        ffned = self.ffn(ffned)
        ffned = self.post_ffn_scale(ffned)
        ffned = self.post_ffn_stoch_depth(ffned)
        output = ffned + attented
        return output


class DeParT(tf.keras.layers.Layer):
    &#34;&#34;&#34;Pure DeParT layers without the embedding and output layers.

    It also creates the class token, which is used to encode the global information of the input,
    using the ClassAttentionBlock.

    Args:
        dim (int): dimension of the input and output
        self_attn_layers (int): number of self-attention layers
        class_attn_layers (int): number of class-attention layers
        expansion (int): expansion factor of the hidden layer, i.e. the hidden layer has size `dim * expansion`
        heads (int): number of heads
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function
        layer_scale_init_value (float): initial value of layer scale. 
        stochastic_depth_drop_rate (float, optional): drop rate of stochastic depth
        class_stochastic_depth_drop_rate (float, optional): drop rate of stochastic depth of the class token
        dropout (float, optional): dropout rate. Defaults to None.
        class_dropout (float, optional): dropout rate of the class token. Defaults to None.
    &#34;&#34;&#34;

    def __init__(self,
                 self_attn_layers: int,
                 class_attn_layers: int,
                 dim: int,
                 expansion: int,
                 heads: int,
                 activation: Callable[[tf.Tensor], tf.Tensor],
                 layer_scale_init_value: float,
                 stochastic_depth_drop_rate: Optional[float] = None,
                 class_stochastic_depth_drop_rate: Optional[float] = None,
                 class_dropout: Optional[float] = None,
                 dropout: Optional[float] = None,):
        # Make sure `dim` is even.
        assert dim % 2 == 0

        super().__init__()
        self.layers, self.dim, self.expansion, self.heads, self.dropout, self.activation, self.class_layers = self_attn_layers, dim, expansion, heads, dropout, activation, class_attn_layers
        self.layer_scale_init_value, self.stochastic_depth_drop_rate, self.class_stochastic_depth_drop_rate = layer_scale_init_value, stochastic_depth_drop_rate, class_stochastic_depth_drop_rate
        self.class_dropout = class_dropout

        self.sa_layers = [SelfAttentionBlock(dim,
                                             heads,
                                             self.stochastic_prob(i, self_attn_layers - 1,
                                                                  stochastic_depth_drop_rate),
                                             layer_scale_init_value,
                                             activation,
                                             expansion,
                                             dropout,) for i in range(self_attn_layers)]

        self.ca_layers = [ClassAttentionBlock(dim,
                                              heads,
                                              self.stochastic_prob(i, class_attn_layers - 1,
                                                                   class_stochastic_depth_drop_rate),
                                              layer_scale_init_value,
                                              activation,
                                              expansion,
                                              class_dropout,) for i in range(class_attn_layers)]

        self.cls_token = tf.Variable(tf.random.truncated_normal((1, 1, dim), stddev=0.02), trainable=True)

    def stochastic_prob(self, step, total_steps, drop_rate):
        return drop_rate * step / total_steps

    def get_config(self):
        config = super(DeParT, self).get_config()
        config.update({name: getattr(self, name)
                      for name in [&#34;layers&#34;, &#34;dim&#34;, &#34;expansion&#34;, &#34;heads&#34;, &#34;dropout&#34;, &#34;activation&#34;, &#34;class_layers&#34;,
                                   &#34;layer_scale_init_value&#34;, &#34;stochastic_depth_drop_rate&#34;, &#34;class_stochastic_depth_drop_rate&#34;, &#34;class_dropout&#34;]})
        return config

    def call(self, inputs: tf.Tensor, mask: tf.Tensor, interaction: Optional[tf.Tensor] = None) -&gt; tf.Tensor:
        &#34;&#34;&#34;Forward pass of the DeParT layers

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
            mask (tf.Tensor): mask tensor of shape `(batch_size, num_particles)`.
                From the mask, a mask tensor of shape `(batch_size, num_particles, num_particles)`
                is calculated, which is used to mask out the attention of padding particles, generated when
                `tf.RaggedTensor` is converted to `tf.Tensor`.
            interaction (tf.Tensor, optional): interaction tensor of shape `(batch_size, num_particles, num_particles, heads)`

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
        &#34;&#34;&#34;
        sa_mask = mask[:, tf.newaxis, tf.newaxis, :] &amp; mask[:, tf.newaxis, :, tf.newaxis]
        hidden = inputs
        for layer in self.sa_layers:
            hidden = layer(hidden, sa_mask, interaction)

        cls_token = tf.tile(self.cls_token, (tf.shape(inputs)[0], 1, 1))
        class_mask = mask[:, tf.newaxis, :]
        class_mask = tf.concat([tf.ones((tf.shape(inputs)[0], 1, 1), dtype=tf.bool), class_mask], axis=2)
        for layer in self.ca_layers:
            cls_token = layer(hidden, cls_token=cls_token, mask=class_mask)
        return cls_token


class FCEmbedding(tf.keras.layers.Layer):
    &#34;&#34;&#34;Embedding layer as a series of fully-connected layers.

    Args:
        embed_dim (int): dimension of the embedding
        embed_layers (int): number of fully-connected layers
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function
    &#34;&#34;&#34;

    def __init__(self, embedding_dim: int, num_embeding_layers: int, activation: Callable[[tf.Tensor], tf.Tensor], ):

        super().__init__()
        self.embedding_dim, self.activation, self.num_embeding_layers = embedding_dim, activation, num_embeding_layers
        self.layers = [tf.keras.layers.Dense(self.embedding_dim, activation=self.activation)
                       for _ in range(self.num_embeding_layers)]

    def get_config(self):
        config = super(FCEmbedding, self).get_config()
        config.update({name: getattr(self, name) for name in [&#34;embedding_dim&#34;, &#34;num_embeding_layers&#34;, &#34;activation&#34;]})
        return config

    def call(self, inputs):
        &#34;&#34;&#34;Forward pass of the embedding layer

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, num_features)`

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, embed_dim)`
        &#34;&#34;&#34;
        hidden = inputs
        for layer in self.layers:
            hidden = layer(hidden)
        return hidden


class CNNEmbedding(tf.keras.layers.Layer):
    &#34;&#34;&#34;Embedding layer of the interaction variables as a series of point-wise convolutional layers.
    The interaction variiables are compuetd for each pair of particles.
    This creates a redundancy in the input, as the matrix is symetric and the diagonal is always zero.
    To save computation, the upper triangular part of the matrix is used as input, which is 
    flattend and the 1D convolutions are applied to it.

    Args:
        num_layers (int): number of convolutional layers
        layer_size (int): number of channels of the hidden layers
        out_dim (int): number of channels of the last convolutional layer which 
            is manually appended as an extra layer after `num_layers` layers.
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function
    &#34;&#34;&#34;

    def __init__(self, num_layers: int, layer_size: int, out_dim: int, activation: Callable[[tf.Tensor], tf.Tensor]):
        super().__init__()
        self.activation, self.num_layers, self.layer_size, self.out_dim = activation, num_layers, layer_size, out_dim

        self.conv_layers = [tf.keras.layers.Conv1D(layer_size, 1) for _ in range(num_layers)]
        self.conv_layers.append(tf.keras.layers.Conv1D(out_dim, 1))
        self.normlizations = [tf.keras.layers.BatchNormalization() for _ in range(num_layers + 1)]
        self.activation_layer = tf.keras.layers.Activation(activation)

    def get_config(self):
        config = super(CNNEmbedding, self).get_config()
        config.update({name: getattr(self, name)
                      for name in [&#34;num_layers&#34;, &#34;layer_size&#34;, &#34;out_dim&#34;, &#34;activation&#34;]})
        return config

    def call(self, inputs):
        &#34;&#34;&#34;Forward pass of the interaction embedding layer

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, num_particles, num_features)`
                This matrix is assumed to be symetric, with zero diagonal, with only the upper-triag part 
                can be used fopr embedding. 

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, num_particles, out_dim)`
        &#34;&#34;&#34;
        ones = tf.ones_like(inputs[0, :, :, 0])
        upper_tril_mask = tf.linalg.band_part(ones, 0, -1)
        diag_mask = tf.linalg.band_part(ones, 0, 0)
        upper_tril_mask = tf.cast(upper_tril_mask - diag_mask, tf.bool)
        flattened_upper_triag = tf.boolean_mask(inputs, upper_tril_mask, axis=1)

        hidden = flattened_upper_triag
        for conv, norm in zip(self.conv_layers, self.normlizations):
            hidden = conv(hidden)
            hidden = norm(hidden)
            hidden = self.activation_layer(hidden)

        true_mask = tf.cast(tf.where(upper_tril_mask), tf.int32)
        out = tf.transpose(hidden, [1, 0, 2])
        out = tf.scatter_nd(true_mask, out, shape=[tf.shape(inputs)[1],
                            tf.shape(inputs)[2], tf.shape(inputs)[0], self.out_dim])
        out = out + tf.transpose(out, [1, 0, 2, 3])
        out = tf.transpose(out, [2, 0, 1, 3])
        return out


class DeParTModel(tf.keras.Model):
    &#34;&#34;&#34;DeParT model with embwith embedding and output layers.

    The model already contains the `tf.keras.layers.Input` layer, so it can be used as a standalone model.

    The input tensor can be either a tensor of shape `(batch_size, num_particles, num_features)` or
    a tuple of tensors `(particle_tensor, interaction_tensor)` of shapes
    `(batch_size, num_particles, num_features)` and `(batch_size, num_particles, num_particles, num_features)`, respectively.

    The model can be used with or without the interaction tensor, depending on the type of the input shape,
    if it is a tuple, the interaction tensor is assumed to be present.

    The input tensor is first passed through the embedding layer, then the ParT layers, and finally the output layer.
    If the interaction tensor is present, it is passed through the interaction embedding layer before the ParT layers.

    If the preprocessing layer is not None, the input tensor is first passed through the preprocessing layer before the embedding layer.
    If the interaction tensor is present, it is passed through the preprocessing layer is an tuple of two layers,
    each of which is applied to the particle and interaction tensors, respectively.

    The output of ParT is a vector of shape `(batch_size, embed_dim)` with extracted class infromation.
    This is then passed through the output layer.
    Layer normalization is applied to the output of the DeParT layers before the output layer.

    Args:
        input_shape (Union[Tuple[None, int], Tuple[Tuple[None, int], Tuple[None, None, int]]]): shape of the input tensor.
            If the interaction tensor is present, it is assumed to be a tuple of two shapes,
            each creating a separate input layer.
        embed_dim (int): dimension of the embedding layer
        embed_layers (int): number of layers of the embedding layer
        self_attn_layers (int): number of self-attention layers
        class_attn_layers (int): number of class-attention layers
        expansion (int): expansion factor of the self-attention layers
        heads (int): number of heads of the self-attention layers
        layer_scale_init_value (float): initial value of the layer scale parameter
        stochastic_depth_drop_rate (float): drop rate of the stochastic depth regularization
        class_stochastic_depth_drop_rate (float): drop rate of the stochastic depth regularization of the class-attention layers
        output_layer (tf.keras.layers.Layer): output layer
        activation (Callable[[tf.Tensor], tf.Tensor]): activation function
        dropout (Optional[float], optional): dropout rate. Defaults to None.
        class_dropout (Optional[float], optional): dropout rate of the class-attention layers. Defaults to None.
        interaction_embed_layers (Optional[int], optional): number of layers of the interaction embedding layer. Defaults to None.
        interaction_embed_layer_size (Optional[int], optional): size of the layers of the interaction embedding layer. Defaults to None.
        preprocess (Union[tf.keras.layers.Layer, None, Tuple[tf.keras.layers.Layer, tf.keras.layers.Layer]], optional): preprocessing layer. Defaults to None.

    &#34;&#34;&#34;

    def __init__(self,
                 input_shape: Union[Tuple[None, int], Tuple[Tuple[None, int], Tuple[None, None, int]]],
                 embed_dim: int,
                 embed_layers: int,
                 self_attn_layers: int,
                 class_attn_layers: int,
                 expansion: int,
                 heads: int,
                 layer_scale_init_value: float,
                 stochastic_depth_drop_rate: float,
                 class_stochastic_depth_drop_rate: float,
                 output_layer: tf.keras.layers.Layer,
                 activation: Callable[[tf.Tensor], tf.Tensor],
                 dropout: Optional[float] = None,
                 class_dropout: Optional[float] = None,
                 preprocess: Union[tf.keras.layers.Layer,
                                   Tuple[tf.keras.layers.Layer, tf.keras.layers.Layer], None] = None,
                 interaction_embed_layers: Optional[int] = None,
                 interaction_embed_layer_size: Optional[int] = None):

        if isinstance(input_shape, tuple) and isinstance(input_shape[0], tuple):
            input = (tf.keras.layers.Input(shape=input_shape[0], ragged=True),
                     tf.keras.layers.Input(shape=input_shape[1], ragged=True))
            row_lengths = input[0].row_lengths()
            hidden = input[0].to_tensor()
            interaction_hidden = input[1].to_tensor()

            if preprocess is not None:
                if not isinstance(preprocess, tuple):
                    raise ValueError(
                        &#34;preprocess must be a tuple of two layers when the input is a tuple of two tensors.&#34;)

                preprocess, interaction_preprocess = preprocess
                if interaction_preprocess is not None:
                    interaction_hidden = interaction_preprocess(interaction_hidden)

            if interaction_embed_layers is None or interaction_embed_layer_size is None:
                raise ValueError(
                    &#34;&#34;&#34;interaction_embed_layers and interaction_embed_layer_size must be specified 
                    when the input is a tuple of two tensors, i.e. the interaction variables are used.&#34;&#34;&#34;)

            embed_interaction = CNNEmbedding(
                interaction_embed_layers,
                interaction_embed_layer_size,
                heads,
                activation)(interaction_hidden)
        else:
            input = tf.keras.layers.Input(shape=input_shape, ragged=True)
            embed_interaction = None
            row_lengths = input.row_lengths()
            hidden = input.to_tensor()

        if preprocess is not None:
            if isinstance(preprocess, tuple):
                raise ValueError(&#34;preprocess must be a single layer when the input is a single tensor.&#34;)
            hidden = preprocess(hidden)

        hidden = FCEmbedding(embed_dim, embed_layers, activation)(hidden)

        transformed = DeParT(self_attn_layers=self_attn_layers,
                             class_attn_layers=class_attn_layers,
                             dim=embed_dim,
                             expansion=expansion,
                             heads=heads,
                             dropout=dropout,
                             class_dropout=class_dropout,
                             activation=activation,
                             layer_scale_init_value=layer_scale_init_value,
                             stochastic_depth_drop_rate=stochastic_depth_drop_rate,
                             class_stochastic_depth_drop_rate=class_stochastic_depth_drop_rate)(hidden, mask=tf.sequence_mask(row_lengths), interaction=embed_interaction)

        transformed = tf.keras.layers.LayerNormalization()(transformed[:, 0, :])
        output = output_layer(transformed)

        super().__init__(inputs=input, outputs=output)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="jidenn.models.DeParT.CNNEmbedding"><code class="flex name class">
<span>class <span class="ident">CNNEmbedding</span></span>
<span>(</span><span>num_layers:Â int, layer_size:Â int, out_dim:Â int, activation:Â Callable[[tensorflow.python.framework.ops.Tensor],Â tensorflow.python.framework.ops.Tensor])</span>
</code></dt>
<dd>
<div class="desc"><p>Embedding layer of the interaction variables as a series of point-wise convolutional layers.
The interaction variiables are compuetd for each pair of particles.
This creates a redundancy in the input, as the matrix is symetric and the diagonal is always zero.
To save computation, the upper triangular part of the matrix is used as input, which is
flattend and the 1D convolutions are applied to it.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>num_layers</code></strong> :&ensp;<code>int</code></dt>
<dd>number of convolutional layers</dd>
<dt><strong><code>layer_size</code></strong> :&ensp;<code>int</code></dt>
<dd>number of channels of the hidden layers</dd>
<dt><strong><code>out_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>number of channels of the last convolutional layer which
is manually appended as an extra layer after <code>num_layers</code> layers.</dd>
</dl>
<p>activation (Callable[[tf.Tensor], tf.Tensor]) activation function</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CNNEmbedding(tf.keras.layers.Layer):
    &#34;&#34;&#34;Embedding layer of the interaction variables as a series of point-wise convolutional layers.
    The interaction variiables are compuetd for each pair of particles.
    This creates a redundancy in the input, as the matrix is symetric and the diagonal is always zero.
    To save computation, the upper triangular part of the matrix is used as input, which is 
    flattend and the 1D convolutions are applied to it.

    Args:
        num_layers (int): number of convolutional layers
        layer_size (int): number of channels of the hidden layers
        out_dim (int): number of channels of the last convolutional layer which 
            is manually appended as an extra layer after `num_layers` layers.
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function
    &#34;&#34;&#34;

    def __init__(self, num_layers: int, layer_size: int, out_dim: int, activation: Callable[[tf.Tensor], tf.Tensor]):
        super().__init__()
        self.activation, self.num_layers, self.layer_size, self.out_dim = activation, num_layers, layer_size, out_dim

        self.conv_layers = [tf.keras.layers.Conv1D(layer_size, 1) for _ in range(num_layers)]
        self.conv_layers.append(tf.keras.layers.Conv1D(out_dim, 1))
        self.normlizations = [tf.keras.layers.BatchNormalization() for _ in range(num_layers + 1)]
        self.activation_layer = tf.keras.layers.Activation(activation)

    def get_config(self):
        config = super(CNNEmbedding, self).get_config()
        config.update({name: getattr(self, name)
                      for name in [&#34;num_layers&#34;, &#34;layer_size&#34;, &#34;out_dim&#34;, &#34;activation&#34;]})
        return config

    def call(self, inputs):
        &#34;&#34;&#34;Forward pass of the interaction embedding layer

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, num_particles, num_features)`
                This matrix is assumed to be symetric, with zero diagonal, with only the upper-triag part 
                can be used fopr embedding. 

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, num_particles, out_dim)`
        &#34;&#34;&#34;
        ones = tf.ones_like(inputs[0, :, :, 0])
        upper_tril_mask = tf.linalg.band_part(ones, 0, -1)
        diag_mask = tf.linalg.band_part(ones, 0, 0)
        upper_tril_mask = tf.cast(upper_tril_mask - diag_mask, tf.bool)
        flattened_upper_triag = tf.boolean_mask(inputs, upper_tril_mask, axis=1)

        hidden = flattened_upper_triag
        for conv, norm in zip(self.conv_layers, self.normlizations):
            hidden = conv(hidden)
            hidden = norm(hidden)
            hidden = self.activation_layer(hidden)

        true_mask = tf.cast(tf.where(upper_tril_mask), tf.int32)
        out = tf.transpose(hidden, [1, 0, 2])
        out = tf.scatter_nd(true_mask, out, shape=[tf.shape(inputs)[1],
                            tf.shape(inputs)[2], tf.shape(inputs)[0], self.out_dim])
        out = out + tf.transpose(out, [1, 0, 2, 3])
        out = tf.transpose(out, [2, 0, 1, 3])
        return out</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="jidenn.models.DeParT.CNNEmbedding.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs)</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass of the interaction embedding layer</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>input tensor of shape <code>(batch_size, num_particles, num_particles, num_features)</code>
This matrix is assumed to be symetric, with zero diagonal, with only the upper-triag part
can be used fopr embedding. </dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tf.Tensor</code></dt>
<dd>output tensor of shape <code>(batch_size, num_particles, num_particles, out_dim)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs):
    &#34;&#34;&#34;Forward pass of the interaction embedding layer

    Args:
        inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, num_particles, num_features)`
            This matrix is assumed to be symetric, with zero diagonal, with only the upper-triag part 
            can be used fopr embedding. 

    Returns:
        tf.Tensor: output tensor of shape `(batch_size, num_particles, num_particles, out_dim)`
    &#34;&#34;&#34;
    ones = tf.ones_like(inputs[0, :, :, 0])
    upper_tril_mask = tf.linalg.band_part(ones, 0, -1)
    diag_mask = tf.linalg.band_part(ones, 0, 0)
    upper_tril_mask = tf.cast(upper_tril_mask - diag_mask, tf.bool)
    flattened_upper_triag = tf.boolean_mask(inputs, upper_tril_mask, axis=1)

    hidden = flattened_upper_triag
    for conv, norm in zip(self.conv_layers, self.normlizations):
        hidden = conv(hidden)
        hidden = norm(hidden)
        hidden = self.activation_layer(hidden)

    true_mask = tf.cast(tf.where(upper_tril_mask), tf.int32)
    out = tf.transpose(hidden, [1, 0, 2])
    out = tf.scatter_nd(true_mask, out, shape=[tf.shape(inputs)[1],
                        tf.shape(inputs)[2], tf.shape(inputs)[0], self.out_dim])
    out = out + tf.transpose(out, [1, 0, 2, 3])
    out = tf.transpose(out, [2, 0, 1, 3])
    return out</code></pre>
</details>
</dd>
<dt id="jidenn.models.DeParT.CNNEmbedding.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of
dict every time it is called. The callers should make a copy of the
returned dict if they want to modify it.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = super(CNNEmbedding, self).get_config()
    config.update({name: getattr(self, name)
                  for name in [&#34;num_layers&#34;, &#34;layer_size&#34;, &#34;out_dim&#34;, &#34;activation&#34;]})
    return config</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="jidenn.models.DeParT.ClassAttentionBlock"><code class="flex name class">
<span>class <span class="ident">ClassAttentionBlock</span></span>
<span>(</span><span>dim:Â int, heads:Â int, stoch_drop_prob:Â float, layer_scale_init_value:Â float, activation:Â Callable[[tensorflow.python.framework.ops.Tensor],Â tensorflow.python.framework.ops.Tensor], expansion:Â int, dropout:Â Optional[float]Â =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Class attention block.
It allows the class token to attend to the input particles, and then feed the attended class token
to the feed-forward network with residual connections and layer normalizations.</p>
<p>This extracts the class information from the attented particles more effectively.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>dimension of the input and output</dd>
<dt><strong><code>heads</code></strong> :&ensp;<code>int</code></dt>
<dd>number of heads</dd>
<dt><strong><code>stoch_drop_prob</code></strong> :&ensp;<code>float</code></dt>
<dd>probability of stochastic dropout</dd>
<dt><strong><code>layer_scale_init_value</code></strong> :&ensp;<code>float</code></dt>
<dd>initial value of layer scale</dd>
<dt>activation (Callable[[tf.Tensor], tf.Tensor]) activation function</dt>
<dt><strong><code>expansion</code></strong> :&ensp;<code>int</code></dt>
<dd>expansion factor of the feed-forward network,
the dimension of the feed-forward network is <code>dim * expansion</code></dd>
<dt><strong><code>dropout</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>dropout rate, defaults to None</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ClassAttentionBlock(tf.keras.layers.Layer):
    &#34;&#34;&#34;Class attention block.
    It allows the class token to attend to the input particles, and then feed the attended class token
    to the feed-forward network with residual connections and layer normalizations.

    This extracts the class information from the attented particles more effectively.

    Args:
        dim (int): dimension of the input and output
        heads (int): number of heads
        stoch_drop_prob (float): probability of stochastic dropout
        layer_scale_init_value (float): initial value of layer scale
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function
        expansion (int): expansion factor of the feed-forward network, 
            the dimension of the feed-forward network is `dim * expansion`
        dropout (float, optional): dropout rate, defaults to None
    &#34;&#34;&#34;

    def __init__(self, dim: int, heads: int, stoch_drop_prob: float, layer_scale_init_value: float, activation: Callable[[tf.Tensor], tf.Tensor], expansion: int,
                 dropout: Optional[float] = None):
        super().__init__()
        self.dim, self.heads, self.dropout, self.stoch_drop_prob, self.layer_scale_init_value, self.activation, self.expansion = dim, heads, dropout, stoch_drop_prob, layer_scale_init_value, activation, expansion

        self.pre_mhca_ln = tf.keras.layers.LayerNormalization()
        self.mhca = MultiheadClassAttention(dim, heads, dropout)
        self.post_mhca_scale = LayerScale(layer_scale_init_value, dim)
        self.post_mhca_stoch_depth = StochasticDepth(drop_prob=stoch_drop_prob)

        self.pre_ffn_ln = tf.keras.layers.LayerNormalization()
        self.ffn = FFN(dim, expansion, activation, dropout)
        self.post_ffn_scale = LayerScale(layer_scale_init_value, dim)
        self.post_ffn_stoch_depth = StochasticDepth(drop_prob=stoch_drop_prob)

    def get_config(self):
        config = super(ClassAttentionBlock, self).get_config()
        config.update({&#34;dim&#34;: self.dim, &#34;heads&#34;: self.heads, &#34;dropout&#34;: self.dropout, &#34;stoch_drop_prob&#34;: self.stoch_drop_prob,
                      &#34;layer_scale_init_value&#34;: self.layer_scale_init_value, &#34;activation&#34;: self.activation, &#34;expansion&#34;: self.expansion})
        return config

    def call(self, inputs: tf.Tensor, class_token: tf.Tensor, mask: tf.Tensor) -&gt; tf.Tensor:
        &#34;&#34;&#34;Forward pass of the class attention block

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
            class_token (tf.Tensor): class token tensor of shape `(batch_size, 1, dim)`
                It is concatenated with the input tensor along the particle dimension,
                at the front of the input tensor.
            mask (tf.Tensor): mask tensor of shape `(batch_size, 1, num_particles)`
                This mask is used to mask out the attention of padding particles, generated when
                tf.RaggedTensor is converted to tf.Tensor.

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, 1, dim)`, an updated class token
        &#34;&#34;&#34;
        attented = tf.concat([class_token, inputs], axis=1)
        attented = self.pre_mhca_ln(attented)
        attented = self.mhca(query=class_token, inputs=attented, mask=mask)
        attented = self.post_mhca_scale(attented)
        attented = self.post_mhca_stoch_depth(attented)
        attented = attented + class_token

        ffned = self.pre_ffn_ln(attented)
        ffned = self.ffn(ffned)
        ffned = self.post_ffn_scale(ffned)
        ffned = self.post_ffn_stoch_depth(ffned)
        output = ffned + attented
        return output</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="jidenn.models.DeParT.ClassAttentionBlock.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs:Â tensorflow.python.framework.ops.Tensor, class_token:Â tensorflow.python.framework.ops.Tensor, mask:Â tensorflow.python.framework.ops.Tensor) â€‘>Â tensorflow.python.framework.ops.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass of the class attention block</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>input tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
<dt><strong><code>class_token</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>class token tensor of shape <code>(batch_size, 1, dim)</code>
It is concatenated with the input tensor along the particle dimension,
at the front of the input tensor.</dd>
<dt><strong><code>mask</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>mask tensor of shape <code>(batch_size, 1, num_particles)</code>
This mask is used to mask out the attention of padding particles, generated when
tf.RaggedTensor is converted to tf.Tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tf.Tensor</code></dt>
<dd>output tensor of shape <code>(batch_size, 1, dim)</code>, an updated class token</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs: tf.Tensor, class_token: tf.Tensor, mask: tf.Tensor) -&gt; tf.Tensor:
    &#34;&#34;&#34;Forward pass of the class attention block

    Args:
        inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
        class_token (tf.Tensor): class token tensor of shape `(batch_size, 1, dim)`
            It is concatenated with the input tensor along the particle dimension,
            at the front of the input tensor.
        mask (tf.Tensor): mask tensor of shape `(batch_size, 1, num_particles)`
            This mask is used to mask out the attention of padding particles, generated when
            tf.RaggedTensor is converted to tf.Tensor.

    Returns:
        tf.Tensor: output tensor of shape `(batch_size, 1, dim)`, an updated class token
    &#34;&#34;&#34;
    attented = tf.concat([class_token, inputs], axis=1)
    attented = self.pre_mhca_ln(attented)
    attented = self.mhca(query=class_token, inputs=attented, mask=mask)
    attented = self.post_mhca_scale(attented)
    attented = self.post_mhca_stoch_depth(attented)
    attented = attented + class_token

    ffned = self.pre_ffn_ln(attented)
    ffned = self.ffn(ffned)
    ffned = self.post_ffn_scale(ffned)
    ffned = self.post_ffn_stoch_depth(ffned)
    output = ffned + attented
    return output</code></pre>
</details>
</dd>
<dt id="jidenn.models.DeParT.ClassAttentionBlock.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of
dict every time it is called. The callers should make a copy of the
returned dict if they want to modify it.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = super(ClassAttentionBlock, self).get_config()
    config.update({&#34;dim&#34;: self.dim, &#34;heads&#34;: self.heads, &#34;dropout&#34;: self.dropout, &#34;stoch_drop_prob&#34;: self.stoch_drop_prob,
                  &#34;layer_scale_init_value&#34;: self.layer_scale_init_value, &#34;activation&#34;: self.activation, &#34;expansion&#34;: self.expansion})
    return config</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="jidenn.models.DeParT.DeParT"><code class="flex name class">
<span>class <span class="ident">DeParT</span></span>
<span>(</span><span>self_attn_layers:Â int, class_attn_layers:Â int, dim:Â int, expansion:Â int, heads:Â int, activation:Â Callable[[tensorflow.python.framework.ops.Tensor],Â tensorflow.python.framework.ops.Tensor], layer_scale_init_value:Â float, stochastic_depth_drop_rate:Â Optional[float]Â =Â None, class_stochastic_depth_drop_rate:Â Optional[float]Â =Â None, class_dropout:Â Optional[float]Â =Â None, dropout:Â Optional[float]Â =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Pure DeParT layers without the embedding and output layers.</p>
<p>It also creates the class token, which is used to encode the global information of the input,
using the ClassAttentionBlock.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>dimension of the input and output</dd>
<dt><strong><code>self_attn_layers</code></strong> :&ensp;<code>int</code></dt>
<dd>number of self-attention layers</dd>
<dt><strong><code>class_attn_layers</code></strong> :&ensp;<code>int</code></dt>
<dd>number of class-attention layers</dd>
<dt><strong><code>expansion</code></strong> :&ensp;<code>int</code></dt>
<dd>expansion factor of the hidden layer, i.e. the hidden layer has size <code>dim * expansion</code></dd>
<dt><strong><code>heads</code></strong> :&ensp;<code>int</code></dt>
<dd>number of heads</dd>
<dt>activation (Callable[[tf.Tensor], tf.Tensor]) activation function</dt>
<dt><strong><code>layer_scale_init_value</code></strong> :&ensp;<code>float</code></dt>
<dd>initial value of layer scale. </dd>
<dt><strong><code>stochastic_depth_drop_rate</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>drop rate of stochastic depth</dd>
<dt><strong><code>class_stochastic_depth_drop_rate</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>drop rate of stochastic depth of the class token</dd>
<dt><strong><code>dropout</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>dropout rate. Defaults to None.</dd>
<dt><strong><code>class_dropout</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>dropout rate of the class token. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DeParT(tf.keras.layers.Layer):
    &#34;&#34;&#34;Pure DeParT layers without the embedding and output layers.

    It also creates the class token, which is used to encode the global information of the input,
    using the ClassAttentionBlock.

    Args:
        dim (int): dimension of the input and output
        self_attn_layers (int): number of self-attention layers
        class_attn_layers (int): number of class-attention layers
        expansion (int): expansion factor of the hidden layer, i.e. the hidden layer has size `dim * expansion`
        heads (int): number of heads
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function
        layer_scale_init_value (float): initial value of layer scale. 
        stochastic_depth_drop_rate (float, optional): drop rate of stochastic depth
        class_stochastic_depth_drop_rate (float, optional): drop rate of stochastic depth of the class token
        dropout (float, optional): dropout rate. Defaults to None.
        class_dropout (float, optional): dropout rate of the class token. Defaults to None.
    &#34;&#34;&#34;

    def __init__(self,
                 self_attn_layers: int,
                 class_attn_layers: int,
                 dim: int,
                 expansion: int,
                 heads: int,
                 activation: Callable[[tf.Tensor], tf.Tensor],
                 layer_scale_init_value: float,
                 stochastic_depth_drop_rate: Optional[float] = None,
                 class_stochastic_depth_drop_rate: Optional[float] = None,
                 class_dropout: Optional[float] = None,
                 dropout: Optional[float] = None,):
        # Make sure `dim` is even.
        assert dim % 2 == 0

        super().__init__()
        self.layers, self.dim, self.expansion, self.heads, self.dropout, self.activation, self.class_layers = self_attn_layers, dim, expansion, heads, dropout, activation, class_attn_layers
        self.layer_scale_init_value, self.stochastic_depth_drop_rate, self.class_stochastic_depth_drop_rate = layer_scale_init_value, stochastic_depth_drop_rate, class_stochastic_depth_drop_rate
        self.class_dropout = class_dropout

        self.sa_layers = [SelfAttentionBlock(dim,
                                             heads,
                                             self.stochastic_prob(i, self_attn_layers - 1,
                                                                  stochastic_depth_drop_rate),
                                             layer_scale_init_value,
                                             activation,
                                             expansion,
                                             dropout,) for i in range(self_attn_layers)]

        self.ca_layers = [ClassAttentionBlock(dim,
                                              heads,
                                              self.stochastic_prob(i, class_attn_layers - 1,
                                                                   class_stochastic_depth_drop_rate),
                                              layer_scale_init_value,
                                              activation,
                                              expansion,
                                              class_dropout,) for i in range(class_attn_layers)]

        self.cls_token = tf.Variable(tf.random.truncated_normal((1, 1, dim), stddev=0.02), trainable=True)

    def stochastic_prob(self, step, total_steps, drop_rate):
        return drop_rate * step / total_steps

    def get_config(self):
        config = super(DeParT, self).get_config()
        config.update({name: getattr(self, name)
                      for name in [&#34;layers&#34;, &#34;dim&#34;, &#34;expansion&#34;, &#34;heads&#34;, &#34;dropout&#34;, &#34;activation&#34;, &#34;class_layers&#34;,
                                   &#34;layer_scale_init_value&#34;, &#34;stochastic_depth_drop_rate&#34;, &#34;class_stochastic_depth_drop_rate&#34;, &#34;class_dropout&#34;]})
        return config

    def call(self, inputs: tf.Tensor, mask: tf.Tensor, interaction: Optional[tf.Tensor] = None) -&gt; tf.Tensor:
        &#34;&#34;&#34;Forward pass of the DeParT layers

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
            mask (tf.Tensor): mask tensor of shape `(batch_size, num_particles)`.
                From the mask, a mask tensor of shape `(batch_size, num_particles, num_particles)`
                is calculated, which is used to mask out the attention of padding particles, generated when
                `tf.RaggedTensor` is converted to `tf.Tensor`.
            interaction (tf.Tensor, optional): interaction tensor of shape `(batch_size, num_particles, num_particles, heads)`

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
        &#34;&#34;&#34;
        sa_mask = mask[:, tf.newaxis, tf.newaxis, :] &amp; mask[:, tf.newaxis, :, tf.newaxis]
        hidden = inputs
        for layer in self.sa_layers:
            hidden = layer(hidden, sa_mask, interaction)

        cls_token = tf.tile(self.cls_token, (tf.shape(inputs)[0], 1, 1))
        class_mask = mask[:, tf.newaxis, :]
        class_mask = tf.concat([tf.ones((tf.shape(inputs)[0], 1, 1), dtype=tf.bool), class_mask], axis=2)
        for layer in self.ca_layers:
            cls_token = layer(hidden, cls_token=cls_token, mask=class_mask)
        return cls_token</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="jidenn.models.DeParT.DeParT.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs:Â tensorflow.python.framework.ops.Tensor, mask:Â tensorflow.python.framework.ops.Tensor, interaction:Â Optional[tensorflow.python.framework.ops.Tensor]Â =Â None) â€‘>Â tensorflow.python.framework.ops.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass of the DeParT layers</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>input tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
<dt><strong><code>mask</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>mask tensor of shape <code>(batch_size, num_particles)</code>.
From the mask, a mask tensor of shape <code>(batch_size, num_particles, num_particles)</code>
is calculated, which is used to mask out the attention of padding particles, generated when
<code>tf.RaggedTensor</code> is converted to <code>tf.Tensor</code>.</dd>
<dt><strong><code>interaction</code></strong> :&ensp;<code>tf.Tensor</code>, optional</dt>
<dd>interaction tensor of shape <code>(batch_size, num_particles, num_particles, heads)</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tf.Tensor</code></dt>
<dd>output tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs: tf.Tensor, mask: tf.Tensor, interaction: Optional[tf.Tensor] = None) -&gt; tf.Tensor:
    &#34;&#34;&#34;Forward pass of the DeParT layers

    Args:
        inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
        mask (tf.Tensor): mask tensor of shape `(batch_size, num_particles)`.
            From the mask, a mask tensor of shape `(batch_size, num_particles, num_particles)`
            is calculated, which is used to mask out the attention of padding particles, generated when
            `tf.RaggedTensor` is converted to `tf.Tensor`.
        interaction (tf.Tensor, optional): interaction tensor of shape `(batch_size, num_particles, num_particles, heads)`

    Returns:
        tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
    &#34;&#34;&#34;
    sa_mask = mask[:, tf.newaxis, tf.newaxis, :] &amp; mask[:, tf.newaxis, :, tf.newaxis]
    hidden = inputs
    for layer in self.sa_layers:
        hidden = layer(hidden, sa_mask, interaction)

    cls_token = tf.tile(self.cls_token, (tf.shape(inputs)[0], 1, 1))
    class_mask = mask[:, tf.newaxis, :]
    class_mask = tf.concat([tf.ones((tf.shape(inputs)[0], 1, 1), dtype=tf.bool), class_mask], axis=2)
    for layer in self.ca_layers:
        cls_token = layer(hidden, cls_token=cls_token, mask=class_mask)
    return cls_token</code></pre>
</details>
</dd>
<dt id="jidenn.models.DeParT.DeParT.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of
dict every time it is called. The callers should make a copy of the
returned dict if they want to modify it.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = super(DeParT, self).get_config()
    config.update({name: getattr(self, name)
                  for name in [&#34;layers&#34;, &#34;dim&#34;, &#34;expansion&#34;, &#34;heads&#34;, &#34;dropout&#34;, &#34;activation&#34;, &#34;class_layers&#34;,
                               &#34;layer_scale_init_value&#34;, &#34;stochastic_depth_drop_rate&#34;, &#34;class_stochastic_depth_drop_rate&#34;, &#34;class_dropout&#34;]})
    return config</code></pre>
</details>
</dd>
<dt id="jidenn.models.DeParT.DeParT.stochastic_prob"><code class="name flex">
<span>def <span class="ident">stochastic_prob</span></span>(<span>self, step, total_steps, drop_rate)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stochastic_prob(self, step, total_steps, drop_rate):
    return drop_rate * step / total_steps</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="jidenn.models.DeParT.DeParTModel"><code class="flex name class">
<span>class <span class="ident">DeParTModel</span></span>
<span>(</span><span>input_shape:Â Union[Tuple[None,Â int],Â Tuple[Tuple[None,Â int],Â Tuple[None,Â None,Â int]]], embed_dim:Â int, embed_layers:Â int, self_attn_layers:Â int, class_attn_layers:Â int, expansion:Â int, heads:Â int, layer_scale_init_value:Â float, stochastic_depth_drop_rate:Â float, class_stochastic_depth_drop_rate:Â float, output_layer:Â keras.engine.base_layer.Layer, activation:Â Callable[[tensorflow.python.framework.ops.Tensor],Â tensorflow.python.framework.ops.Tensor], dropout:Â Optional[float]Â =Â None, class_dropout:Â Optional[float]Â =Â None, preprocess:Â Union[keras.engine.base_layer.Layer,Â ForwardRef(None),Â Tuple[keras.engine.base_layer.Layer,Â keras.engine.base_layer.Layer]]Â =Â None, interaction_embed_layers:Â Optional[int]Â =Â None, interaction_embed_layer_size:Â Optional[int]Â =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>DeParT model with embwith embedding and output layers.</p>
<p>The model already contains the <code>tf.keras.layers.Input</code> layer, so it can be used as a standalone model.</p>
<p>The input tensor can be either a tensor of shape <code>(batch_size, num_particles, num_features)</code> or
a tuple of tensors <code>(particle_tensor, interaction_tensor)</code> of shapes
<code>(batch_size, num_particles, num_features)</code> and <code>(batch_size, num_particles, num_particles, num_features)</code>, respectively.</p>
<p>The model can be used with or without the interaction tensor, depending on the type of the input shape,
if it is a tuple, the interaction tensor is assumed to be present.</p>
<p>The input tensor is first passed through the embedding layer, then the ParT layers, and finally the output layer.
If the interaction tensor is present, it is passed through the interaction embedding layer before the ParT layers.</p>
<p>If the preprocessing layer is not None, the input tensor is first passed through the preprocessing layer before the embedding layer.
If the interaction tensor is present, it is passed through the preprocessing layer is an tuple of two layers,
each of which is applied to the particle and interaction tensors, respectively.</p>
<p>The output of ParT is a vector of shape <code>(batch_size, embed_dim)</code> with extracted class infromation.
This is then passed through the output layer.
Layer normalization is applied to the output of the DeParT layers before the output layer.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_shape</code></strong> :&ensp;<code>Union[Tuple[None, int], Tuple[Tuple[None, int], Tuple[None, None, int]]]</code></dt>
<dd>shape of the input tensor.
If the interaction tensor is present, it is assumed to be a tuple of two shapes,
each creating a separate input layer.</dd>
<dt><strong><code>embed_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>dimension of the embedding layer</dd>
<dt><strong><code>embed_layers</code></strong> :&ensp;<code>int</code></dt>
<dd>number of layers of the embedding layer</dd>
<dt><strong><code>self_attn_layers</code></strong> :&ensp;<code>int</code></dt>
<dd>number of self-attention layers</dd>
<dt><strong><code>class_attn_layers</code></strong> :&ensp;<code>int</code></dt>
<dd>number of class-attention layers</dd>
<dt><strong><code>expansion</code></strong> :&ensp;<code>int</code></dt>
<dd>expansion factor of the self-attention layers</dd>
<dt><strong><code>heads</code></strong> :&ensp;<code>int</code></dt>
<dd>number of heads of the self-attention layers</dd>
<dt><strong><code>layer_scale_init_value</code></strong> :&ensp;<code>float</code></dt>
<dd>initial value of the layer scale parameter</dd>
<dt><strong><code>stochastic_depth_drop_rate</code></strong> :&ensp;<code>float</code></dt>
<dd>drop rate of the stochastic depth regularization</dd>
<dt><strong><code>class_stochastic_depth_drop_rate</code></strong> :&ensp;<code>float</code></dt>
<dd>drop rate of the stochastic depth regularization of the class-attention layers</dd>
<dt><strong><code>output_layer</code></strong> :&ensp;<code>tf.keras.layers.Layer</code></dt>
<dd>output layer</dd>
<dt><strong><code>activation</code></strong> :&ensp;<code>Callable[[tf.Tensor], tf.Tensor]</code></dt>
<dd>activation function</dd>
<dt><strong><code>dropout</code></strong> :&ensp;<code>Optional[float]</code>, optional</dt>
<dd>dropout rate. Defaults to None.</dd>
<dt><strong><code>class_dropout</code></strong> :&ensp;<code>Optional[float]</code>, optional</dt>
<dd>dropout rate of the class-attention layers. Defaults to None.</dd>
<dt><strong><code>interaction_embed_layers</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>number of layers of the interaction embedding layer. Defaults to None.</dd>
<dt><strong><code>interaction_embed_layer_size</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>size of the layers of the interaction embedding layer. Defaults to None.</dd>
<dt><strong><code>preprocess</code></strong> :&ensp;<code>Union[tf.keras.layers.Layer, None, Tuple[tf.keras.layers.Layer, tf.keras.layers.Layer]]</code>, optional</dt>
<dd>preprocessing layer. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DeParTModel(tf.keras.Model):
    &#34;&#34;&#34;DeParT model with embwith embedding and output layers.

    The model already contains the `tf.keras.layers.Input` layer, so it can be used as a standalone model.

    The input tensor can be either a tensor of shape `(batch_size, num_particles, num_features)` or
    a tuple of tensors `(particle_tensor, interaction_tensor)` of shapes
    `(batch_size, num_particles, num_features)` and `(batch_size, num_particles, num_particles, num_features)`, respectively.

    The model can be used with or without the interaction tensor, depending on the type of the input shape,
    if it is a tuple, the interaction tensor is assumed to be present.

    The input tensor is first passed through the embedding layer, then the ParT layers, and finally the output layer.
    If the interaction tensor is present, it is passed through the interaction embedding layer before the ParT layers.

    If the preprocessing layer is not None, the input tensor is first passed through the preprocessing layer before the embedding layer.
    If the interaction tensor is present, it is passed through the preprocessing layer is an tuple of two layers,
    each of which is applied to the particle and interaction tensors, respectively.

    The output of ParT is a vector of shape `(batch_size, embed_dim)` with extracted class infromation.
    This is then passed through the output layer.
    Layer normalization is applied to the output of the DeParT layers before the output layer.

    Args:
        input_shape (Union[Tuple[None, int], Tuple[Tuple[None, int], Tuple[None, None, int]]]): shape of the input tensor.
            If the interaction tensor is present, it is assumed to be a tuple of two shapes,
            each creating a separate input layer.
        embed_dim (int): dimension of the embedding layer
        embed_layers (int): number of layers of the embedding layer
        self_attn_layers (int): number of self-attention layers
        class_attn_layers (int): number of class-attention layers
        expansion (int): expansion factor of the self-attention layers
        heads (int): number of heads of the self-attention layers
        layer_scale_init_value (float): initial value of the layer scale parameter
        stochastic_depth_drop_rate (float): drop rate of the stochastic depth regularization
        class_stochastic_depth_drop_rate (float): drop rate of the stochastic depth regularization of the class-attention layers
        output_layer (tf.keras.layers.Layer): output layer
        activation (Callable[[tf.Tensor], tf.Tensor]): activation function
        dropout (Optional[float], optional): dropout rate. Defaults to None.
        class_dropout (Optional[float], optional): dropout rate of the class-attention layers. Defaults to None.
        interaction_embed_layers (Optional[int], optional): number of layers of the interaction embedding layer. Defaults to None.
        interaction_embed_layer_size (Optional[int], optional): size of the layers of the interaction embedding layer. Defaults to None.
        preprocess (Union[tf.keras.layers.Layer, None, Tuple[tf.keras.layers.Layer, tf.keras.layers.Layer]], optional): preprocessing layer. Defaults to None.

    &#34;&#34;&#34;

    def __init__(self,
                 input_shape: Union[Tuple[None, int], Tuple[Tuple[None, int], Tuple[None, None, int]]],
                 embed_dim: int,
                 embed_layers: int,
                 self_attn_layers: int,
                 class_attn_layers: int,
                 expansion: int,
                 heads: int,
                 layer_scale_init_value: float,
                 stochastic_depth_drop_rate: float,
                 class_stochastic_depth_drop_rate: float,
                 output_layer: tf.keras.layers.Layer,
                 activation: Callable[[tf.Tensor], tf.Tensor],
                 dropout: Optional[float] = None,
                 class_dropout: Optional[float] = None,
                 preprocess: Union[tf.keras.layers.Layer,
                                   Tuple[tf.keras.layers.Layer, tf.keras.layers.Layer], None] = None,
                 interaction_embed_layers: Optional[int] = None,
                 interaction_embed_layer_size: Optional[int] = None):

        if isinstance(input_shape, tuple) and isinstance(input_shape[0], tuple):
            input = (tf.keras.layers.Input(shape=input_shape[0], ragged=True),
                     tf.keras.layers.Input(shape=input_shape[1], ragged=True))
            row_lengths = input[0].row_lengths()
            hidden = input[0].to_tensor()
            interaction_hidden = input[1].to_tensor()

            if preprocess is not None:
                if not isinstance(preprocess, tuple):
                    raise ValueError(
                        &#34;preprocess must be a tuple of two layers when the input is a tuple of two tensors.&#34;)

                preprocess, interaction_preprocess = preprocess
                if interaction_preprocess is not None:
                    interaction_hidden = interaction_preprocess(interaction_hidden)

            if interaction_embed_layers is None or interaction_embed_layer_size is None:
                raise ValueError(
                    &#34;&#34;&#34;interaction_embed_layers and interaction_embed_layer_size must be specified 
                    when the input is a tuple of two tensors, i.e. the interaction variables are used.&#34;&#34;&#34;)

            embed_interaction = CNNEmbedding(
                interaction_embed_layers,
                interaction_embed_layer_size,
                heads,
                activation)(interaction_hidden)
        else:
            input = tf.keras.layers.Input(shape=input_shape, ragged=True)
            embed_interaction = None
            row_lengths = input.row_lengths()
            hidden = input.to_tensor()

        if preprocess is not None:
            if isinstance(preprocess, tuple):
                raise ValueError(&#34;preprocess must be a single layer when the input is a single tensor.&#34;)
            hidden = preprocess(hidden)

        hidden = FCEmbedding(embed_dim, embed_layers, activation)(hidden)

        transformed = DeParT(self_attn_layers=self_attn_layers,
                             class_attn_layers=class_attn_layers,
                             dim=embed_dim,
                             expansion=expansion,
                             heads=heads,
                             dropout=dropout,
                             class_dropout=class_dropout,
                             activation=activation,
                             layer_scale_init_value=layer_scale_init_value,
                             stochastic_depth_drop_rate=stochastic_depth_drop_rate,
                             class_stochastic_depth_drop_rate=class_stochastic_depth_drop_rate)(hidden, mask=tf.sequence_mask(row_lengths), interaction=embed_interaction)

        transformed = tf.keras.layers.LayerNormalization()(transformed[:, 0, :])
        output = output_layer(transformed)

        super().__init__(inputs=input, outputs=output)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.training.Model</li>
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
<li>keras.utils.version_utils.ModelVersionSelector</li>
</ul>
</dd>
<dt id="jidenn.models.DeParT.FCEmbedding"><code class="flex name class">
<span>class <span class="ident">FCEmbedding</span></span>
<span>(</span><span>embedding_dim:Â int, num_embeding_layers:Â int, activation:Â Callable[[tensorflow.python.framework.ops.Tensor],Â tensorflow.python.framework.ops.Tensor])</span>
</code></dt>
<dd>
<div class="desc"><p>Embedding layer as a series of fully-connected layers.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>embed_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>dimension of the embedding</dd>
<dt><strong><code>embed_layers</code></strong> :&ensp;<code>int</code></dt>
<dd>number of fully-connected layers</dd>
</dl>
<p>activation (Callable[[tf.Tensor], tf.Tensor]) activation function</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FCEmbedding(tf.keras.layers.Layer):
    &#34;&#34;&#34;Embedding layer as a series of fully-connected layers.

    Args:
        embed_dim (int): dimension of the embedding
        embed_layers (int): number of fully-connected layers
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function
    &#34;&#34;&#34;

    def __init__(self, embedding_dim: int, num_embeding_layers: int, activation: Callable[[tf.Tensor], tf.Tensor], ):

        super().__init__()
        self.embedding_dim, self.activation, self.num_embeding_layers = embedding_dim, activation, num_embeding_layers
        self.layers = [tf.keras.layers.Dense(self.embedding_dim, activation=self.activation)
                       for _ in range(self.num_embeding_layers)]

    def get_config(self):
        config = super(FCEmbedding, self).get_config()
        config.update({name: getattr(self, name) for name in [&#34;embedding_dim&#34;, &#34;num_embeding_layers&#34;, &#34;activation&#34;]})
        return config

    def call(self, inputs):
        &#34;&#34;&#34;Forward pass of the embedding layer

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, num_features)`

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, embed_dim)`
        &#34;&#34;&#34;
        hidden = inputs
        for layer in self.layers:
            hidden = layer(hidden)
        return hidden</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="jidenn.models.DeParT.FCEmbedding.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs)</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass of the embedding layer</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>input tensor of shape <code>(batch_size, num_particles, num_features)</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tf.Tensor</code></dt>
<dd>output tensor of shape <code>(batch_size, num_particles, embed_dim)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs):
    &#34;&#34;&#34;Forward pass of the embedding layer

    Args:
        inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, num_features)`

    Returns:
        tf.Tensor: output tensor of shape `(batch_size, num_particles, embed_dim)`
    &#34;&#34;&#34;
    hidden = inputs
    for layer in self.layers:
        hidden = layer(hidden)
    return hidden</code></pre>
</details>
</dd>
<dt id="jidenn.models.DeParT.FCEmbedding.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of
dict every time it is called. The callers should make a copy of the
returned dict if they want to modify it.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = super(FCEmbedding, self).get_config()
    config.update({name: getattr(self, name) for name in [&#34;embedding_dim&#34;, &#34;num_embeding_layers&#34;, &#34;activation&#34;]})
    return config</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="jidenn.models.DeParT.FFN"><code class="flex name class">
<span>class <span class="ident">FFN</span></span>
<span>(</span><span>dim:Â int, expansion:Â int, activation:Â Callable[[tensorflow.python.framework.ops.Tensor],Â tensorflow.python.framework.ops.Tensor], dropout:Â Optional[float]Â =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Feed-forward network
On top of the Transformer FFN layer, it adds a layer normalization in between the two dense layers.</p>
<p>On top of ParT FFN layer, it adds a gated linear unit (GLU) activation function.
This adds additional weights to the layer, so to keep the number of parameters the same,
the size of the first hidden layer is <code>dim * expansion * 2 / 3</code> and the gate
hidden layer is the same dimension. </p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>dimension of the input and output</dd>
<dt><strong><code>expansion</code></strong> :&ensp;<code>int</code></dt>
<dd>expansion factor of the hidden layer, i.e. the hidden layer has size <code>dim * expansion</code></dd>
<dt>activation (Callable[[tf.Tensor], tf.Tensor]) activation function</dt>
<dt><strong><code>dropout</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>dropout rate. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FFN(tf.keras.layers.Layer):
    &#34;&#34;&#34;Feed-forward network
    On top of the Transformer FFN layer, it adds a layer normalization in between the two dense layers.

    On top of ParT FFN layer, it adds a gated linear unit (GLU) activation function.
    This adds additional weights to the layer, so to keep the number of parameters the same,
    the size of the first hidden layer is `dim * expansion * 2 / 3` and the gate 
    hidden layer is the same dimension. 

    Args:
        dim (int): dimension of the input and output
        expansion (int): expansion factor of the hidden layer, i.e. the hidden layer has size `dim * expansion`
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function
        dropout (float, optional): dropout rate. Defaults to None.
    &#34;&#34;&#34;

    def __init__(self, dim: int, expansion: int, activation: Callable[[tf.Tensor], tf.Tensor], dropout: Optional[float] = None):
        super().__init__()
        self.dim, self.expansion, self.activation, self.dropout = dim, expansion, activation, dropout

        self.wide_dense = tf.keras.layers.Dense(int(dim * expansion * 2 / 3), activation=activation, use_bias=False)
        self.gate_dense = tf.keras.layers.Dense(int(dim * expansion * 2 / 3), activation=None, use_bias=False)
        self.dense = tf.keras.layers.Dense(dim, activation=None, use_bias=False)
        self.ln = tf.keras.layers.LayerNormalization()
        self.layer_dropout = tf.keras.layers.Dropout(dropout)

    def get_config(self):
        config = super().get_config()
        config.update({&#34;dim&#34;: self.dim, &#34;expansion&#34;: self.expansion,
                      &#34;activation&#34;: self.activation, &#34;dropout&#34;: self.dropout})
        return config

    def call(self, inputs):
        &#34;&#34;&#34;Forward pass of the feed-forward network
        Includes a layer normalization layer in between the two dense layers

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
        &#34;&#34;&#34;
        output = self.wide_dense(inputs) * self.gate_dense(inputs)
        output = self.ln(output)
        output = self.dense(output)
        output = self.layer_dropout(output)
        return output</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="jidenn.models.DeParT.FFN.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs)</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass of the feed-forward network
Includes a layer normalization layer in between the two dense layers</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>input tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tf.Tensor</code></dt>
<dd>output tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs):
    &#34;&#34;&#34;Forward pass of the feed-forward network
    Includes a layer normalization layer in between the two dense layers

    Args:
        inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`

    Returns:
        tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
    &#34;&#34;&#34;
    output = self.wide_dense(inputs) * self.gate_dense(inputs)
    output = self.ln(output)
    output = self.dense(output)
    output = self.layer_dropout(output)
    return output</code></pre>
</details>
</dd>
<dt id="jidenn.models.DeParT.FFN.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of
dict every time it is called. The callers should make a copy of the
returned dict if they want to modify it.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = super().get_config()
    config.update({&#34;dim&#34;: self.dim, &#34;expansion&#34;: self.expansion,
                  &#34;activation&#34;: self.activation, &#34;dropout&#34;: self.dropout})
    return config</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="jidenn.models.DeParT.LayerScale"><code class="flex name class">
<span>class <span class="ident">LayerScale</span></span>
<span>(</span><span>init_values:Â float, dim:Â int)</span>
</code></dt>
<dd>
<div class="desc"><p>Layer scale layer
Layer Scale layer helps to stabilize the training of the model.
When the model has a large number of layers, the variance of the input to each layer can be very different.
To stabilize the training, we scale the input to each layer by a learnable scalar parameter,
which is initialized to a small value.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>init_values</code></strong> :&ensp;<code>float</code></dt>
<dd>initial value of the layer scale</dd>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>dimension of the input and output</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LayerScale(tf.keras.layers.Layer):
    &#34;&#34;&#34;Layer scale layer
    Layer Scale layer helps to stabilize the training of the model.
    When the model has a large number of layers, the variance of the input to each layer can be very different.
    To stabilize the training, we scale the input to each layer by a learnable scalar parameter,
    which is initialized to a small value.

    Args:
        init_values (float): initial value of the layer scale
        dim (int): dimension of the input and output
    &#34;&#34;&#34;

    def __init__(self, init_values: float, dim: int, ):
        super().__init__()
        self.gamma = tf.Variable(init_values * tf.ones((dim,)))

    def call(self, x: tf.Tensor, training=False) -&gt; tf.Tensor:
        &#34;&#34;&#34;Forward pass of the layer scale layer

        Args:
            x (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
        &#34;&#34;&#34;
        return x * self.gamma</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="jidenn.models.DeParT.LayerScale.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, x:Â tensorflow.python.framework.ops.Tensor, training=False) â€‘>Â tensorflow.python.framework.ops.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass of the layer scale layer</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>input tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tf.Tensor</code></dt>
<dd>output tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, x: tf.Tensor, training=False) -&gt; tf.Tensor:
    &#34;&#34;&#34;Forward pass of the layer scale layer

    Args:
        x (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
    Returns:
        tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
    &#34;&#34;&#34;
    return x * self.gamma</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="jidenn.models.DeParT.MultiheadClassAttention"><code class="flex name class">
<span>class <span class="ident">MultiheadClassAttention</span></span>
<span>(</span><span>dim:Â int, heads:Â int, dropout:Â Optional[float]Â =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Multi-head class attention layer
This layer is a wrapper around the <code>tf.keras.layers.MultiHeadAttention</code> layer,
to fix the key, and value to be the same as the input, and only use the class token
as the query.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>dimension of the input and output</dd>
<dt><strong><code>heads</code></strong> :&ensp;<code>int</code></dt>
<dd>number of heads</dd>
<dt><strong><code>dropout</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>dropout rate, defaults to None</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiheadClassAttention(tf.keras.layers.Layer):
    &#34;&#34;&#34;Multi-head class attention layer
    This layer is a wrapper around the `tf.keras.layers.MultiHeadAttention` layer, 
    to fix the key, and value to be the same as the input, and only use the class token
    as the query.

    Args:
        dim (int): dimension of the input and output
        heads (int): number of heads
        dropout (float, optional): dropout rate, defaults to None
    &#34;&#34;&#34;

    def __init__(self, dim: int, heads: int, dropout: Optional[float] = None):
        super().__init__()
        self.dim, self.heads, self.dropout = dim, heads, dropout
        self.mha = tf.keras.layers.MultiHeadAttention(key_dim=dim // heads, num_heads=heads, dropout=dropout)

    def get_config(self):
        config = super(MultiheadClassAttention, self).get_config()
        config.update({&#34;dim&#34;: self.dim, &#34;heads&#34;: self.heads, &#34;dropout&#34;: self.dropout})
        return config

    def call(self, query, inputs, mask):
        &#34;&#34;&#34;Forward pass of the multi-head self-attention layer

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
            class_token (tf.Tensor): class token tensor of shape `(batch_size, 1, dim)`
            mask (tf.Tensor): mask tensor of shape `(batch_size, 1, num_particles)`
                This mask is used to mask out the attention of padding particles, generated when
                tf.RaggedTensor is converted to tf.Tensor.
        Returns:
            tf.Tensor: output tensor of shape `(batch_size, 1, dim)`
        &#34;&#34;&#34;
        output = self.mha(query=query, value=inputs, key=inputs, attention_mask=mask)
        return output</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="jidenn.models.DeParT.MultiheadClassAttention.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, query, inputs, mask)</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass of the multi-head self-attention layer</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>input tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
<dt><strong><code>class_token</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>class token tensor of shape <code>(batch_size, 1, dim)</code></dd>
<dt><strong><code>mask</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>mask tensor of shape <code>(batch_size, 1, num_particles)</code>
This mask is used to mask out the attention of padding particles, generated when
tf.RaggedTensor is converted to tf.Tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tf.Tensor</code></dt>
<dd>output tensor of shape <code>(batch_size, 1, dim)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, query, inputs, mask):
    &#34;&#34;&#34;Forward pass of the multi-head self-attention layer

    Args:
        inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
        class_token (tf.Tensor): class token tensor of shape `(batch_size, 1, dim)`
        mask (tf.Tensor): mask tensor of shape `(batch_size, 1, num_particles)`
            This mask is used to mask out the attention of padding particles, generated when
            tf.RaggedTensor is converted to tf.Tensor.
    Returns:
        tf.Tensor: output tensor of shape `(batch_size, 1, dim)`
    &#34;&#34;&#34;
    output = self.mha(query=query, value=inputs, key=inputs, attention_mask=mask)
    return output</code></pre>
</details>
</dd>
<dt id="jidenn.models.DeParT.MultiheadClassAttention.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of
dict every time it is called. The callers should make a copy of the
returned dict if they want to modify it.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = super(MultiheadClassAttention, self).get_config()
    config.update({&#34;dim&#34;: self.dim, &#34;heads&#34;: self.heads, &#34;dropout&#34;: self.dropout})
    return config</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="jidenn.models.DeParT.SelfAttentionBlock"><code class="flex name class">
<span>class <span class="ident">SelfAttentionBlock</span></span>
<span>(</span><span>dim:Â int, heads:Â int, stoch_drop_prob:Â float, layer_scale_init_value:Â float, activation:Â Callable[[tensorflow.python.framework.ops.Tensor],Â tensorflow.python.framework.ops.Tensor], expansion:Â int, dropout:Â Optional[float]Â =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Self-attention block.
It contains a talking multi-head self-attention layer and a feed-forward network with residual connections
and layer normalizations. The self-attention layer includes the interaction variables.
Additionally, the stochastic dropout and layer scale are applied to the output of the self-attention layer
and feed-forward network.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>dimension of the input and output</dd>
<dt><strong><code>heads</code></strong> :&ensp;<code>int</code></dt>
<dd>number of heads</dd>
<dt><strong><code>stoch_drop_prob</code></strong> :&ensp;<code>float</code></dt>
<dd>probability of stochastic dropout</dd>
<dt><strong><code>layer_scale_init_value</code></strong> :&ensp;<code>float</code></dt>
<dd>initial value of layer scale</dd>
<dt>activation (Callable[[tf.Tensor], tf.Tensor]) activation function</dt>
<dt><strong><code>expansion</code></strong> :&ensp;<code>int</code></dt>
<dd>expansion factor of the feed-forward network,
the dimension of the feed-forward network is <code>dim * expansion</code></dd>
<dt><strong><code>dropout</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>dropout rate, defaults to None</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SelfAttentionBlock(tf.keras.layers.Layer):
    &#34;&#34;&#34;Self-attention block.
    It contains a talking multi-head self-attention layer and a feed-forward network with residual connections
    and layer normalizations. The self-attention layer includes the interaction variables.
    Additionally, the stochastic dropout and layer scale are applied to the output of the self-attention layer
    and feed-forward network.

    Args:
        dim (int): dimension of the input and output
        heads (int): number of heads
        stoch_drop_prob (float): probability of stochastic dropout
        layer_scale_init_value (float): initial value of layer scale
        activation (Callable[[tf.Tensor], tf.Tensor]) activation function
        expansion (int): expansion factor of the feed-forward network, 
            the dimension of the feed-forward network is `dim * expansion`
        dropout (float, optional): dropout rate, defaults to None

    &#34;&#34;&#34;

    def __init__(self, dim: int, heads: int, stoch_drop_prob: float, layer_scale_init_value: float, activation: Callable[[tf.Tensor], tf.Tensor], expansion: int,
                 dropout: Optional[float] = None):
        super().__init__()
        self.dim, self.heads, self.dropout, self.stoch_drop_prob, self.layer_scale_init_value, self.activation, self.expansion = dim, heads, dropout, stoch_drop_prob, layer_scale_init_value, activation, expansion

        self.pre_mhsa_ln = tf.keras.layers.LayerNormalization()
        self.mhsa = TalkingMultiheadSelfAttention(dim, heads, dropout)
        self.post_mhsa_scale = LayerScale(layer_scale_init_value, dim)
        self.post_mhsa_stoch_depth = StochasticDepth(drop_prob=stoch_drop_prob)

        self.pre_ffn_ln = tf.keras.layers.LayerNormalization()
        self.ffn = FFN(dim, expansion, activation, dropout)
        self.post_ffn_scale = LayerScale(layer_scale_init_value, dim)
        self.post_ffn_stoch_depth = StochasticDepth(drop_prob=stoch_drop_prob)

    def get_config(self):
        config = super(SelfAttentionBlock, self).get_config()
        config.update({&#34;dim&#34;: self.dim, &#34;heads&#34;: self.heads, &#34;dropout&#34;: self.dropout, &#34;stoch_drop_prob&#34;: self.stoch_drop_prob,
                      &#34;layer_scale_init_value&#34;: self.layer_scale_init_value, &#34;activation&#34;: self.activation, &#34;expansion&#34;: self.expansion})
        return config

    def call(self, inputs, mask, interaction=None):
        &#34;&#34;&#34;Forward pass of the self-attention block

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
            mask (tf.Tensor, optional): mask tensor of shape `(batch_size, num_particles, num_particles)`. Defaults to None.
                This mask is used to mask out the attention of padding particles, generated when
                tf.RaggedTensor is converted to tf.Tensor.
            interaction (tf.Tensor, optional): interaction tensor of shape `(batch_size, num_particles, num_particles, heads)`. Defaults to None.

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
        &#34;&#34;&#34;
        attented = self.pre_mhsa_ln(inputs)
        attented = self.mhsa(attented, mask, interaction)
        attented = self.post_mhsa_scale(attented)
        attented = self.post_mhsa_stoch_depth(attented)
        attented = attented + inputs

        ffned = self.pre_ffn_ln(attented)
        ffned = self.ffn(ffned)
        ffned = self.post_ffn_scale(ffned)
        ffned = self.post_ffn_stoch_depth(ffned)
        output = ffned + attented

        return output</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="jidenn.models.DeParT.SelfAttentionBlock.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs, mask, interaction=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass of the self-attention block</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>input tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
<dt><strong><code>mask</code></strong> :&ensp;<code>tf.Tensor</code>, optional</dt>
<dd>mask tensor of shape <code>(batch_size, num_particles, num_particles)</code>. Defaults to None.
This mask is used to mask out the attention of padding particles, generated when
tf.RaggedTensor is converted to tf.Tensor.</dd>
<dt><strong><code>interaction</code></strong> :&ensp;<code>tf.Tensor</code>, optional</dt>
<dd>interaction tensor of shape <code>(batch_size, num_particles, num_particles, heads)</code>. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tf.Tensor</code></dt>
<dd>output tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs, mask, interaction=None):
    &#34;&#34;&#34;Forward pass of the self-attention block

    Args:
        inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
        mask (tf.Tensor, optional): mask tensor of shape `(batch_size, num_particles, num_particles)`. Defaults to None.
            This mask is used to mask out the attention of padding particles, generated when
            tf.RaggedTensor is converted to tf.Tensor.
        interaction (tf.Tensor, optional): interaction tensor of shape `(batch_size, num_particles, num_particles, heads)`. Defaults to None.

    Returns:
        tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
    &#34;&#34;&#34;
    attented = self.pre_mhsa_ln(inputs)
    attented = self.mhsa(attented, mask, interaction)
    attented = self.post_mhsa_scale(attented)
    attented = self.post_mhsa_stoch_depth(attented)
    attented = attented + inputs

    ffned = self.pre_ffn_ln(attented)
    ffned = self.ffn(ffned)
    ffned = self.post_ffn_scale(ffned)
    ffned = self.post_ffn_stoch_depth(ffned)
    output = ffned + attented

    return output</code></pre>
</details>
</dd>
<dt id="jidenn.models.DeParT.SelfAttentionBlock.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of
dict every time it is called. The callers should make a copy of the
returned dict if they want to modify it.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = super(SelfAttentionBlock, self).get_config()
    config.update({&#34;dim&#34;: self.dim, &#34;heads&#34;: self.heads, &#34;dropout&#34;: self.dropout, &#34;stoch_drop_prob&#34;: self.stoch_drop_prob,
                  &#34;layer_scale_init_value&#34;: self.layer_scale_init_value, &#34;activation&#34;: self.activation, &#34;expansion&#34;: self.expansion})
    return config</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="jidenn.models.DeParT.StochasticDepth"><code class="flex name class">
<span>class <span class="ident">StochasticDepth</span></span>
<span>(</span><span>drop_prob:Â float)</span>
</code></dt>
<dd>
<div class="desc"><p>Stochastic depth layer.</p>
<p>Stochastic depth is a regularization technique that randomly drops layers instead
of individial neurons.</p>
<p>The probability of dropping should increase with the depth of the layer.
This must be done manually by the user when creating the layer.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>drop_prob</code></strong> :&ensp;<code>float</code></dt>
<dd>probability of dropping the layer</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class StochasticDepth(tf.keras.layers.Layer):
    &#34;&#34;&#34;Stochastic depth layer.

    Stochastic depth is a regularization technique that randomly drops layers instead 
    of individial neurons.

    The probability of dropping should increase with the depth of the layer.
    This must be done manually by the user when creating the layer.

    Args:
        drop_prob (float): probability of dropping the layer
    &#34;&#34;&#34;

    def __init__(self, drop_prob: float):
        super().__init__()
        self.drop_prob = drop_prob

    def call(self, x: tf.Tensor, training=False) -&gt; tf.Tensor:
        &#34;&#34;&#34;Forward pass of the stochastic depth layer

        Args:
            x (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`

        &#34;&#34;&#34;
        if training:
            keep_prob = 1 - self.drop_prob
            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)
            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)
            random_tensor = tf.floor(random_tensor)
            return (x / keep_prob) * random_tensor
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="jidenn.models.DeParT.StochasticDepth.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, x:Â tensorflow.python.framework.ops.Tensor, training=False) â€‘>Â tensorflow.python.framework.ops.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass of the stochastic depth layer</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>input tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tf.Tensor</code></dt>
<dd>output tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, x: tf.Tensor, training=False) -&gt; tf.Tensor:
    &#34;&#34;&#34;Forward pass of the stochastic depth layer

    Args:
        x (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`

    Returns:
        tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`

    &#34;&#34;&#34;
    if training:
        keep_prob = 1 - self.drop_prob
        shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)
        random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)
        random_tensor = tf.floor(random_tensor)
        return (x / keep_prob) * random_tensor
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="jidenn.models.DeParT.TalkingMultiheadClassAttention"><code class="flex name class">
<span>class <span class="ident">TalkingMultiheadClassAttention</span></span>
<span>(</span><span>dim:Â int, heads:Â int, dropout:Â Optional[float]Â =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Talking Multi-head class-attention layer
Standalone implementation of the multi-head class-attention layer, which
includes the talking heads mechanism.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>dimension of the input and output</dd>
<dt><strong><code>heads</code></strong> :&ensp;<code>int</code></dt>
<dd>number of heads</dd>
<dt><strong><code>dropout</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>dropout rate, defaults to None</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TalkingMultiheadClassAttention(tf.keras.layers.Layer):
    &#34;&#34;&#34;Talking Multi-head class-attention layer
    Standalone implementation of the multi-head class-attention layer, which
    includes the talking heads mechanism.

    Args:
        dim (int): dimension of the input and output
        heads (int): number of heads
        dropout (float, optional): dropout rate, defaults to None

    &#34;&#34;&#34;

    def __init__(self, dim: int, heads: int, dropout: Optional[float] = None):
        super().__init__()
        self.dim, self.heads = dim, heads

        self.linear_kv = tf.keras.layers.Dense(dim * 2)
        self.linear_q = tf.keras.layers.Dense(dim)
        self.linear_out = tf.keras.layers.Dense(dim)

        self.linear_talking_1 = tf.keras.layers.Dense(heads)
        self.linear_talking_2 = tf.keras.layers.Dense(heads)

        self.dropout = tf.keras.layers.Dropout(dropout)
        self.attn_drop = tf.keras.layers.Dropout(dropout)

    def get_config(self):
        config = super(TalkingMultiheadClassAttention, self).get_config()
        config.update({&#34;dim&#34;: self.dim, &#34;heads&#34;: self.heads})
        return config

    def call(self, inputs: tf.Tensor, class_token: tf.Tensor, mask: tf.Tensor, training: bool = False) -&gt; tf.Tensor:
        &#34;&#34;&#34;Forward pass of the multi-head class-attention layer

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
            class_token (tf.Tensor): class token tensor of shape `(batch_size, 1, dim)`
            mask (tf.Tensor): mask tensor of shape `(batch_size, 1, num_particles)`
                This mask is used to mask out the attention of padding particles, generated when
                tf.RaggedTensor is converted to tf.Tensor.
            training (bool, optional): whether the model is in training mode. Defaults to False.
        Returns:
            tf.Tensor: output tensor of shape `(batch_size, 1, dim)`
        &#34;&#34;&#34;

        B, N, C = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2]

        kv = self.linear_kv(inputs)  # (B, N, C * 3)
        kv = tf.reshape(kv, [B, N, 2, self.heads, C // self.heads])  # (B, N, 3, H, C // H)
        kv = tf.transpose(kv, [2, 0, 3, 1, 4])  # (3, B, H, N, C // H)
        k, v = kv[0], kv[1]  # 2 x (B, H, N, C // H)

        q = self.linear_q(class_token)  # (B, 1, C)
        q = tf.reshape(q, [B, self.heads, 1, C // self.heads])  # (B, H, 1, C // H)

        attention_weights = tf.linalg.matmul(q, k, transpose_b=True) / (q.shape[-1] ** 0.5)  # (B, H, 1, N)

        attention_weights = self.linear_talking_1(tf.transpose(attention_weights, [0, 2, 3, 1]))  # (B, 1, N, H)
        attention_weights = tf.transpose(attention_weights, [0, 3, 1, 2])  # (B, H, 1, N)

        attention = tf.keras.layers.Softmax()(attention_weights, mask=mask)  # (B, H, 1, N)
        attention = self.linear_talking_2(tf.transpose(attention, [0, 2, 3, 1]))  # (B, 1, N, H)
        attention = tf.transpose(attention, [0, 3, 1, 2])  # (B, H, 1, N)
        attention = self.attn_drop(attention, training)  # (B, H, 1, N)

        output = tf.linalg.matmul(attention, v)  # (B, H, 1, C // H)
        output = tf.transpose(output, [0, 2, 1, 3])  # (B, 1, H, C // H)
        output = tf.reshape(output, [B, 1, C])  # (B, 1, C)
        output = self.linear_out(output)  # (B, 1, C)
        output = self.dropout(output, training)
        return output</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="jidenn.models.DeParT.TalkingMultiheadClassAttention.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs:Â tensorflow.python.framework.ops.Tensor, class_token:Â tensorflow.python.framework.ops.Tensor, mask:Â tensorflow.python.framework.ops.Tensor, training:Â boolÂ =Â False) â€‘>Â tensorflow.python.framework.ops.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass of the multi-head class-attention layer</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>input tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
<dt><strong><code>class_token</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>class token tensor of shape <code>(batch_size, 1, dim)</code></dd>
<dt><strong><code>mask</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>mask tensor of shape <code>(batch_size, 1, num_particles)</code>
This mask is used to mask out the attention of padding particles, generated when
tf.RaggedTensor is converted to tf.Tensor.</dd>
<dt><strong><code>training</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>whether the model is in training mode. Defaults to False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tf.Tensor</code></dt>
<dd>output tensor of shape <code>(batch_size, 1, dim)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs: tf.Tensor, class_token: tf.Tensor, mask: tf.Tensor, training: bool = False) -&gt; tf.Tensor:
    &#34;&#34;&#34;Forward pass of the multi-head class-attention layer

    Args:
        inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
        class_token (tf.Tensor): class token tensor of shape `(batch_size, 1, dim)`
        mask (tf.Tensor): mask tensor of shape `(batch_size, 1, num_particles)`
            This mask is used to mask out the attention of padding particles, generated when
            tf.RaggedTensor is converted to tf.Tensor.
        training (bool, optional): whether the model is in training mode. Defaults to False.
    Returns:
        tf.Tensor: output tensor of shape `(batch_size, 1, dim)`
    &#34;&#34;&#34;

    B, N, C = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2]

    kv = self.linear_kv(inputs)  # (B, N, C * 3)
    kv = tf.reshape(kv, [B, N, 2, self.heads, C // self.heads])  # (B, N, 3, H, C // H)
    kv = tf.transpose(kv, [2, 0, 3, 1, 4])  # (3, B, H, N, C // H)
    k, v = kv[0], kv[1]  # 2 x (B, H, N, C // H)

    q = self.linear_q(class_token)  # (B, 1, C)
    q = tf.reshape(q, [B, self.heads, 1, C // self.heads])  # (B, H, 1, C // H)

    attention_weights = tf.linalg.matmul(q, k, transpose_b=True) / (q.shape[-1] ** 0.5)  # (B, H, 1, N)

    attention_weights = self.linear_talking_1(tf.transpose(attention_weights, [0, 2, 3, 1]))  # (B, 1, N, H)
    attention_weights = tf.transpose(attention_weights, [0, 3, 1, 2])  # (B, H, 1, N)

    attention = tf.keras.layers.Softmax()(attention_weights, mask=mask)  # (B, H, 1, N)
    attention = self.linear_talking_2(tf.transpose(attention, [0, 2, 3, 1]))  # (B, 1, N, H)
    attention = tf.transpose(attention, [0, 3, 1, 2])  # (B, H, 1, N)
    attention = self.attn_drop(attention, training)  # (B, H, 1, N)

    output = tf.linalg.matmul(attention, v)  # (B, H, 1, C // H)
    output = tf.transpose(output, [0, 2, 1, 3])  # (B, 1, H, C // H)
    output = tf.reshape(output, [B, 1, C])  # (B, 1, C)
    output = self.linear_out(output)  # (B, 1, C)
    output = self.dropout(output, training)
    return output</code></pre>
</details>
</dd>
<dt id="jidenn.models.DeParT.TalkingMultiheadClassAttention.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of
dict every time it is called. The callers should make a copy of the
returned dict if they want to modify it.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = super(TalkingMultiheadClassAttention, self).get_config()
    config.update({&#34;dim&#34;: self.dim, &#34;heads&#34;: self.heads})
    return config</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="jidenn.models.DeParT.TalkingMultiheadSelfAttention"><code class="flex name class">
<span>class <span class="ident">TalkingMultiheadSelfAttention</span></span>
<span>(</span><span>dim:Â int, heads:Â int, dropout:Â Optional[float]Â =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Talking Multi-head self-attention layer
Standalone implementation of the multi-head self-attention layer, which
includes the interaction variables and the talking heads mechanism.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>dimension of the input and output</dd>
<dt><strong><code>heads</code></strong> :&ensp;<code>int</code></dt>
<dd>number of heads</dd>
<dt><strong><code>dropout</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>dropout rate. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TalkingMultiheadSelfAttention(tf.keras.layers.Layer):
    &#34;&#34;&#34;Talking Multi-head self-attention layer
    Standalone implementation of the multi-head self-attention layer, which
    includes the interaction variables and the talking heads mechanism.

    Args:
        dim (int): dimension of the input and output
        heads (int): number of heads
        dropout (float, optional): dropout rate. Defaults to None.

    &#34;&#34;&#34;

    def __init__(self, dim: int, heads: int, dropout: Optional[float] = None):
        super().__init__()
        self.dim, self.heads = dim, heads

        self.linear_qkv = tf.keras.layers.Dense(dim * 3)
        self.linear_out = tf.keras.layers.Dense(dim)

        self.linear_talking_1 = tf.keras.layers.Dense(heads)
        self.linear_talking_2 = tf.keras.layers.Dense(heads)

        self.dropout = tf.keras.layers.Dropout(dropout)
        self.attn_drop = tf.keras.layers.Dropout(dropout)

    def get_config(self):
        config = super(TalkingMultiheadSelfAttention, self).get_config()
        config.update({&#34;dim&#34;: self.dim, &#34;heads&#34;: self.heads})
        return config

    def call(self, inputs: tf.Tensor, mask: tf.Tensor, interaction: Optional[tf.Tensor] = None, training: bool = False) -&gt; tf.Tensor:
        &#34;&#34;&#34;Forward pass of the talking multi-head self-attention layer

        Args:
            inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
            mask (tf.Tensor): mask tensor of shape `(batch_size, num_particles, num_particles)`
                This mask is used to mask out the attention of padding particles, generated when
                tf.RaggedTensor is converted to tf.Tensor.
            interaction (tf.Tensor, optional): interaction tensor of shape `(batch_size, num_particles, num_particles, heads)`
            training (bool, optional): whether the model is in training mode. Defaults to False.

        Returns:
            tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
        &#34;&#34;&#34;
        B, N, C = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2]

        qkv = self.linear_qkv(inputs)  # (B, N, C * 3)
        qkv = tf.reshape(qkv, [B, N, 3, self.heads, C // self.heads])  # (B, N, 3, H, C // H)
        qkv = tf.transpose(qkv, [2, 0, 3, 1, 4])  # (3, B, H, N, C // H)
        q, k, v = qkv[0], qkv[1], qkv[2]  # 3 x (B, H, N, C // H)

        attention_weights = tf.linalg.matmul(q, k, transpose_b=True) / (q.shape[-1] ** 0.5)  # (B, H, N, N)

        attention_weights = self.linear_talking_1(tf.transpose(attention_weights, [0, 2, 3, 1]))  # (B, N, N, H)
        attention_weights = tf.transpose(attention_weights, [0, 3, 1, 2])  # (B, H, N, N)

        if interaction is not None:
            interaction = tf.transpose(interaction, [0, 3, 1, 2])  # (B, H, N, N)
            attention_weights += interaction

        attention = tf.keras.layers.Softmax()(attention_weights, mask=mask)  # (B, H, N, N)
        attention = self.linear_talking_2(tf.transpose(attention, [0, 2, 3, 1]))  # (B, N, N, H)
        attention = tf.transpose(attention, [0, 3, 1, 2])  # (B, H, N, N)
        attention = self.attn_drop(attention, training)  # (B, H, N, N)

        output = tf.linalg.matmul(attention, v)  # (B, H, N, C // H)
        output = tf.transpose(output, [0, 2, 1, 3])  # (B, N, H, C // H)
        output = tf.reshape(output, [B, N, C])  # (B, N, C)
        output = self.linear_out(output)  # (B, N, C)
        output = self.dropout(output, training)
        return output</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="jidenn.models.DeParT.TalkingMultiheadSelfAttention.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs:Â tensorflow.python.framework.ops.Tensor, mask:Â tensorflow.python.framework.ops.Tensor, interaction:Â Optional[tensorflow.python.framework.ops.Tensor]Â =Â None, training:Â boolÂ =Â False) â€‘>Â tensorflow.python.framework.ops.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass of the talking multi-head self-attention layer</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>input tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
<dt><strong><code>mask</code></strong> :&ensp;<code>tf.Tensor</code></dt>
<dd>mask tensor of shape <code>(batch_size, num_particles, num_particles)</code>
This mask is used to mask out the attention of padding particles, generated when
tf.RaggedTensor is converted to tf.Tensor.</dd>
<dt><strong><code>interaction</code></strong> :&ensp;<code>tf.Tensor</code>, optional</dt>
<dd>interaction tensor of shape <code>(batch_size, num_particles, num_particles, heads)</code></dd>
<dt><strong><code>training</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>whether the model is in training mode. Defaults to False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tf.Tensor</code></dt>
<dd>output tensor of shape <code>(batch_size, num_particles, dim)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs: tf.Tensor, mask: tf.Tensor, interaction: Optional[tf.Tensor] = None, training: bool = False) -&gt; tf.Tensor:
    &#34;&#34;&#34;Forward pass of the talking multi-head self-attention layer

    Args:
        inputs (tf.Tensor): input tensor of shape `(batch_size, num_particles, dim)`
        mask (tf.Tensor): mask tensor of shape `(batch_size, num_particles, num_particles)`
            This mask is used to mask out the attention of padding particles, generated when
            tf.RaggedTensor is converted to tf.Tensor.
        interaction (tf.Tensor, optional): interaction tensor of shape `(batch_size, num_particles, num_particles, heads)`
        training (bool, optional): whether the model is in training mode. Defaults to False.

    Returns:
        tf.Tensor: output tensor of shape `(batch_size, num_particles, dim)`
    &#34;&#34;&#34;
    B, N, C = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2]

    qkv = self.linear_qkv(inputs)  # (B, N, C * 3)
    qkv = tf.reshape(qkv, [B, N, 3, self.heads, C // self.heads])  # (B, N, 3, H, C // H)
    qkv = tf.transpose(qkv, [2, 0, 3, 1, 4])  # (3, B, H, N, C // H)
    q, k, v = qkv[0], qkv[1], qkv[2]  # 3 x (B, H, N, C // H)

    attention_weights = tf.linalg.matmul(q, k, transpose_b=True) / (q.shape[-1] ** 0.5)  # (B, H, N, N)

    attention_weights = self.linear_talking_1(tf.transpose(attention_weights, [0, 2, 3, 1]))  # (B, N, N, H)
    attention_weights = tf.transpose(attention_weights, [0, 3, 1, 2])  # (B, H, N, N)

    if interaction is not None:
        interaction = tf.transpose(interaction, [0, 3, 1, 2])  # (B, H, N, N)
        attention_weights += interaction

    attention = tf.keras.layers.Softmax()(attention_weights, mask=mask)  # (B, H, N, N)
    attention = self.linear_talking_2(tf.transpose(attention, [0, 2, 3, 1]))  # (B, N, N, H)
    attention = tf.transpose(attention, [0, 3, 1, 2])  # (B, H, N, N)
    attention = self.attn_drop(attention, training)  # (B, H, N, N)

    output = tf.linalg.matmul(attention, v)  # (B, H, N, C // H)
    output = tf.transpose(output, [0, 2, 1, 3])  # (B, N, H, C // H)
    output = tf.reshape(output, [B, N, C])  # (B, N, C)
    output = self.linear_out(output)  # (B, N, C)
    output = self.dropout(output, training)
    return output</code></pre>
</details>
</dd>
<dt id="jidenn.models.DeParT.TalkingMultiheadSelfAttention.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of
dict every time it is called. The callers should make a copy of the
returned dict if they want to modify it.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = super(TalkingMultiheadSelfAttention, self).get_config()
    config.update({&#34;dim&#34;: self.dim, &#34;heads&#34;: self.heads})
    return config</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="JIDENN" href="https://jansam.wieno.sk/JIDENN/">
<img src="images/q_g_tagging.jpeg" alt=""> JIDENN
</a>
</header>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="jidenn.models" href="index.html">jidenn.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="jidenn.models.DeParT.CNNEmbedding" href="#jidenn.models.DeParT.CNNEmbedding">CNNEmbedding</a></code></h4>
<ul class="">
<li><code><a title="jidenn.models.DeParT.CNNEmbedding.call" href="#jidenn.models.DeParT.CNNEmbedding.call">call</a></code></li>
<li><code><a title="jidenn.models.DeParT.CNNEmbedding.get_config" href="#jidenn.models.DeParT.CNNEmbedding.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="jidenn.models.DeParT.ClassAttentionBlock" href="#jidenn.models.DeParT.ClassAttentionBlock">ClassAttentionBlock</a></code></h4>
<ul class="">
<li><code><a title="jidenn.models.DeParT.ClassAttentionBlock.call" href="#jidenn.models.DeParT.ClassAttentionBlock.call">call</a></code></li>
<li><code><a title="jidenn.models.DeParT.ClassAttentionBlock.get_config" href="#jidenn.models.DeParT.ClassAttentionBlock.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="jidenn.models.DeParT.DeParT" href="#jidenn.models.DeParT.DeParT">DeParT</a></code></h4>
<ul class="">
<li><code><a title="jidenn.models.DeParT.DeParT.call" href="#jidenn.models.DeParT.DeParT.call">call</a></code></li>
<li><code><a title="jidenn.models.DeParT.DeParT.get_config" href="#jidenn.models.DeParT.DeParT.get_config">get_config</a></code></li>
<li><code><a title="jidenn.models.DeParT.DeParT.stochastic_prob" href="#jidenn.models.DeParT.DeParT.stochastic_prob">stochastic_prob</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="jidenn.models.DeParT.DeParTModel" href="#jidenn.models.DeParT.DeParTModel">DeParTModel</a></code></h4>
</li>
<li>
<h4><code><a title="jidenn.models.DeParT.FCEmbedding" href="#jidenn.models.DeParT.FCEmbedding">FCEmbedding</a></code></h4>
<ul class="">
<li><code><a title="jidenn.models.DeParT.FCEmbedding.call" href="#jidenn.models.DeParT.FCEmbedding.call">call</a></code></li>
<li><code><a title="jidenn.models.DeParT.FCEmbedding.get_config" href="#jidenn.models.DeParT.FCEmbedding.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="jidenn.models.DeParT.FFN" href="#jidenn.models.DeParT.FFN">FFN</a></code></h4>
<ul class="">
<li><code><a title="jidenn.models.DeParT.FFN.call" href="#jidenn.models.DeParT.FFN.call">call</a></code></li>
<li><code><a title="jidenn.models.DeParT.FFN.get_config" href="#jidenn.models.DeParT.FFN.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="jidenn.models.DeParT.LayerScale" href="#jidenn.models.DeParT.LayerScale">LayerScale</a></code></h4>
<ul class="">
<li><code><a title="jidenn.models.DeParT.LayerScale.call" href="#jidenn.models.DeParT.LayerScale.call">call</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="jidenn.models.DeParT.MultiheadClassAttention" href="#jidenn.models.DeParT.MultiheadClassAttention">MultiheadClassAttention</a></code></h4>
<ul class="">
<li><code><a title="jidenn.models.DeParT.MultiheadClassAttention.call" href="#jidenn.models.DeParT.MultiheadClassAttention.call">call</a></code></li>
<li><code><a title="jidenn.models.DeParT.MultiheadClassAttention.get_config" href="#jidenn.models.DeParT.MultiheadClassAttention.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="jidenn.models.DeParT.SelfAttentionBlock" href="#jidenn.models.DeParT.SelfAttentionBlock">SelfAttentionBlock</a></code></h4>
<ul class="">
<li><code><a title="jidenn.models.DeParT.SelfAttentionBlock.call" href="#jidenn.models.DeParT.SelfAttentionBlock.call">call</a></code></li>
<li><code><a title="jidenn.models.DeParT.SelfAttentionBlock.get_config" href="#jidenn.models.DeParT.SelfAttentionBlock.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="jidenn.models.DeParT.StochasticDepth" href="#jidenn.models.DeParT.StochasticDepth">StochasticDepth</a></code></h4>
<ul class="">
<li><code><a title="jidenn.models.DeParT.StochasticDepth.call" href="#jidenn.models.DeParT.StochasticDepth.call">call</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="jidenn.models.DeParT.TalkingMultiheadClassAttention" href="#jidenn.models.DeParT.TalkingMultiheadClassAttention">TalkingMultiheadClassAttention</a></code></h4>
<ul class="">
<li><code><a title="jidenn.models.DeParT.TalkingMultiheadClassAttention.call" href="#jidenn.models.DeParT.TalkingMultiheadClassAttention.call">call</a></code></li>
<li><code><a title="jidenn.models.DeParT.TalkingMultiheadClassAttention.get_config" href="#jidenn.models.DeParT.TalkingMultiheadClassAttention.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="jidenn.models.DeParT.TalkingMultiheadSelfAttention" href="#jidenn.models.DeParT.TalkingMultiheadSelfAttention">TalkingMultiheadSelfAttention</a></code></h4>
<ul class="">
<li><code><a title="jidenn.models.DeParT.TalkingMultiheadSelfAttention.call" href="#jidenn.models.DeParT.TalkingMultiheadSelfAttention.call">call</a></code></li>
<li><code><a title="jidenn.models.DeParT.TalkingMultiheadSelfAttention.get_config" href="#jidenn.models.DeParT.TalkingMultiheadSelfAttention.get_config">get_config</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>
<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>jidenn.model_builders.optimizer_initialization API documentation</title>
<meta name="description" content="Module for initializing the optimizer from the config file.
The corresponding config dataclass is defined in `jidenn.config.config.Optimizer`." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="canonical" href="http://jansam.wieno.sk/JIDENN/jidenn/model_builders/optimizer_initialization.html">
<link rel="icon" href="images/q_g_tagging.jpeg">
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>jidenn.model_builders.optimizer_initialization</code></h1>
</header>
<section id="section-intro">
<p>Module for initializing the optimizer from the config file.
The corresponding config dataclass is defined in <code><a title="jidenn.config.config.Optimizer" href="../config/config.html#jidenn.config.config.Optimizer">Optimizer</a></code>.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Module for initializing the optimizer from the config file.
The corresponding config dataclass is defined in `jidenn.config.config.Optimizer`.
&#34;&#34;&#34;
import tensorflow as tf
import tensorflow_addons as tfa

from jidenn.config import config
from .LearningRateSchedulers import LinearWarmup


class Lion(tf.keras.optimizers.Optimizer):
    &#34;&#34;&#34;Original source: https://github.com/keras-team/keras/blob/master/keras/optimizers/lion.py
    Optimizer that implements the Lion algorithm.

    The Lion optimizer is a stochastic-gradient-descent method that uses the
    sign operator to control the magnitude of the update, unlike other adaptive
    optimizers such as Adam that rely on second-order moments. This make
    Lion more memory-efficient as it only keeps track of the momentum. According
    to the authors (see reference), its performance gain over Adam grows with
    the batch size. Because the update of Lion is produced through the sign
    operation, resulting in a larger norm, a suitable learning rate for Lion is
    typically 3-10x smaller than that for AdamW. The weight decay for Lion
    should be in turn 3-10x larger than that for AdamW to maintain a
    similar strength (lr * wd).

    Args:
      learning_rate: A `tf.Tensor`, floating point value, a schedule that is a
        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable
        that takes no arguments and returns the actual value to use. The
        learning rate. Defaults to 0.0001.
      beta_1: A float value or a constant float tensor, or a callable
        that takes no arguments and returns the actual value to use. The rate
        to combine the current gradient and the 1st moment estimate.
      beta_2: A float value or a constant float tensor, or a callable
        that takes no arguments and returns the actual value to use. The
        exponential decay rate for the 1st moment estimate.


    References:
      - [Chen et al., 2023](http://arxiv.org/abs/2302.06675)
      - [Authors&#39; implementation](
          http://github.com/google/automl/tree/master/lion)

    &#34;&#34;&#34;

    def __init__(
        self,
        learning_rate=0.0001,
        beta_1=0.9,
        beta_2=0.99,
        weight_decay=None,
        clipnorm=None,
        clipvalue=None,
        global_clipnorm=None,
        use_ema=False,
        ema_momentum=0.99,
        ema_overwrite_frequency=None,
        jit_compile=True,
        name=&#34;Lion&#34;,
        **kwargs,
    ):
        super().__init__(
            name=name,
            weight_decay=weight_decay,
            clipnorm=clipnorm,
            clipvalue=clipvalue,
            global_clipnorm=global_clipnorm,
            use_ema=use_ema,
            ema_momentum=ema_momentum,
            ema_overwrite_frequency=ema_overwrite_frequency,
            jit_compile=jit_compile,
            **kwargs,
        )
        self._learning_rate = self._build_learning_rate(learning_rate)
        self.beta_1 = beta_1
        self.beta_2 = beta_2
        if beta_1 &lt;= 0 or beta_1 &gt; 1:
            raise ValueError(
                f&#34;`beta_1`={beta_1} must be between ]0, 1]. Otherwise, &#34;
                &#34;the optimizer degenerates to SignSGD.&#34;
            )

    def build(self, var_list):
        &#34;&#34;&#34;Initialize optimizer variables.

        Lion optimizer has one variable `momentums`.

        Args:
          var_list: list of model variables to build Lion variables on.
        &#34;&#34;&#34;
        super().build(var_list)
        if hasattr(self, &#34;_built&#34;) and self._built:
            return
        self.momentums = []
        for var in var_list:
            self.momentums.append(
                self.add_variable_from_reference(
                    model_variable=var, variable_name=&#34;m&#34;
                )
            )
        self._built = True

    def update_step(self, gradient, variable):
        &#34;&#34;&#34;Update step given gradient and the associated model variable.&#34;&#34;&#34;
        lr = tf.cast(self.learning_rate, variable.dtype)
        beta_1 = tf.cast(self.beta_1, variable.dtype)
        beta_2 = tf.cast(self.beta_2, variable.dtype)
        var_key = self._var_key(variable)
        m = self.momentums[self._index_dict[var_key]]

        if isinstance(gradient, tf.IndexedSlices):
            # Sparse gradients (use m as a buffer)
            m.assign(m * beta_1)
            m.scatter_add(
                tf.IndexedSlices(
                    gradient.values * (1.0 - beta_1), gradient.indices
                )
            )
            variable.assign_sub(lr * tf.math.sign(m))

            m.assign(m * beta_2 / beta_1)
            m.scatter_add(
                tf.IndexedSlices(
                    gradient.values * (1.0 - beta_2 / beta_1), gradient.indices
                )
            )
        else:
            # Dense gradients
            variable.assign_sub(
                lr * tf.math.sign(m * beta_1 + gradient * (1.0 - beta_1))
            )
            m.assign(m * beta_2 + gradient * (1.0 - beta_2))

    def get_config(self):
        config = super().get_config()

        config.update(
            {
                &#34;learning_rate&#34;: self._serialize_hyperparameter(
                    self._learning_rate
                ),
                &#34;beta_1&#34;: self.beta_1,
                &#34;beta_2&#34;: self.beta_2,
            }
        )
        return config


def get_optimizer(args_optimizer: config.Optimizer) -&gt; tf.keras.optimizers.Optimizer:
    &#34;&#34;&#34;Initializes the optimizer from the config file.
    Possible optimizers are `tf.optimizers.Adam` and `tfa.optimizers.LAMB`.
    If the `weight_decay` parameter is set, the `tfa.optimizers.AdamW` optimizer is used.

    Args:
        args_optimizer (jidenn.config.config.Optimizer): config dataclass for the optimizer

    Raises:
        NotImplementedError: Raised if the optimizer is not supported.

    Returns:
        tf.keras.optimizers.Optimizer: Optimizer object with set parameters.
    &#34;&#34;&#34;

    optimizer = &#39;Adam&#39; if args_optimizer.name is None else args_optimizer.name
    learning_rate = 0.001 if args_optimizer.learning_rate is None else args_optimizer.learning_rate
    decay_steps = None if args_optimizer.decay_steps is None else args_optimizer.decay_steps
    warmup_steps = None if args_optimizer.warmup_steps is None else args_optimizer.warmup_steps
    beta_1 = 0.9 if args_optimizer.beta_1 is None else args_optimizer.beta_1
    beta_2 = 0.999 if args_optimizer.beta_2 is None else args_optimizer.beta_2
    epsilon = 1e-6 if args_optimizer.epsilon is None else args_optimizer.epsilon
    clipnorm = None if args_optimizer.clipnorm is None or args_optimizer.clipnorm == 0.0 else args_optimizer.clipnorm
    weight_decay = 0.0 if args_optimizer.weight_decay is None else args_optimizer.weight_decay
    min_lr = 0.0 if args_optimizer.min_learning_rate is None else args_optimizer.min_learning_rate
    
    l_r = tf.keras.optimizers.schedules.CosineDecay(
        learning_rate, decay_steps, alpha=min_lr) if decay_steps is not None else learning_rate

    if warmup_steps is not None:
        l_r = LinearWarmup(warmup_steps, l_r)

    if optimizer == &#39;LAMB&#39;:
        return tfa.optimizers.LAMB(learning_rate=l_r,
                                   weight_decay=weight_decay,
                                   beta_1=beta_1,
                                   beta_2=beta_2,
                                   epsilon=epsilon,
                                   clipnorm=clipnorm)
    elif optimizer == &#39;Lion&#39;:
        return Lion(learning_rate=l_r,
                    weight_decay=weight_decay,
                    beta_1=beta_1,
                    beta_2=beta_2,
                    clipnorm=clipnorm)

    elif optimizer == &#39;Adam&#39;:
        if weight_decay &gt; 0.0:
            return tfa.optimizers.AdamW(learning_rate=l_r,
                                        weight_decay=weight_decay,
                                        beta_1=beta_1,
                                        beta_2=beta_2,
                                        epsilon=epsilon,
                                        clipnorm=clipnorm)
        else:
            return tf.optimizers.Adam(learning_rate=l_r,
                                      beta_1=beta_1,
                                      beta_2=beta_2,
                                      epsilon=epsilon,
                                      clipnorm=clipnorm)
    else:
        raise NotImplementedError(f&#39;Optimizer {optimizer} not supported.&#39;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="jidenn.model_builders.optimizer_initialization.get_optimizer"><code class="name flex">
<span>def <span class="ident">get_optimizer</span></span>(<span>args_optimizer: <a title="jidenn.config.config.Optimizer" href="../config/config.html#jidenn.config.config.Optimizer">Optimizer</a>) ‑> keras.optimizers.optimizer_experimental.optimizer.Optimizer</span>
</code></dt>
<dd>
<div class="desc"><p>Initializes the optimizer from the config file.
Possible optimizers are <code>tf.optimizers.Adam</code> and <code>tfa.optimizers.LAMB</code>.
If the <code>weight_decay</code> parameter is set, the <code>tfa.optimizers.AdamW</code> optimizer is used.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>args_optimizer</code></strong> :&ensp;<code><a title="jidenn.config.config.Optimizer" href="../config/config.html#jidenn.config.config.Optimizer">Optimizer</a></code></dt>
<dd>config dataclass for the optimizer</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>NotImplementedError</code></dt>
<dd>Raised if the optimizer is not supported.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tf.keras.optimizers.Optimizer</code></dt>
<dd>Optimizer object with set parameters.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_optimizer(args_optimizer: config.Optimizer) -&gt; tf.keras.optimizers.Optimizer:
    &#34;&#34;&#34;Initializes the optimizer from the config file.
    Possible optimizers are `tf.optimizers.Adam` and `tfa.optimizers.LAMB`.
    If the `weight_decay` parameter is set, the `tfa.optimizers.AdamW` optimizer is used.

    Args:
        args_optimizer (jidenn.config.config.Optimizer): config dataclass for the optimizer

    Raises:
        NotImplementedError: Raised if the optimizer is not supported.

    Returns:
        tf.keras.optimizers.Optimizer: Optimizer object with set parameters.
    &#34;&#34;&#34;

    optimizer = &#39;Adam&#39; if args_optimizer.name is None else args_optimizer.name
    learning_rate = 0.001 if args_optimizer.learning_rate is None else args_optimizer.learning_rate
    decay_steps = None if args_optimizer.decay_steps is None else args_optimizer.decay_steps
    warmup_steps = None if args_optimizer.warmup_steps is None else args_optimizer.warmup_steps
    beta_1 = 0.9 if args_optimizer.beta_1 is None else args_optimizer.beta_1
    beta_2 = 0.999 if args_optimizer.beta_2 is None else args_optimizer.beta_2
    epsilon = 1e-6 if args_optimizer.epsilon is None else args_optimizer.epsilon
    clipnorm = None if args_optimizer.clipnorm is None or args_optimizer.clipnorm == 0.0 else args_optimizer.clipnorm
    weight_decay = 0.0 if args_optimizer.weight_decay is None else args_optimizer.weight_decay
    min_lr = 0.0 if args_optimizer.min_learning_rate is None else args_optimizer.min_learning_rate
    
    l_r = tf.keras.optimizers.schedules.CosineDecay(
        learning_rate, decay_steps, alpha=min_lr) if decay_steps is not None else learning_rate

    if warmup_steps is not None:
        l_r = LinearWarmup(warmup_steps, l_r)

    if optimizer == &#39;LAMB&#39;:
        return tfa.optimizers.LAMB(learning_rate=l_r,
                                   weight_decay=weight_decay,
                                   beta_1=beta_1,
                                   beta_2=beta_2,
                                   epsilon=epsilon,
                                   clipnorm=clipnorm)
    elif optimizer == &#39;Lion&#39;:
        return Lion(learning_rate=l_r,
                    weight_decay=weight_decay,
                    beta_1=beta_1,
                    beta_2=beta_2,
                    clipnorm=clipnorm)

    elif optimizer == &#39;Adam&#39;:
        if weight_decay &gt; 0.0:
            return tfa.optimizers.AdamW(learning_rate=l_r,
                                        weight_decay=weight_decay,
                                        beta_1=beta_1,
                                        beta_2=beta_2,
                                        epsilon=epsilon,
                                        clipnorm=clipnorm)
        else:
            return tf.optimizers.Adam(learning_rate=l_r,
                                      beta_1=beta_1,
                                      beta_2=beta_2,
                                      epsilon=epsilon,
                                      clipnorm=clipnorm)
    else:
        raise NotImplementedError(f&#39;Optimizer {optimizer} not supported.&#39;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="jidenn.model_builders.optimizer_initialization.Lion"><code class="flex name class">
<span>class <span class="ident">Lion</span></span>
<span>(</span><span>learning_rate=0.0001, beta_1=0.9, beta_2=0.99, weight_decay=None, clipnorm=None, clipvalue=None, global_clipnorm=None, use_ema=False, ema_momentum=0.99, ema_overwrite_frequency=None, jit_compile=True, name='Lion', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Original source: <a href="https://github.com/keras-team/keras/blob/master/keras/optimizers/lion.py">https://github.com/keras-team/keras/blob/master/keras/optimizers/lion.py</a>
Optimizer that implements the Lion algorithm.</p>
<p>The Lion optimizer is a stochastic-gradient-descent method that uses the
sign operator to control the magnitude of the update, unlike other adaptive
optimizers such as Adam that rely on second-order moments. This make
Lion more memory-efficient as it only keeps track of the momentum. According
to the authors (see reference), its performance gain over Adam grows with
the batch size. Because the update of Lion is produced through the sign
operation, resulting in a larger norm, a suitable learning rate for Lion is
typically 3-10x smaller than that for AdamW. The weight decay for Lion
should be in turn 3-10x larger than that for AdamW to maintain a
similar strength (lr * wd).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>learning_rate</code></strong></dt>
<dd>A <code>tf.Tensor</code>, floating point value, a schedule that is a
<code>tf.keras.optimizers.schedules.LearningRateSchedule</code>, or a callable
that takes no arguments and returns the actual value to use. The
learning rate. Defaults to 0.0001.</dd>
<dt><strong><code>beta_1</code></strong></dt>
<dd>A float value or a constant float tensor, or a callable
that takes no arguments and returns the actual value to use. The rate
to combine the current gradient and the 1st moment estimate.</dd>
<dt><strong><code>beta_2</code></strong></dt>
<dd>A float value or a constant float tensor, or a callable
that takes no arguments and returns the actual value to use. The
exponential decay rate for the 1st moment estimate.</dd>
</dl>
<h2 id="references">References</h2>
<ul>
<li><a href="http://arxiv.org/abs/2302.06675">Chen et al., 2023</a></li>
<li><a href="http://github.com/google/automl/tree/master/lion">Authors' implementation</a></li>
</ul>
<p>Create a new Optimizer.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Lion(tf.keras.optimizers.Optimizer):
    &#34;&#34;&#34;Original source: https://github.com/keras-team/keras/blob/master/keras/optimizers/lion.py
    Optimizer that implements the Lion algorithm.

    The Lion optimizer is a stochastic-gradient-descent method that uses the
    sign operator to control the magnitude of the update, unlike other adaptive
    optimizers such as Adam that rely on second-order moments. This make
    Lion more memory-efficient as it only keeps track of the momentum. According
    to the authors (see reference), its performance gain over Adam grows with
    the batch size. Because the update of Lion is produced through the sign
    operation, resulting in a larger norm, a suitable learning rate for Lion is
    typically 3-10x smaller than that for AdamW. The weight decay for Lion
    should be in turn 3-10x larger than that for AdamW to maintain a
    similar strength (lr * wd).

    Args:
      learning_rate: A `tf.Tensor`, floating point value, a schedule that is a
        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable
        that takes no arguments and returns the actual value to use. The
        learning rate. Defaults to 0.0001.
      beta_1: A float value or a constant float tensor, or a callable
        that takes no arguments and returns the actual value to use. The rate
        to combine the current gradient and the 1st moment estimate.
      beta_2: A float value or a constant float tensor, or a callable
        that takes no arguments and returns the actual value to use. The
        exponential decay rate for the 1st moment estimate.


    References:
      - [Chen et al., 2023](http://arxiv.org/abs/2302.06675)
      - [Authors&#39; implementation](
          http://github.com/google/automl/tree/master/lion)

    &#34;&#34;&#34;

    def __init__(
        self,
        learning_rate=0.0001,
        beta_1=0.9,
        beta_2=0.99,
        weight_decay=None,
        clipnorm=None,
        clipvalue=None,
        global_clipnorm=None,
        use_ema=False,
        ema_momentum=0.99,
        ema_overwrite_frequency=None,
        jit_compile=True,
        name=&#34;Lion&#34;,
        **kwargs,
    ):
        super().__init__(
            name=name,
            weight_decay=weight_decay,
            clipnorm=clipnorm,
            clipvalue=clipvalue,
            global_clipnorm=global_clipnorm,
            use_ema=use_ema,
            ema_momentum=ema_momentum,
            ema_overwrite_frequency=ema_overwrite_frequency,
            jit_compile=jit_compile,
            **kwargs,
        )
        self._learning_rate = self._build_learning_rate(learning_rate)
        self.beta_1 = beta_1
        self.beta_2 = beta_2
        if beta_1 &lt;= 0 or beta_1 &gt; 1:
            raise ValueError(
                f&#34;`beta_1`={beta_1} must be between ]0, 1]. Otherwise, &#34;
                &#34;the optimizer degenerates to SignSGD.&#34;
            )

    def build(self, var_list):
        &#34;&#34;&#34;Initialize optimizer variables.

        Lion optimizer has one variable `momentums`.

        Args:
          var_list: list of model variables to build Lion variables on.
        &#34;&#34;&#34;
        super().build(var_list)
        if hasattr(self, &#34;_built&#34;) and self._built:
            return
        self.momentums = []
        for var in var_list:
            self.momentums.append(
                self.add_variable_from_reference(
                    model_variable=var, variable_name=&#34;m&#34;
                )
            )
        self._built = True

    def update_step(self, gradient, variable):
        &#34;&#34;&#34;Update step given gradient and the associated model variable.&#34;&#34;&#34;
        lr = tf.cast(self.learning_rate, variable.dtype)
        beta_1 = tf.cast(self.beta_1, variable.dtype)
        beta_2 = tf.cast(self.beta_2, variable.dtype)
        var_key = self._var_key(variable)
        m = self.momentums[self._index_dict[var_key]]

        if isinstance(gradient, tf.IndexedSlices):
            # Sparse gradients (use m as a buffer)
            m.assign(m * beta_1)
            m.scatter_add(
                tf.IndexedSlices(
                    gradient.values * (1.0 - beta_1), gradient.indices
                )
            )
            variable.assign_sub(lr * tf.math.sign(m))

            m.assign(m * beta_2 / beta_1)
            m.scatter_add(
                tf.IndexedSlices(
                    gradient.values * (1.0 - beta_2 / beta_1), gradient.indices
                )
            )
        else:
            # Dense gradients
            variable.assign_sub(
                lr * tf.math.sign(m * beta_1 + gradient * (1.0 - beta_1))
            )
            m.assign(m * beta_2 + gradient * (1.0 - beta_2))

    def get_config(self):
        config = super().get_config()

        config.update(
            {
                &#34;learning_rate&#34;: self._serialize_hyperparameter(
                    self._learning_rate
                ),
                &#34;beta_1&#34;: self.beta_1,
                &#34;beta_2&#34;: self.beta_2,
            }
        )
        return config</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.optimizers.optimizer_experimental.optimizer.Optimizer</li>
<li>keras.optimizers.optimizer_experimental.optimizer._BaseOptimizer</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="jidenn.model_builders.optimizer_initialization.Lion.build"><code class="name flex">
<span>def <span class="ident">build</span></span>(<span>self, var_list)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize optimizer variables.</p>
<p>Lion optimizer has one variable <code>momentums</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>var_list</code></strong></dt>
<dd>list of model variables to build Lion variables on.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build(self, var_list):
    &#34;&#34;&#34;Initialize optimizer variables.

    Lion optimizer has one variable `momentums`.

    Args:
      var_list: list of model variables to build Lion variables on.
    &#34;&#34;&#34;
    super().build(var_list)
    if hasattr(self, &#34;_built&#34;) and self._built:
        return
    self.momentums = []
    for var in var_list:
        self.momentums.append(
            self.add_variable_from_reference(
                model_variable=var, variable_name=&#34;m&#34;
            )
        )
    self._built = True</code></pre>
</details>
</dd>
<dt id="jidenn.model_builders.optimizer_initialization.Lion.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the optimizer.</p>
<p>An optimizer config is a Python dictionary (serializable)
containing the configuration of an optimizer.
The same optimizer can be reinstantiated later
(without any saved state) from this configuration.</p>
<p>Subclass optimizer should override this method to include other
hyperparameters.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = super().get_config()

    config.update(
        {
            &#34;learning_rate&#34;: self._serialize_hyperparameter(
                self._learning_rate
            ),
            &#34;beta_1&#34;: self.beta_1,
            &#34;beta_2&#34;: self.beta_2,
        }
    )
    return config</code></pre>
</details>
</dd>
<dt id="jidenn.model_builders.optimizer_initialization.Lion.update_step"><code class="name flex">
<span>def <span class="ident">update_step</span></span>(<span>self, gradient, variable)</span>
</code></dt>
<dd>
<div class="desc"><p>Update step given gradient and the associated model variable.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_step(self, gradient, variable):
    &#34;&#34;&#34;Update step given gradient and the associated model variable.&#34;&#34;&#34;
    lr = tf.cast(self.learning_rate, variable.dtype)
    beta_1 = tf.cast(self.beta_1, variable.dtype)
    beta_2 = tf.cast(self.beta_2, variable.dtype)
    var_key = self._var_key(variable)
    m = self.momentums[self._index_dict[var_key]]

    if isinstance(gradient, tf.IndexedSlices):
        # Sparse gradients (use m as a buffer)
        m.assign(m * beta_1)
        m.scatter_add(
            tf.IndexedSlices(
                gradient.values * (1.0 - beta_1), gradient.indices
            )
        )
        variable.assign_sub(lr * tf.math.sign(m))

        m.assign(m * beta_2 / beta_1)
        m.scatter_add(
            tf.IndexedSlices(
                gradient.values * (1.0 - beta_2 / beta_1), gradient.indices
            )
        )
    else:
        # Dense gradients
        variable.assign_sub(
            lr * tf.math.sign(m * beta_1 + gradient * (1.0 - beta_1))
        )
        m.assign(m * beta_2 + gradient * (1.0 - beta_2))</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="JIDENN" href="https://jansam.wieno.sk/JIDENN/">
<img src="images/q_g_tagging.jpeg" alt=""> JIDENN
</a>
</header>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="jidenn.model_builders" href="index.html">jidenn.model_builders</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="jidenn.model_builders.optimizer_initialization.get_optimizer" href="#jidenn.model_builders.optimizer_initialization.get_optimizer">get_optimizer</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="jidenn.model_builders.optimizer_initialization.Lion" href="#jidenn.model_builders.optimizer_initialization.Lion">Lion</a></code></h4>
<ul class="">
<li><code><a title="jidenn.model_builders.optimizer_initialization.Lion.build" href="#jidenn.model_builders.optimizer_initialization.Lion.build">build</a></code></li>
<li><code><a title="jidenn.model_builders.optimizer_initialization.Lion.get_config" href="#jidenn.model_builders.optimizer_initialization.Lion.get_config">get_config</a></code></li>
<li><code><a title="jidenn.model_builders.optimizer_initialization.Lion.update_step" href="#jidenn.model_builders.optimizer_initialization.Lion.update_step">update_step</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>